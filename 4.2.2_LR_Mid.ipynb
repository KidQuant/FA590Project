{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import sklearn.preprocessing\n",
    "import datetime\n",
    "from TimeBasedCV import TimeBasedCV\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "from sklearn.metrics import make_scorer, r2_score\n",
    "import warnings\n",
    "import statsmodels.api as sm\n",
    "warnings.simplefilter(action='ignore', category=Warning)\n",
    "from sklearn.linear_model import HuberRegressor\n",
    "# pd.set_option('display.max_rows', None)\n",
    "# more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/factors_1970.csv', parse_dates=['DATE'])\n",
    "# df = pd.read_csv('data/features_subset.csv', parse_dates=['DATE'])\n",
    "# df = pd.read_csv('factors_1900.csv', parse_dates=['DATE'])\n",
    "# df.drop(columns=['sic2'], inplace=True)\n",
    "\n",
    "df = df[['DATE','permno','mom12m', 'mvel1', 'bm', 'acc', 'roaq', 'agr', 'egr','macro_tbl', 'macro_svar','risk_premium']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DATE</th>\n",
       "      <th>permno</th>\n",
       "      <th>mom12m</th>\n",
       "      <th>mvel1</th>\n",
       "      <th>bm</th>\n",
       "      <th>acc</th>\n",
       "      <th>roaq</th>\n",
       "      <th>agr</th>\n",
       "      <th>egr</th>\n",
       "      <th>macro_tbl</th>\n",
       "      <th>macro_svar</th>\n",
       "      <th>risk_premium</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1970-02-27</td>\n",
       "      <td>10401</td>\n",
       "      <td>-0.057315</td>\n",
       "      <td>26227356.0</td>\n",
       "      <td>0.796609</td>\n",
       "      <td>0.055965</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.067613</td>\n",
       "      <td>0.038232</td>\n",
       "      <td>0.0713</td>\n",
       "      <td>0.001059</td>\n",
       "      <td>0.4798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1970-02-27</td>\n",
       "      <td>10604</td>\n",
       "      <td>-0.234695</td>\n",
       "      <td>3196008.0</td>\n",
       "      <td>0.245563</td>\n",
       "      <td>0.055042</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.299523</td>\n",
       "      <td>0.072061</td>\n",
       "      <td>0.0713</td>\n",
       "      <td>0.001059</td>\n",
       "      <td>-11.7111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1970-02-27</td>\n",
       "      <td>10786</td>\n",
       "      <td>-0.171812</td>\n",
       "      <td>1133566.5</td>\n",
       "      <td>1.379277</td>\n",
       "      <td>0.055042</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.007556</td>\n",
       "      <td>0.014443</td>\n",
       "      <td>0.0713</td>\n",
       "      <td>0.001059</td>\n",
       "      <td>-1.9956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1970-02-27</td>\n",
       "      <td>10890</td>\n",
       "      <td>0.359525</td>\n",
       "      <td>2662344.0</td>\n",
       "      <td>0.144765</td>\n",
       "      <td>0.055042</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.293566</td>\n",
       "      <td>0.171707</td>\n",
       "      <td>0.0713</td>\n",
       "      <td>0.001059</td>\n",
       "      <td>-10.3290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1970-02-27</td>\n",
       "      <td>11260</td>\n",
       "      <td>-0.321664</td>\n",
       "      <td>1342376.0</td>\n",
       "      <td>0.794354</td>\n",
       "      <td>0.055042</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.140970</td>\n",
       "      <td>0.126208</td>\n",
       "      <td>0.0713</td>\n",
       "      <td>0.001059</td>\n",
       "      <td>-10.4214</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        DATE  permno    mom12m       mvel1        bm       acc  roaq  \\\n",
       "0 1970-02-27   10401 -0.057315  26227356.0  0.796609  0.055965   NaN   \n",
       "1 1970-02-27   10604 -0.234695   3196008.0  0.245563  0.055042   NaN   \n",
       "2 1970-02-27   10786 -0.171812   1133566.5  1.379277  0.055042   NaN   \n",
       "3 1970-02-27   10890  0.359525   2662344.0  0.144765  0.055042   NaN   \n",
       "4 1970-02-27   11260 -0.321664   1342376.0  0.794354  0.055042   NaN   \n",
       "\n",
       "        agr       egr  macro_tbl  macro_svar  risk_premium  \n",
       "0 -0.067613  0.038232     0.0713    0.001059        0.4798  \n",
       "1 -0.299523  0.072061     0.0713    0.001059      -11.7111  \n",
       "2  0.007556  0.014443     0.0713    0.001059       -1.9956  \n",
       "3 -0.293566  0.171707     0.0713    0.001059      -10.3290  \n",
       "4 -0.140970  0.126208     0.0713    0.001059      -10.4214  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Sort observations by date and stock id\n",
    "df[df.columns[2:]] = df[df.columns[2:]].astype('float32')\n",
    "df = df.sort_values(by = ['DATE', 'permno'], ascending = True)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['permno2'] = df['permno'].copy()\n",
    "df['DATE2'] = df['DATE'].copy()\n",
    "df = df.set_index(['DATE2','permno2'])\n",
    "df['mvel12'] = df['mvel1'].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "p=0.3 \n",
    "df_top= df.groupby('DATE').apply(lambda x: x.nlargest(int(len(x)*p),'mvel1')).reset_index(drop=True)  \n",
    "df_bottom = df.groupby('DATE').apply(lambda x: x.nsmallest(int(len(x)*p),'mvel1')).reset_index(drop=True)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df.columns[~df.columns.isin(['DATE', 'DATE2', \"mvel2\",'permno',\"permno2\",'risk_premium'])].tolist()\n",
    "df[features]=df.groupby('DATE')[features].rank(pct=True)\n",
    "\n",
    "df[features] = 2*df[features] - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_r2(y_true, y_pred, in_sample=True, benchmark=None):\n",
    "    if in_sample:\n",
    "        return 1 - (np.sum((y_true - y_pred) ** 2) / \n",
    "                    np.sum((y_true - np.mean(y_true)) ** 2))\n",
    "    else:\n",
    "        if benchmark is None:\n",
    "            raise ValueError(\"Benchmark must be provided for out-of-sample R-squared calculation.\")\n",
    "        return 1 - (np.sum((y_true - y_pred) ** 2) / \n",
    "                    np.sum((y_true - benchmark) ** 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train period: 1980-01-31 - 1985-01-31 ,val period: 1985-01-31 - 1987-01-31 # train records 18589 ,# val records 11286\n",
      "Train period: 1981-01-31 - 1986-01-31 ,val period: 1986-01-31 - 1988-01-31 # train records 20125 ,# val records 13057\n",
      "Train period: 1982-01-31 - 1987-01-31 ,val period: 1987-01-31 - 1989-01-31 # train records 22963 ,# val records 12572\n",
      "Train period: 1983-01-31 - 1988-01-31 ,val period: 1988-01-31 - 1990-01-31 # train records 26711 ,# val records 12781\n",
      "Train period: 1984-01-31 - 1989-01-31 ,val period: 1989-01-31 - 1991-01-31 # train records 28226 ,# val records 13403\n",
      "Train period: 1985-01-31 - 1990-01-31 ,val period: 1990-01-31 - 1992-01-31 # train records 30708 ,# val records 13616\n",
      "Train period: 1986-01-31 - 1991-01-31 ,val period: 1991-01-31 - 1993-01-31 # train records 32391 ,# val records 15806\n",
      "Train period: 1987-01-31 - 1992-01-31 ,val period: 1992-01-31 - 1994-01-31 # train records 33038 ,# val records 17371\n",
      "Train period: 1988-01-31 - 1993-01-31 ,val period: 1993-01-31 - 1995-01-31 # train records 35140 ,# val records 18821\n",
      "Train period: 1989-01-31 - 1994-01-31 ,val period: 1994-01-31 - 1996-01-31 # train records 37837 ,# val records 21369\n",
      "Train period: 1990-01-31 - 1995-01-31 ,val period: 1995-01-31 - 1997-01-31 # train records 41180 ,# val records 24121\n",
      "Train period: 1991-01-31 - 1996-01-31 ,val period: 1996-01-31 - 1998-01-31 # train records 45803 ,# val records 28955\n",
      "Train period: 1992-01-31 - 1997-01-31 ,val period: 1997-01-31 - 1999-01-31 # train records 51685 ,# val records 31959\n",
      "Train period: 1993-01-31 - 1998-01-31 ,val period: 1998-01-31 - 2000-01-31 # train records 58952 ,# val records 30796\n",
      "Train period: 1994-01-31 - 1999-01-31 ,val period: 1999-01-31 - 2001-01-31 # train records 66273 ,# val records 33236\n",
      "Train period: 1995-01-31 - 2000-01-31 ,val period: 2000-01-31 - 2002-01-31 # train records 70927 ,# val records 34622\n",
      "Train period: 1996-01-31 - 2001-01-31 ,val period: 2001-01-31 - 2003-01-31 # train records 78140 ,# val records 31682\n",
      "Train period: 1997-01-31 - 2002-01-31 ,val period: 2002-01-31 - 2004-01-31 # train records 81428 ,# val records 33091\n",
      "Train period: 1998-01-31 - 2003-01-31 ,val period: 2003-01-31 - 2005-01-31 # train records 80867 ,# val records 35622\n",
      "Train period: 1999-01-31 - 2004-01-31 ,val period: 2004-01-31 - 2006-01-31 # train records 82560 ,# val records 39570\n",
      "Train period: 2000-01-31 - 2005-01-31 ,val period: 2005-01-31 - 2007-01-31 # train records 85693 ,# val records 45111\n",
      "Train period: 2001-01-31 - 2006-01-31 ,val period: 2006-01-31 - 2008-01-31 # train records 88894 ,# val records 47991\n",
      "Train period: 2002-01-31 - 2007-01-31 ,val period: 2007-01-31 - 2009-01-31 # train records 96182 ,# val records 46419\n",
      "Train period: 2003-01-31 - 2008-01-31 ,val period: 2008-01-31 - 2010-01-31 # train records 105203 ,# val records 38716\n",
      "Train period: 2004-01-31 - 2009-01-31 ,val period: 2009-01-31 - 2011-01-31 # train records 109510 ,# val records 34937\n",
      "Train period: 2005-01-31 - 2010-01-31 ,val period: 2010-01-31 - 2012-01-31 # train records 108297 ,# val records 39748\n",
      "Train period: 2006-01-31 - 2011-01-31 ,val period: 2011-01-31 - 2013-01-31 # train records 104877 ,# val records 43094\n",
      "Train period: 2007-01-31 - 2012-01-31 ,val period: 2012-01-31 - 2014-01-31 # train records 102934 ,# val records 45393\n",
      "Train period: 2008-01-31 - 2013-01-31 ,val period: 2013-01-31 - 2015-01-31 # train records 99980 ,# val records 52517\n",
      "Train period: 2009-01-31 - 2014-01-31 ,val period: 2014-01-31 - 2016-01-31 # train records 101908 ,# val records 55101\n",
      "Train period: 2010-01-31 - 2015-01-31 ,val period: 2015-01-31 - 2017-01-31 # train records 113781 ,# val records 49648\n",
      "Train period: 2011-01-31 - 2016-01-31 ,val period: 2016-01-31 - 2018-01-31 # train records 122072 ,# val records 50289\n",
      "Train period: 2012-01-31 - 2017-01-31 ,val period: 2017-01-31 - 2019-01-31 # train records 123681 ,# val records 55523\n",
      "Train period: 2013-01-31 - 2018-01-31 ,val period: 2018-01-31 - 2020-01-31 # train records 129267 ,# val records 55692\n",
      "Train period: 2014-01-31 - 2019-01-31 ,val period: 2019-01-31 - 2021-01-31 # train records 133811 ,# val records 56439\n",
      "R2OOS Linear Regression:  0.029904217893433227\n"
     ]
    }
   ],
   "source": [
    "tscv = TimeBasedCV(train_period=60,\n",
    "                   val_period=24,\n",
    "                #    test_period=12,\n",
    "                   freq='months')\n",
    "\n",
    "features = df.columns[~df.columns.isin(['permno', 'permno2', 'mvel12', 'DATE2','macro_tbl', 'macro_svar','risk_premium'])].tolist()\n",
    "\n",
    "X = df[features]\n",
    "y = df[['risk_premium']]\n",
    "\n",
    "X_train = sm.add_constant(X)\n",
    "\n",
    "predictions = []\n",
    "y_val_list = []\n",
    "dates = []\n",
    "r2_train_list = []\n",
    "r2_val_list = []\n",
    "dic_r2_all = {}\n",
    "\n",
    "for train_index, val_index in tscv.split(X, first_split_date= datetime.date(1985,1,31), second_split_date= datetime.date(1987,1,31)):\n",
    "\n",
    "    X_train   = X.loc[train_index].drop('DATE', axis=1)\n",
    "    y_train = y.loc[train_index]\n",
    "\n",
    "    X_val   = X.loc[val_index].drop('DATE', axis=1)\n",
    "    y_val = y.loc[val_index]\n",
    "    lm = LinearRegression()\n",
    "   \n",
    "    lm.fit(X_train, y_train)\n",
    "    preds = lm.predict(X_val) \n",
    "    #Save predictions, dates and the true values of the dependent variable to list  \n",
    "    predictions.append(preds)\n",
    "    dates.append(y_val.index)\n",
    "    y_val_list.append(y_val)\n",
    "    \n",
    "    #Calculate OOS model performance the for current window\n",
    "    r2 = 1-np.sum(pow(y_val-preds,2))/np.sum(pow(y_val,2))\n",
    "    #Save OOS model performance and the respective month to dictionary\n",
    "    dic_r2_all[\"r2.\" + str(y_val.index)] = r2\n",
    "\n",
    "predictions_all_full = np.concatenate(predictions, axis=0)\n",
    "y_test_list_all_full = np.concatenate(y_val_list, axis=0)\n",
    "dates_all_full = np.concatenate(dates, axis=0)\n",
    "R2FULL = 1-np.sum(pow(y_test_list_all_full-predictions_all_full,2))/np.sum(pow(y_test_list_all_full,2))\n",
    "print(\"R2OOS Linear Regression: \", R2FULL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train period: 1980-01-31 - 1985-01-31 ,val period: 1985-01-31 - 1987-01-31 # train records 5549 ,# val records 3377\n",
      "Train period: 1981-01-31 - 1986-01-31 ,val period: 1986-01-31 - 1988-01-31 # train records 6012 ,# val records 3906\n",
      "Train period: 1982-01-31 - 1987-01-31 ,val period: 1987-01-31 - 1989-01-31 # train records 6863 ,# val records 3764\n",
      "Train period: 1983-01-31 - 1988-01-31 ,val period: 1988-01-31 - 1990-01-31 # train records 7988 ,# val records 3826\n",
      "Train period: 1984-01-31 - 1989-01-31 ,val period: 1989-01-31 - 1991-01-31 # train records 8445 ,# val records 4008\n",
      "Train period: 1985-01-31 - 1990-01-31 ,val period: 1990-01-31 - 1992-01-31 # train records 9190 ,# val records 4072\n",
      "Train period: 1986-01-31 - 1991-01-31 ,val period: 1991-01-31 - 1993-01-31 # train records 9691 ,# val records 4732\n",
      "Train period: 1987-01-31 - 1992-01-31 ,val period: 1992-01-31 - 1994-01-31 # train records 9885 ,# val records 5202\n",
      "Train period: 1988-01-31 - 1993-01-31 ,val period: 1993-01-31 - 1995-01-31 # train records 10517 ,# val records 5634\n",
      "Train period: 1989-01-31 - 1994-01-31 ,val period: 1994-01-31 - 1996-01-31 # train records 11323 ,# val records 6399\n",
      "Train period: 1990-01-31 - 1995-01-31 ,val period: 1995-01-31 - 1997-01-31 # train records 12325 ,# val records 7227\n",
      "Train period: 1991-01-31 - 1996-01-31 ,val period: 1996-01-31 - 1998-01-31 # train records 13714 ,# val records 8676\n",
      "Train period: 1992-01-31 - 1997-01-31 ,val period: 1997-01-31 - 1999-01-31 # train records 15480 ,# val records 9577\n",
      "Train period: 1993-01-31 - 1998-01-31 ,val period: 1998-01-31 - 2000-01-31 # train records 17658 ,# val records 9231\n",
      "Train period: 1994-01-31 - 1999-01-31 ,val period: 1999-01-31 - 2001-01-31 # train records 19855 ,# val records 9962\n",
      "Train period: 1995-01-31 - 2000-01-31 ,val period: 2000-01-31 - 2002-01-31 # train records 21255 ,# val records 10376\n",
      "Train period: 1996-01-31 - 2001-01-31 ,val period: 2001-01-31 - 2003-01-31 # train records 23418 ,# val records 9496\n",
      "Train period: 1997-01-31 - 2002-01-31 ,val period: 2002-01-31 - 2004-01-31 # train records 24404 ,# val records 9918\n",
      "Train period: 1998-01-31 - 2003-01-31 ,val period: 2003-01-31 - 2005-01-31 # train records 24238 ,# val records 10677\n",
      "Train period: 1999-01-31 - 2004-01-31 ,val period: 2004-01-31 - 2006-01-31 # train records 24745 ,# val records 11862\n",
      "Train period: 2000-01-31 - 2005-01-31 ,val period: 2005-01-31 - 2007-01-31 # train records 25684 ,# val records 13523\n",
      "Train period: 2001-01-31 - 2006-01-31 ,val period: 2006-01-31 - 2008-01-31 # train records 26645 ,# val records 14386\n",
      "Train period: 2002-01-31 - 2007-01-31 ,val period: 2007-01-31 - 2009-01-31 # train records 28831 ,# val records 13913\n",
      "Train period: 2003-01-31 - 2008-01-31 ,val period: 2008-01-31 - 2010-01-31 # train records 31535 ,# val records 11601\n",
      "Train period: 2004-01-31 - 2009-01-31 ,val period: 2009-01-31 - 2011-01-31 # train records 32826 ,# val records 10469\n",
      "Train period: 2005-01-31 - 2010-01-31 ,val period: 2010-01-31 - 2012-01-31 # train records 32459 ,# val records 11915\n",
      "Train period: 2006-01-31 - 2011-01-31 ,val period: 2011-01-31 - 2013-01-31 # train records 31433 ,# val records 12919\n",
      "Train period: 2007-01-31 - 2012-01-31 ,val period: 2012-01-31 - 2014-01-31 # train records 30851 ,# val records 13608\n",
      "Train period: 2008-01-31 - 2013-01-31 ,val period: 2013-01-31 - 2015-01-31 # train records 29966 ,# val records 15743\n",
      "Train period: 2009-01-31 - 2014-01-31 ,val period: 2014-01-31 - 2016-01-31 # train records 30546 ,# val records 16519\n",
      "Train period: 2010-01-31 - 2015-01-31 ,val period: 2015-01-31 - 2017-01-31 # train records 34108 ,# val records 14886\n",
      "Train period: 2011-01-31 - 2016-01-31 ,val period: 2016-01-31 - 2018-01-31 # train records 36596 ,# val records 15078\n",
      "Train period: 2012-01-31 - 2017-01-31 ,val period: 2017-01-31 - 2019-01-31 # train records 37079 ,# val records 16647\n",
      "Train period: 2013-01-31 - 2018-01-31 ,val period: 2018-01-31 - 2020-01-31 # train records 38755 ,# val records 16696\n",
      "Train period: 2014-01-31 - 2019-01-31 ,val period: 2019-01-31 - 2021-01-31 # train records 40118 ,# val records 16920\n",
      "R2OOS Linear Regression:  0.03986620903015137\n"
     ]
    }
   ],
   "source": [
    "tscv = TimeBasedCV(train_period=60,\n",
    "                   val_period=24,\n",
    "                   freq='months')\n",
    "\n",
    "features = df.columns[~df.columns.isin(['permno', 'permno2', 'mvel12', 'DATE2','macro_tbl', 'macro_svar','risk_premium'])].tolist()\n",
    "\n",
    "X = df_top[features]\n",
    "y = df_top[['risk_premium']]\n",
    "\n",
    "X_train = sm.add_constant(X)\n",
    "\n",
    "predictions_top = []\n",
    "y_test_list_top =[]\n",
    "dates_top = []\n",
    "dic_r2_all_top = {}\n",
    "\n",
    "for train_index, val_index in tscv.split(X, first_split_date= datetime.date(1985,1,31), second_split_date= datetime.date(1987,1,31)):\n",
    "\n",
    "    X_train   = X.loc[train_index].drop('DATE', axis=1)\n",
    "    y_train = y.loc[train_index]\n",
    "\n",
    "    X_val   = X.loc[val_index].drop('DATE', axis=1)\n",
    "    y_val = y.loc[val_index]\n",
    "    lm = LinearRegression()\n",
    "    lm.fit(X_train, y_train)\n",
    "\n",
    "    preds = lm.predict(X_val)    \n",
    "    predictions_top.append(preds)\n",
    "    dates_top.append(y_val.index)\n",
    "    y_test_list_top.append(y_val)\n",
    "\n",
    "    r2 = 1-np.sum(pow(y_val-preds,2))/np.sum(pow(y_val,2))\n",
    "    dic_r2_all_top[\"r2.\" + str(y_val.index)] = r2\n",
    "\n",
    "predictions_all_top= np.concatenate(predictions_top, axis=0)\n",
    "y_test_list_all_top= np.concatenate(y_test_list_top, axis=0) \n",
    "dates_all_top= np.concatenate(dates_top, axis=0)\n",
    "\n",
    "#Calculate OOS model performance over the entire test period in line with Gu et al (2020)\n",
    "R2TOP = 1-np.sum(pow(y_test_list_all_top-predictions_all_top,2))/np.sum(pow(y_test_list_all_top,2))\n",
    "print(\"R2OOS Linear Regression: \", R2TOP)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train period: 1980-01-31 - 1985-01-31 ,val period: 1985-01-31 - 1987-01-31 # train records 5549 ,# val records 3377\n",
      "Train period: 1981-01-31 - 1986-01-31 ,val period: 1986-01-31 - 1988-01-31 # train records 6012 ,# val records 3906\n",
      "Train period: 1982-01-31 - 1987-01-31 ,val period: 1987-01-31 - 1989-01-31 # train records 6863 ,# val records 3764\n",
      "Train period: 1983-01-31 - 1988-01-31 ,val period: 1988-01-31 - 1990-01-31 # train records 7988 ,# val records 3826\n",
      "Train period: 1984-01-31 - 1989-01-31 ,val period: 1989-01-31 - 1991-01-31 # train records 8445 ,# val records 4008\n",
      "Train period: 1985-01-31 - 1990-01-31 ,val period: 1990-01-31 - 1992-01-31 # train records 9190 ,# val records 4072\n",
      "Train period: 1986-01-31 - 1991-01-31 ,val period: 1991-01-31 - 1993-01-31 # train records 9691 ,# val records 4732\n",
      "Train period: 1987-01-31 - 1992-01-31 ,val period: 1992-01-31 - 1994-01-31 # train records 9885 ,# val records 5202\n",
      "Train period: 1988-01-31 - 1993-01-31 ,val period: 1993-01-31 - 1995-01-31 # train records 10517 ,# val records 5634\n",
      "Train period: 1989-01-31 - 1994-01-31 ,val period: 1994-01-31 - 1996-01-31 # train records 11323 ,# val records 6399\n",
      "Train period: 1990-01-31 - 1995-01-31 ,val period: 1995-01-31 - 1997-01-31 # train records 12325 ,# val records 7227\n",
      "Train period: 1991-01-31 - 1996-01-31 ,val period: 1996-01-31 - 1998-01-31 # train records 13714 ,# val records 8676\n",
      "Train period: 1992-01-31 - 1997-01-31 ,val period: 1997-01-31 - 1999-01-31 # train records 15480 ,# val records 9577\n",
      "Train period: 1993-01-31 - 1998-01-31 ,val period: 1998-01-31 - 2000-01-31 # train records 17658 ,# val records 9231\n",
      "Train period: 1994-01-31 - 1999-01-31 ,val period: 1999-01-31 - 2001-01-31 # train records 19855 ,# val records 9962\n",
      "Train period: 1995-01-31 - 2000-01-31 ,val period: 2000-01-31 - 2002-01-31 # train records 21255 ,# val records 10376\n",
      "Train period: 1996-01-31 - 2001-01-31 ,val period: 2001-01-31 - 2003-01-31 # train records 23418 ,# val records 9496\n",
      "Train period: 1997-01-31 - 2002-01-31 ,val period: 2002-01-31 - 2004-01-31 # train records 24404 ,# val records 9918\n",
      "Train period: 1998-01-31 - 2003-01-31 ,val period: 2003-01-31 - 2005-01-31 # train records 24238 ,# val records 10677\n",
      "Train period: 1999-01-31 - 2004-01-31 ,val period: 2004-01-31 - 2006-01-31 # train records 24745 ,# val records 11862\n",
      "Train period: 2000-01-31 - 2005-01-31 ,val period: 2005-01-31 - 2007-01-31 # train records 25684 ,# val records 13523\n",
      "Train period: 2001-01-31 - 2006-01-31 ,val period: 2006-01-31 - 2008-01-31 # train records 26645 ,# val records 14386\n",
      "Train period: 2002-01-31 - 2007-01-31 ,val period: 2007-01-31 - 2009-01-31 # train records 28831 ,# val records 13913\n",
      "Train period: 2003-01-31 - 2008-01-31 ,val period: 2008-01-31 - 2010-01-31 # train records 31535 ,# val records 11601\n",
      "Train period: 2004-01-31 - 2009-01-31 ,val period: 2009-01-31 - 2011-01-31 # train records 32826 ,# val records 10469\n",
      "Train period: 2005-01-31 - 2010-01-31 ,val period: 2010-01-31 - 2012-01-31 # train records 32459 ,# val records 11915\n",
      "Train period: 2006-01-31 - 2011-01-31 ,val period: 2011-01-31 - 2013-01-31 # train records 31433 ,# val records 12919\n",
      "Train period: 2007-01-31 - 2012-01-31 ,val period: 2012-01-31 - 2014-01-31 # train records 30851 ,# val records 13608\n",
      "Train period: 2008-01-31 - 2013-01-31 ,val period: 2013-01-31 - 2015-01-31 # train records 29966 ,# val records 15743\n",
      "Train period: 2009-01-31 - 2014-01-31 ,val period: 2014-01-31 - 2016-01-31 # train records 30546 ,# val records 16519\n",
      "Train period: 2010-01-31 - 2015-01-31 ,val period: 2015-01-31 - 2017-01-31 # train records 34108 ,# val records 14886\n",
      "Train period: 2011-01-31 - 2016-01-31 ,val period: 2016-01-31 - 2018-01-31 # train records 36596 ,# val records 15078\n",
      "Train period: 2012-01-31 - 2017-01-31 ,val period: 2017-01-31 - 2019-01-31 # train records 37079 ,# val records 16647\n",
      "Train period: 2013-01-31 - 2018-01-31 ,val period: 2018-01-31 - 2020-01-31 # train records 38755 ,# val records 16696\n",
      "Train period: 2014-01-31 - 2019-01-31 ,val period: 2019-01-31 - 2021-01-31 # train records 40118 ,# val records 16920\n",
      "R2OOS Linear Regression:  0.023747742176055908\n"
     ]
    }
   ],
   "source": [
    "tscv = TimeBasedCV(train_period=60,\n",
    "                   val_period=24,\n",
    "                   freq='months')\n",
    "\n",
    "\n",
    "features = df_7.columns[~df_7.columns.isin(['permno', 'permno2', 'mvel12', 'DATE2', 'risk_premium'])].tolist()\n",
    "X = df_bottom[features]\n",
    "y = df_bottom[\"risk_premium\"]\n",
    "\n",
    "predictions_bottom = []\n",
    "y_test_list_bottom =[]\n",
    "dates_bottom = []\n",
    "dic_r2_all_bottom = {}\n",
    "\n",
    "\n",
    "for train_index, val_index in tscv.split(X, first_split_date= datetime.date(1985,1,31), second_split_date= datetime.date(1987,1,31)):\n",
    "\n",
    "    X_train   = X.loc[train_index].drop('DATE', axis=1)\n",
    "    y_train = y.loc[train_index]\n",
    "\n",
    "    X_val   = X.loc[val_index].drop('DATE', axis=1)\n",
    "    y_val = y.loc[val_index]\n",
    "    lm = LinearRegression()\n",
    "    lm.fit(X_train, y_train)\n",
    "   \n",
    "    preds = lm.predict(X_val)    \n",
    "    predictions_bottom.append(preds)\n",
    "    dates_bottom.append(y_val.index)\n",
    "    y_test_list_bottom.append(y_val)\n",
    "\n",
    "    r2 = 1-np.sum(pow(y_val-preds,2))/np.sum(pow(y_val,2))\n",
    "    dic_r2_all_bottom[\"r2.\" + str(y_val.index)] = r2\n",
    "\n",
    "predictions_all_bottom= np.concatenate(predictions_bottom, axis=0)\n",
    "y_test_list_all_bottom= np.concatenate(y_test_list_bottom, axis=0) \n",
    "dates_all_bottom= np.concatenate(dates_bottom, axis=0)\n",
    "\n",
    "#Calculate OOS model performance over the entire test period in line with Gu et al (2020)\n",
    "R2BOTTOM = 1-np.sum(pow(y_test_list_all_bottom-predictions_all_bottom,2))/np.sum(pow(y_test_list_all_bottom,2))\n",
    "print(\"R2OOS Linear Regression: \", R2BOTTOM)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LR-7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Full Sample</th>\n",
       "      <td>0.029904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Large Firms</th>\n",
       "      <td>0.039866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Small Firms</th>\n",
       "      <td>0.023748</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 LR-7\n",
       "Full Sample  0.029904\n",
       "Large Firms  0.039866\n",
       "Small Firms  0.023748"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chart = np.array([[R2FULL],\n",
    "                  [R2TOP],\n",
    "                  [R2BOTTOM]])\n",
    "\n",
    "r2_lm = pd.DataFrame(chart, columns=['LR-7'],\n",
    "                     index=['Full Sample', 'Large Firms', 'Small Firms'])\n",
    "\n",
    "r2_lm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_lm.to_csv(r'r2_lr-7.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df.columns[~df.columns.isin(['permno','permno','DATE','DATE2','mvel1','risk_premium', 'year'])].tolist()\n",
    "df['YEAR'] = df['DATE'].dt.year\n",
    "\n",
    "X_train = df[features].loc[(df['YEAR']>=2012) & (df['YEAR']<=2016)]\n",
    "y_train = df['risk_premium'].loc[(df['YEAR']>=2012) & (df['YEAR']<=2016)]\n",
    "\n",
    "X_val = df[features].loc[(df['YEAR']>=2017) & (df['YEAR']<=2018)]\n",
    "y_val = df['risk_premium'].loc[(df['YEAR']>=2017) & (df['YEAR']<=2018)]\n",
    "\n",
    "\n",
    "lm_model = LinearRegression()\n",
    "lm_model.fit(X_train, y_train)\n",
    "y_pred_train = lm_model.predict(X_train) \n",
    "\n",
    "y_pred_val = lm_model.predict(X_val) \n",
    "\n",
    "r2_score_train = r2_score(y_train, y_pred_train)\n",
    "r2_score_val = r2_score(y_val, y_pred_val)\n",
    "\n",
    "print(f'R2 score on training set: {r2_score_train}')\n",
    "print(f'R2 score on validation set: {r2_score_val}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in features:\n",
    "    globals()['df_' + str(j)] =  df.copy()\n",
    "    globals()['df_' + str(j)][str(j)] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic = {}\n",
    "\n",
    "    \n",
    "for j in features:\n",
    "    df_var = globals()['df_' + str(j)]\n",
    "    \n",
    "    X_train = df_var[features].loc[(df_var[\"year\"]>=2012) & (df_var[\"year\"]<=2016)]\n",
    "    y_train = df_var['risk_premium'].loc[(df_var[\"year\"]>=2012) & (df_var[\"year\"]<=2016)]\n",
    "\n",
    "    X_val = df_var[features].loc[(df_var[\"year\"]>=2017) & (df_var[\"year\"]<=2018)]\n",
    "    y_val = df_var['risk_premium'].loc[(df_var[\"year\"]>=2017) & (df_var[\"year\"]<=2018)]\n",
    "    \n",
    "    lm_model = LinearRegression()\n",
    "    lm_model.fit(X_train, y_train)\n",
    "\n",
    "    y_pred_train = lm_model.predict(X_train) \n",
    "\n",
    "    y_pred_val = lm_model.predict(X_val) \n",
    "\n",
    "    r2_score_train = r2_score(y_train, y_pred_train)\n",
    "    r2_score_val = r2_score(y_val, y_pred_val)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>yhat</th>\n",
       "      <th>y_true</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MonthYear</th>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">1985-01</th>\n",
       "      <th>10137</th>\n",
       "      <td>[-9.773998118885068]</td>\n",
       "      <td>[-6.745699882507324]</td>\n",
       "      <td>1985-01-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10145</th>\n",
       "      <td>[-10.34263499131986]</td>\n",
       "      <td>[4.993500232696533]</td>\n",
       "      <td>1985-01-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10161</th>\n",
       "      <td>[-9.628691139371131]</td>\n",
       "      <td>[8.103799819946289]</td>\n",
       "      <td>1985-01-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10225</th>\n",
       "      <td>[-9.931275029792303]</td>\n",
       "      <td>[-8.828200340270996]</td>\n",
       "      <td>1985-01-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10233</th>\n",
       "      <td>[-10.07605669679902]</td>\n",
       "      <td>[-6.663899898529053]</td>\n",
       "      <td>1985-01-31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 yhat                y_true       date\n",
       "MonthYear id                                                          \n",
       "1985-01   10137  [-9.773998118885068]  [-6.745699882507324] 1985-01-31\n",
       "          10145  [-10.34263499131986]   [4.993500232696533] 1985-01-31\n",
       "          10161  [-9.628691139371131]   [8.103799819946289] 1985-01-31\n",
       "          10225  [-9.931275029792303]  [-8.828200340270996] 1985-01-31\n",
       "          10233  [-10.07605669679902]  [-6.663899898529053] 1985-01-31"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yhat = predictions_all_full.tolist()\n",
    "y_true = y_test_list_all_full.tolist()\n",
    "i = dates_all_full.tolist()\n",
    "\n",
    "results = pd.DataFrame(\n",
    "    {'identifier': i,\n",
    "     'yhat': yhat,\n",
    "     'y_true': y_true\n",
    "    })\n",
    "\n",
    "\n",
    "results[\"identifier\"]= results[\"identifier\"].astype(\"str\")\n",
    "results[\"date\"] = results[\"identifier\"].str[12:22]\n",
    "results[\"id\"] = results[\"identifier\"].str[35:40]\n",
    "results.drop([\"identifier\"],axis = 1, inplace=True)\n",
    "results['date'] = pd.to_datetime(results['date'], format='%Y-%m-%d')\n",
    "results['MonthYear'] = results['date'].dt.to_period('M')\n",
    "results = results.sort_values(by = ['date', 'id'], ascending = True)\n",
    "results = results.set_index(['MonthYear','id'])\n",
    "results.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "results['yhat'] = results['yhat'].apply(lambda x: x[0])\n",
    "results['y_true'] = results['y_true'].apply(lambda x: x[0])\n",
    "results.to_csv('results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['mvel12'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m data \u001b[38;5;241m=\u001b[39m df[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmvel12\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmacro_tbl\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmacro_svar\u001b[39m\u001b[38;5;124m'\u001b[39m]]\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m      2\u001b[0m data\u001b[38;5;241m.\u001b[39mreset_index(inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      3\u001b[0m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpermno2\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpermno2\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstr\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\drebi\\miniconda3\\envs\\statclass\\Lib\\site-packages\\pandas\\core\\frame.py:4108\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4106\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[0;32m   4107\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[1;32m-> 4108\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39m_get_indexer_strict(key, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m   4110\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[0;32m   4111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\drebi\\miniconda3\\envs\\statclass\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6200\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[1;34m(self, key, axis_name)\u001b[0m\n\u001b[0;32m   6197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   6198\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[1;32m-> 6200\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_if_missing(keyarr, indexer, axis_name)\n\u001b[0;32m   6202\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[0;32m   6203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[0;32m   6204\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\drebi\\miniconda3\\envs\\statclass\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6252\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[1;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[0;32m   6249\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   6251\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[1;32m-> 6252\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['mvel12'] not in index\""
     ]
    }
   ],
   "source": [
    "data = df[['mvel12', 'macro_tbl', 'macro_svar']].copy()\n",
    "data.reset_index(inplace=True)\n",
    "data['permno2'] = data['permno2'].astype('str')\n",
    "data['MonthYear'] = data['DATE2'].dt.to_period('M')\n",
    "data.drop('DATE2', axis=1, inplace=True)\n",
    "data.rename(columns={'permno2': 'id'}, inplace=True)\n",
    "data.rename(columns={'mvel12': 'market_cap'}, inplace=True)\n",
    "data.rename(columns={'macro_tbl': 'risk_free_rate'}, inplace=True)\n",
    "data = data.set_index(['MonthYear','id'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigdata = pd.merge(results, data,left_index=True, right_index=True)\n",
    "bigdata.reset_index(inplace=True)\n",
    "bigdata['returns'] = bigdata['y_true'] + bigdata['risk_free_rate']\n",
    "bigdata['MonthYear1'] = bigdata['MonthYear'].copy()\n",
    "bigdata['MonthYear'] = bigdata['MonthYear'].astype('int64')\n",
    "bigdata['NumMonth'] = bigdata['MonthYear'] - 251\n",
    "bigdata['NumMonth'].unique()\n",
    "bigdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in bigdata['NumMonth'].unique():\n",
    "    globals()['df_' + str(i)] = bigdata[bigdata['NumMonth'] == i]\n",
    "\n",
    "for i in bigdata[\"NumMonth\"].unique():\n",
    "    globals()['df_' + str(i)][\"rank\"]= globals()['df_' + str(i)]['yhat'].rank(method='first')\n",
    "    \n",
    "for i in bigdata[\"NumMonth\"].unique():\n",
    "    globals()['df_' + str(i)][\"DecileRank\"]=pd.qcut(globals()['df_' + str(i)]['rank'].values, 10, labels = False)\n",
    "\n",
    "#Drop normal rank, retain only decile ranks \n",
    "for i in bigdata[\"NumMonth\"].unique():\n",
    "     globals()['df_' + str(i)].drop('rank', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in bigdata[\"NumMonth\"].unique():\n",
    "    for j,g in globals()['df_' + str(i)].groupby('DecileRank'):\n",
    "        globals()['df_' + str(i)+ \"_\" + str(j)] =  g\n",
    "\n",
    "for j in np.arange(0,10,1):\n",
    "    globals()['rank_' + str(j)] = pd.concat([globals()['df_1_'+ str(j)], globals()['df_2_'+ str(j)]], axis=0)\n",
    "    \n",
    "# Generate 10 Dataframes for the 10 Decile portfolios 0-9: rank_9: top portfolio, rank_0: bottom portfolio\n",
    "for i in np.arange(2,361,1):\n",
    "    for j in np.arange(0,10,1):\n",
    "        globals()['rank_' + str(j)] = pd.concat([globals()['rank_' + str(j)], globals()['df_' + str(i+1)+ \"_\" + str(j)]], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_0['res'] = abs(rank_0['y_true'] - rank_0['yhat'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(rank_0.groupby('MonthYear')['yhat'].mean()/rank_9.groupby('MonthYear')['returns'].std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(rank_9.groupby('NumMonth')['yhat'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(rank_9.groupby('MonthYear')['y_true'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get equal und value weights per stock per month in each decile portfolio:\n",
    "# ew = Equally weighted\n",
    "# vw = Value weighted\n",
    "\n",
    "for j in np.arange(0,10,1):\n",
    "    globals()['rank_' + str(j)][\"eq_weights\"] = 1/globals()['rank_' + str(j)].groupby('MonthYear')[\"id\"].transform('size')\n",
    "    globals()['rank_' + str(j)][\"me_weights\"] = globals()['rank_' + str(j)][\"market_cap\"]/globals()['rank_' + str(j)].groupby('MonthYear')[\"market_cap\"].transform('sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Weighted excess return per stock in t+1\n",
    "for j in np.arange(0,10,1):\n",
    "    globals()['rank_' + str(j)]['excess_return_stock_ew'] = globals()['rank_' + str(j)][\"y_true\"]*globals()['rank_' + str(j)][\"eq_weights\"]\n",
    "    globals()['rank_' + str(j)]['excess_return_stock_vw'] = globals()['rank_' + str(j)][\"y_true\"]*globals()['rank_' + str(j)][\"me_weights\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# weighted return per stock in t+1 (to use for the sharpe ratio)\n",
    "for j in np.arange(0,10,1):\n",
    "    globals()['rank_' + str(j)]['return_stock_ew'] = globals()['rank_' + str(j)][\"returns\"]*globals()['rank_' + str(j)][\"eq_weights\"]\n",
    "    globals()['rank_' + str(j)]['return_stock_vw'] = globals()['rank_' + str(j)][\"returns\"]*globals()['rank_' + str(j)][\"me_weights\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Portfolio excess return in t+1\n",
    "for j in np.arange(0,10,1):\n",
    "    globals()['rank_' + str(j)]['excess_return_portfolio_ew'] = globals()['rank_' + str(j)].groupby('MonthYear')[\"excess_return_stock_ew\"].transform('sum')\n",
    "    globals()['rank_' + str(j)]['excess_return_portfolio_vw'] = globals()['rank_' + str(j)].groupby('MonthYear')[\"excess_return_stock_vw\"].transform('sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Portfolio return in t+1 (to use for the sharpe ratio) \n",
    "for j in np.arange(0,10,1):\n",
    "    globals()['rank_' + str(j)]['return_portfolio_ew'] = globals()['rank_' + str(j)].groupby('MonthYear')[\"return_stock_ew\"].transform('sum')\n",
    "    globals()['rank_' + str(j)]['return_portfolio_vw'] = globals()['rank_' + str(j)].groupby('MonthYear')[\"return_stock_vw\"].transform('sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Weighted predicted excess return per stock in t+1\n",
    "for j in np.arange(0,10,1):\n",
    "    globals()['rank_' + str(j)]['pred_excess_return_stock_ew'] = globals()['rank_' + str(j)][\"yhat\"]*globals()['rank_' + str(j)][\"eq_weights\"]\n",
    "    globals()['rank_' + str(j)]['pred_excess_return_stock_vw'] = globals()['rank_' + str(j)][\"yhat\"]*globals()['rank_' + str(j)][\"me_weights\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Portfolio predicted excess return in t+1\n",
    "for j in np.arange(0,10,1):\n",
    "    globals()['rank_' + str(j)]['pred_excess_return_portfolio_ew'] = globals()['rank_' + str(j)].groupby('MonthYear')[\"pred_excess_return_stock_ew\"].transform('sum')\n",
    "    globals()['rank_' + str(j)]['pred_excess_return_portfolio_vw'] = globals()['rank_' + str(j)].groupby('MonthYear')[\"pred_excess_return_stock_vw\"].transform('sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate dataframes, containing the portfolio returns on mohtly basis for each decile portfolio\n",
    "# e.g., montly_rank_0: dataframe, containing only the monthly portfolio excess returns (predicted and real) \n",
    "# for  the bottom rank\n",
    "\n",
    "for j in np.arange(0,10,1):\n",
    "    globals()['montly_rank_' + str(j)] = globals()['rank_' + str(j)][[\"MonthYear1\", \"DecileRank\",\n",
    "                                                                      \"excess_return_portfolio_ew\",\n",
    "                                                                      \"excess_return_portfolio_vw\", \n",
    "                                                                      \"pred_excess_return_portfolio_ew\",\n",
    "                                                                      \"pred_excess_return_portfolio_vw\",\n",
    "                                                                      \"return_portfolio_ew\",\n",
    "                                                                      \"return_portfolio_vw\"]]\n",
    "    \n",
    "for j in np.arange(0,10,1):\n",
    "    globals()['montly_rank_' + str(j)]=globals()['montly_rank_' + str(j)].drop_duplicates()\n",
    "    globals()['montly_rank_' + str(j)]=globals()['montly_rank_' + str(j)].set_index(\"MonthYear1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for j in np.arange(0,10,1):\n",
    "    #Time-series average of realized excess returns\n",
    "    globals()[\"ew_mean_return_rank_\" +  str(j)]= globals()['montly_rank_' + str(j)][\"excess_return_portfolio_ew\"].mean()\n",
    "    globals()[\"vw_mean_return_rank_\" +  str(j)]= globals()['montly_rank_' + str(j)][\"excess_return_portfolio_vw\"].mean()\n",
    "    #Time-series average of predicted excess returns\n",
    "    globals()[\"ew_mean_pred_return_rank_\" +  str(j)]= globals()['montly_rank_' + str(j)][\"pred_excess_return_portfolio_ew\"].mean()\n",
    "    globals()[\"vw_mean_pred_return_rank_\" +  str(j)]= globals()['montly_rank_' + str(j)][\"pred_excess_return_portfolio_vw\"].mean()\n",
    "    #Standard deviation of realized excess returns\n",
    "    globals()[\"std_ew_rank_\" +  str(j)]= globals()['montly_rank_' + str(j)][\"excess_return_portfolio_ew\"].std()\n",
    "    globals()[\"std_vw_rank_\" +  str(j)]= globals()['montly_rank_' + str(j)][\"excess_return_portfolio_vw\"].std()\n",
    "    #Annualized sharpe ratio of realized excess returns\n",
    "    globals()[\"sharpe_ew_rank_\" +  str(j)]= (globals()['montly_rank_' + str(j)][\"excess_return_portfolio_ew\"].mean()/globals()['montly_rank_' + str(j)][\"return_portfolio_ew\"].std())* np.sqrt(12)\n",
    "    globals()[\"sharpe_vw_rank_\" +  str(j)]= (globals()['montly_rank_' + str(j)][\"excess_return_portfolio_vw\"].mean()/globals()['montly_rank_' + str(j)][\"return_portfolio_vw\"].std())* np.sqrt(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the zero-net-investment long-short portfolio the top (long) and bottom(short) decile portfolios are needed\n",
    "\n",
    "long_monthly = rank_9[[\"NumMonth\",\"MonthYear1\", \"DecileRank\", \"excess_return_portfolio_ew\",\n",
    "                       \"excess_return_portfolio_vw\",\"pred_excess_return_portfolio_ew\",\n",
    "                       \"pred_excess_return_portfolio_vw\",\"return_portfolio_ew\",\n",
    "                        \"return_portfolio_vw\"]].drop_duplicates()\n",
    "\n",
    "short_monthly = rank_0[[\"NumMonth\",\"MonthYear1\", \"DecileRank\", \"excess_return_portfolio_ew\",\n",
    "                       \"excess_return_portfolio_vw\",\"pred_excess_return_portfolio_ew\",\n",
    "                       \"pred_excess_return_portfolio_vw\",\"return_portfolio_ew\",\n",
    "                        \"return_portfolio_vw\"]].drop_duplicates()\n",
    "\n",
    "# Create a column, indication the stategy \n",
    "long_monthly[\"Strategy\"]= \"long\"\n",
    "short_monthly[\"Strategy\"]= \"short\"\n",
    "\n",
    "# Merge to get the zero net investment portfolio\n",
    "zeronet_monthly= pd.concat([long_monthly, short_monthly])\n",
    "zeronet_monthly = zeronet_monthly.sort_values(by = ['NumMonth',\"Strategy\"])\n",
    "zeronet_monthly[\"return_portfolio_vw\"] = zeronet_monthly[\"return_portfolio_vw\"].astype('float64')\n",
    "\n",
    "#Create two new columns containing the exess return of the portfolio and initially set the values to zero.\n",
    "zeronet_monthly[\"excess_return_zeronet_ew\"] =0\n",
    "zeronet_monthly[\"excess_return_zeronet_vw\"] =0\n",
    "\n",
    "# excess return zeronet in t = (weigted excess return long in t) - (weigted excess return short in t)\n",
    "for i in range(0, len(zeronet_monthly)):\n",
    "    if zeronet_monthly.iloc[i,9] == \"long\":\n",
    "        zeronet_monthly.iloc[i, -2] = zeronet_monthly.iloc[i, 3]-zeronet_monthly.iloc[i+1, 3]\n",
    "    else:\n",
    "        zeronet_monthly.iloc[i, -2] = zeronet_monthly.iloc[i-1, 3]-zeronet_monthly.iloc[i, 3]\n",
    "        \n",
    "for i in range(0, len(zeronet_monthly)):\n",
    "    if zeronet_monthly.iloc[i,9] == \"long\":\n",
    "        zeronet_monthly.iloc[i, -1] = zeronet_monthly.iloc[i, 4]-zeronet_monthly.iloc[i+1, 4]\n",
    "    else:\n",
    "        zeronet_monthly.iloc[i, -1] = zeronet_monthly.iloc[i-1, 4]-zeronet_monthly.iloc[i, 4]\n",
    "\n",
    "#Create two new columns containing predicted the exess return of the portfolio and initially set the values to zero.\n",
    "zeronet_monthly[\"pred_excess_return_zeronet_ew\"] =0\n",
    "zeronet_monthly[\"pred_excess_return_zeronet_vw\"] =0\n",
    "\n",
    "# predicted excess return zeronet in t = (weigted predicted excess return long in t) - (weigted predicted excess return short in t)\n",
    "for i in range(0, len(zeronet_monthly)):\n",
    "    if zeronet_monthly.iloc[i,9] == \"long\":\n",
    "        zeronet_monthly.iloc[i, -2] = zeronet_monthly.iloc[i, 5]-zeronet_monthly.iloc[i+1, 5]\n",
    "    else:\n",
    "        zeronet_monthly.iloc[i, -2] = zeronet_monthly.iloc[i-1, 5]-zeronet_monthly.iloc[i, 5]\n",
    "        \n",
    "for i in range(0, len(zeronet_monthly)):\n",
    "    if zeronet_monthly.iloc[i,9] == \"long\":\n",
    "        zeronet_monthly.iloc[i, -1] = zeronet_monthly.iloc[i, 6]-zeronet_monthly.iloc[i+1, 6]\n",
    "    else:\n",
    "        zeronet_monthly.iloc[i, -1] = zeronet_monthly.iloc[i-1, 6]-zeronet_monthly.iloc[i, 6]\n",
    "\n",
    "#Create two new columns containing return of the portfolio and initially set the values to zero.       \n",
    "zeronet_monthly[\"return_zeronet_ew\"] =0\n",
    "zeronet_monthly[\"return_zeronet_vw\"] =0\n",
    "\n",
    "# return zeronet in t = (weigted return long in t) - (weigted return short in t)\n",
    "for i in range(0, len(zeronet_monthly)):\n",
    "    if zeronet_monthly.iloc[i,9] == \"long\":\n",
    "        zeronet_monthly.iloc[i, -2] = zeronet_monthly.iloc[i, 7]-zeronet_monthly.iloc[i+1, 7]\n",
    "    else:\n",
    "        zeronet_monthly.iloc[i, -2] = zeronet_monthly.iloc[i-1, 7]-zeronet_monthly.iloc[i, 7]\n",
    "        \n",
    "for i in range(0, len(zeronet_monthly)):\n",
    "    if zeronet_monthly.iloc[i,9] == \"long\":\n",
    "        zeronet_monthly.iloc[i, -1] = zeronet_monthly.iloc[i, 8]-zeronet_monthly.iloc[i+1, 8]\n",
    "    else:\n",
    "        zeronet_monthly.iloc[i, -1] = zeronet_monthly.iloc[i-1, 8]-zeronet_monthly.iloc[i, 8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zeronet_monthly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Only the measures at portfolio level are needed\n",
    "zeronet_monthly = zeronet_monthly[['NumMonth', 'MonthYear1', 'excess_return_zeronet_ew',\n",
    "                                   'excess_return_zeronet_vw', 'pred_excess_return_zeronet_ew',\n",
    "                                   'pred_excess_return_zeronet_vw','return_zeronet_ew',\n",
    "                                   'return_zeronet_vw']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Calculate zero-net portfolio performance measures                                            \n",
    "#Time-series average of realized excess returns                                             \n",
    "ew_mean_return_zeronet= zeronet_monthly[\"excess_return_zeronet_ew\"].mean()\n",
    "vw_mean_return_zeronet= zeronet_monthly[\"excess_return_zeronet_vw\"].mean()\n",
    "#Time-series average of predicted excess returns\n",
    "ew_mean_pred_return_zeronet = zeronet_monthly[\"pred_excess_return_zeronet_ew\"].mean()\n",
    "vw_mean_pred_return_zeronet = zeronet_monthly[\"pred_excess_return_zeronet_vw\"].mean()\n",
    "#Standard deviation of realized excess returns\n",
    "std_ew_zeronet = zeronet_monthly[\"excess_return_zeronet_ew\"].std()\n",
    "std_vw_zeronet = zeronet_monthly[\"excess_return_zeronet_vw\"].std()\n",
    "#Annualized sharpe ratio of realized excess returns\n",
    "sharpe_ew_zeronet = (zeronet_monthly[\"excess_return_zeronet_ew\"].mean()/zeronet_monthly[\"return_zeronet_ew\"].std())* np.sqrt(12)\n",
    "sharpe_vw_zeronet = (zeronet_monthly[\"excess_return_zeronet_vw\"].mean()/zeronet_monthly[\"return_zeronet_vw\"].std())* np.sqrt(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chart_np = np.array([[ew_mean_pred_return_rank_0, ew_mean_return_rank_0, std_ew_rank_0, sharpe_ew_rank_0],\n",
    "                     [ew_mean_pred_return_rank_1, ew_mean_return_rank_1, std_ew_rank_1, sharpe_ew_rank_1],\n",
    "                     [ew_mean_pred_return_rank_2, ew_mean_return_rank_2, std_ew_rank_2, sharpe_ew_rank_2],\n",
    "                     [ew_mean_pred_return_rank_3, ew_mean_return_rank_3, std_ew_rank_3, sharpe_ew_rank_3],\n",
    "                     [ew_mean_pred_return_rank_4, ew_mean_return_rank_4, std_ew_rank_4, sharpe_ew_rank_4],\n",
    "                     [ew_mean_pred_return_rank_5, ew_mean_return_rank_5, std_ew_rank_5, sharpe_ew_rank_5],\n",
    "                     [ew_mean_pred_return_rank_6, ew_mean_return_rank_6, std_ew_rank_6, sharpe_ew_rank_6],\n",
    "                     [ew_mean_pred_return_rank_7, ew_mean_return_rank_7, std_ew_rank_7, sharpe_ew_rank_7],\n",
    "                     [ew_mean_pred_return_rank_8, ew_mean_return_rank_8, std_ew_rank_8, sharpe_ew_rank_8],\n",
    "                     [ew_mean_pred_return_rank_9, ew_mean_return_rank_9, std_ew_rank_9, sharpe_ew_rank_9],\n",
    "                     [ew_mean_pred_return_zeronet, ew_mean_return_zeronet, std_ew_zeronet, sharpe_ew_zeronet]])\n",
    "\n",
    "ew_df = pd.DataFrame(chart_np, columns=['Pred', 'Real', 'Std', 'Sharpe'],\n",
    "                              index=['Low (L)', '2', '3', '4', '5','6','7','8',\"9\",'High (H)', \"H-L\"])\n",
    "\n",
    "ew_df['Pred'] = pd.Series([\"{0:.2f}%\".format(val) for val in ew_df['Pred']], index = ew_df.index)\n",
    "ew_df['Real'] = pd.Series([\"{0:.2f}%\".format(val) for val in ew_df['Real']], index = ew_df.index)\n",
    "ew_df['Std'] = pd.Series([\"{0:.2f}%\".format(val) for val in ew_df['Std']], index = ew_df.index)\n",
    "ew_df['Sharpe'] = pd.Series([(\"%.2f\" % round(val, 2)) for val in ew_df['Sharpe']], index = ew_df.index)\n",
    "ew_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "statclass",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
