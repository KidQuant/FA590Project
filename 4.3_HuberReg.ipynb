{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import sklearn.preprocessing\n",
    "import datetime\n",
    "from TimeBasedCV import TimeBasedCV\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "from sklearn.metrics import make_scorer, r2_score\n",
    "import statsmodels.api as sm\n",
    "import warnings\n",
    "import pickle\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=Warning)\n",
    "from sklearn.linear_model import HuberRegressor\n",
    "# pd.set_option('display.max_rows', None)\n",
    "# more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('data/factors_1970.csv', parse_dates=['DATE'])\n",
    "# df = pd.read_csv('data/features_subset.csv', parse_dates=['DATE'])\n",
    "# df = pd.read_csv('factors_1900.csv', parse_dates=['DATE'])\n",
    "# df.drop(columns=['sic2'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('data/features_1975.pkl', 'wb') as f:\n",
    "#     pickle.dump(df, f)\n",
    "\n",
    "with open('data/features_1975.pkl', 'rb') as f:\n",
    "    df = pickle.load(f)\n",
    "    print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sort observations by date and stock id\n",
    "df[df.columns[2:]] = df[df.columns[2:]].astype('float32')\n",
    "df = df.sort_values(by = ['DATE', 'permno'], ascending = True)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['permno2'] = df['permno'].copy()\n",
    "df['DATE2'] = df['DATE'].copy()\n",
    "df = df.set_index(['DATE2','permno2'])\n",
    "\n",
    "#Make a copy of  the \"me\" variable (market equity) before rank standartization to use afterwards for value weighting\n",
    "df['mvel12'] = df['mvel1'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p=0.3 \n",
    "df_top= df.groupby('DATE').apply(lambda x: x.nlargest(int(len(x)*p),'mvel1')).reset_index(drop=True)  \n",
    "df_bottom = df.groupby('DATE').apply(lambda x: x.nsmallest(int(len(x)*p),'mvel1')).reset_index(drop=True)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calculate_r2(y_true, y_pred, in_sample=True, benchmark=None):\n",
    "    if in_sample:\n",
    "        return 1 - (np.sum((y_true - y_pred) ** 2) / \n",
    "                    np.sum((y_true - np.mean(y_true)) ** 2))\n",
    "    else:\n",
    "        if benchmark is None:\n",
    "            raise ValueError(\"Benchmark must be provided for out-of-sample R-squared calculation.\")\n",
    "        return 1 - (np.sum((y_true - y_pred) ** 2) / \n",
    "                    np.sum((y_true - benchmark) ** 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df.columns[~df.columns.isin(['DATE', 'DATE2', \"mvel2\",'permno',\"permno2\",'risk_premium'])].tolist()\n",
    "df[features]=df.groupby('DATE')[features].rank(pct=True)\n",
    "\n",
    "df[features] = 2*df[features] - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tscv = TimeBasedCV(train_period=60,\n",
    "                   val_period=24,\n",
    "                #    test_period=12,\n",
    "                   freq='months')\n",
    "\n",
    "features = df.columns[~df.columns.isin(['permno', 'permno2', 'mvel12', 'DATE2', 'risk_premium'])].tolist()\n",
    "\n",
    "X = df[features]\n",
    "y = df[['risk_premium']]\n",
    "\n",
    "predictions = []\n",
    "y_val_list = []\n",
    "dates = []\n",
    "dic_r2_all = {}\n",
    "\n",
    "\n",
    "for train_index, val_index in tscv.split(X, first_split_date= datetime.date(1985,1,31), second_split_date= datetime.date(1987,1,31)):\n",
    "\n",
    "\n",
    "    X_train   = X.loc[train_index].drop('DATE', axis=1)\n",
    "    y_train = y.loc[train_index]\n",
    "\n",
    "    X_val   = X.loc[val_index].drop('DATE', axis=1)\n",
    "    y_val = y.loc[val_index]\n",
    "\n",
    "    huber = HuberRegressor(max_iter=1000)\n",
    "\n",
    "    # fitting the huber model\n",
    "    huber.fit(X_train, y_train)\n",
    "\n",
    "    # predicting on the training set and assiging to the training list\n",
    "    preds_train = huber.predict(X_train)\n",
    "\n",
    "    preds_val = huber.predict(X_val) \n",
    "    predictions.append(preds_val)\n",
    "    dates.append(y_val.index)\n",
    "    y_val_list.append(y_val)\n",
    "    \n",
    "    #Calculate OOS model performance the for current window\n",
    "    r2_train = 1-np.sum(pow(y_train['risk_premium']-preds_train,2))/np.sum(pow(y_train['risk_premium'],2))\n",
    "    r2_val = 1-np.sum(pow(y_val['risk_premium']-preds_val,2))/np.sum(pow(y_val['risk_premium'],2))\n",
    "    #Save OOS model performance and the respective month to dictionary\n",
    "    dic_r2_all[\"r2.\" + str(y_val['risk_premium'].index)] = r2_val\n",
    "    \n",
    "\n",
    "    print(f'R2 {y_train.index[0][0].date()} - {y_train.index[-1][0].date()} training set {r2_train}')\n",
    "    print(f'R2 {y_val.index[0][0].date()} - {y_val.index[-1][0].date()} validation set {r2_val}')\n",
    "    print('---')\n",
    "\n",
    "predictions_all_full = np.concatenate(predictions, axis=0)\n",
    "y_test_list_all_full = np.concatenate(y_val_list, axis=0)\n",
    "dates_all_full = np.concatenate(dates, axis=0)\n",
    "\n",
    "# R2FULL = 1-np.sum(pow(y_test_list_all_full-predictions_all_full,2))/np.sum(pow(y_test_list_all_full,2))\n",
    "# print(\"R2OOS Linear Regression: \", R2FULL)\n",
    "R2FULL = r2_score(y_test_list_all_full, predictions_all_full)\n",
    "print(\"R2OOS Linear Regression: \", R2FULL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tscv = TimeBasedCV(train_period=60,\n",
    "                   val_period=24,\n",
    "                   freq='months')\n",
    "\n",
    "features = df.columns[~df.columns.isin(['permno', 'permno2', 'mvel12', 'DATE2', 'risk_premium'])].tolist()\n",
    "\n",
    "X = df_top[features]\n",
    "y = df_top[['risk_premium']]\n",
    "\n",
    "predictions_top = []\n",
    "y_val_list_top = []\n",
    "dates_top = []\n",
    "dic_r2_all_top = {}\n",
    "\n",
    "\n",
    "for train_index, val_index in tscv.split(X, first_split_date= datetime.date(1985,1,31), second_split_date= datetime.date(1987,1,31)):\n",
    "\n",
    "    X_train   = X.loc[train_index].drop('DATE', axis=1)\n",
    "    y_train = y.loc[train_index]\n",
    "\n",
    "    X_val   = X.loc[val_index].drop('DATE', axis=1)\n",
    "    y_val = y.loc[val_index]\n",
    "\n",
    "    huber = HuberRegressor(max_iter=1000)\n",
    "\n",
    "    # fitting the huber model\n",
    "    huber.fit(X_train, y_train)\n",
    "\n",
    "    # predicting on the training set and assiging to the training list\n",
    "    preds_train = huber.predict(X_train)\n",
    "\n",
    "    preds_val = huber.predict(X_val) \n",
    "    predictions_top.append(preds_val)\n",
    "    dates_top.append(y_val.index)\n",
    "    y_val_list_top.append(y_val)\n",
    "    \n",
    "    #Calculate OOS model performance the for current window\n",
    "    r2_train = 1-np.sum(pow(y_train['risk_premium']-preds_train,2))/np.sum(pow(y_train['risk_premium'],2))\n",
    "    r2_val = 1-np.sum(pow(y_val['risk_premium']-preds_val,2))/np.sum(pow(y_val['risk_premium'],2))\n",
    "    #Save OOS model performance and the respective month to dictionary\n",
    "    dic_r2_all_top[\"r2.\" + str(y_val['risk_premium'].index)] = r2_val\n",
    "    \n",
    "\n",
    "    print(f'R2  training set {r2_train}')\n",
    "    print(f'R2  validation set {r2_val}')\n",
    "    print('---')\n",
    "\n",
    "predictions_all_full_top = np.concatenate(predictions_top, axis=0)\n",
    "y_test_list_all_full_top = np.concatenate(y_val_list_top, axis=0)\n",
    "dates_all_full_top = np.concatenate(dates_top, axis=0)\n",
    "\n",
    "# R2TOP = 1-np.sum(pow(y_test_list_all_full_top-predictions_all_full_top,2))/np.sum(pow(y_test_list_all_full_top,2))\n",
    "# print(\"R2OOS Linear Regression: \", R2TOP)\n",
    "\n",
    "\n",
    "R2TOP = r2_score(y_test_list_all_full_top, predictions_all_full_top)\n",
    "print(\"R2OOS Linear Regression: \", R2TOP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tscv = TimeBasedCV(train_period=60,\n",
    "                   val_period=24,\n",
    "                   test_period=12,\n",
    "                   freq='months')\n",
    "\n",
    "\n",
    "features = df.columns[~df.columns.isin(['permno', 'permno2', 'mvel12', 'DATE2', 'risk_premium'])].tolist()\n",
    "X = df_bottom[features]\n",
    "y = df_bottom[['risk_premium']]\n",
    "\n",
    "#Empty containers to save results from each window\n",
    "\n",
    "predictions_bottom = []\n",
    "y_val_list_bottom =[]\n",
    "dates_bottom = []\n",
    "dic_r2_all_bottom = {}\n",
    "\n",
    "\n",
    "for train_index, val_index in tscv.split(X, first_split_date= datetime.date(1985,1,31), second_split_date= datetime.date(1987,1,31)):\n",
    "\n",
    "    X_train   = X.loc[train_index].drop('DATE', axis=1)\n",
    "    y_train = y.loc[train_index]\n",
    "\n",
    "    X_val   = X.loc[val_index].drop('DATE', axis=1)\n",
    "    y_val = y.loc[val_index]\n",
    "\n",
    "    huber = HuberRegressor(max_iter=1000)\n",
    "\n",
    "    # fitting the huber model\n",
    "    huber.fit(X_train, y_train)\n",
    "\n",
    "    # predicting on the training set and assiging to the training list\n",
    "    preds_train = huber.predict(X_train)\n",
    "\n",
    "    preds_val = huber.predict(X_val) \n",
    "    predictions_bottom.append(preds_val)\n",
    "    dates_bottom.append(y_val.index)\n",
    "    y_val_list_bottom.append(y_val)\n",
    "    \n",
    "    #Calculate OOS model performance the for current window\n",
    "    r2_train = 1-np.sum(pow(y_train['risk_premium']-preds_train,2))/np.sum(pow(y_train['risk_premium'],2))\n",
    "    r2_val = 1-np.sum(pow(y_val['risk_premium']-preds_val,2))/np.sum(pow(y_val['risk_premium'],2))\n",
    "    #Save OOS model performance and the respective month to dictionary\n",
    "    dic_r2_all_bottom[\"r2.\" + str(y_val['risk_premium'].index)] = r2_val\n",
    "    \n",
    "\n",
    "    print(f'R2  training set {r2_train}')\n",
    "    print(f'R2  validation set {r2_val}')\n",
    "    print('---')\n",
    "\n",
    "predictions_all_full_bottom = np.concatenate(predictions_bottom, axis=0)\n",
    "y_test_list_all_full_bottom = np.concatenate(y_val_list_bottom, axis=0)\n",
    "dates_all_full_bottom = np.concatenate(dates_bottom, axis=0)\n",
    "\n",
    "\n",
    "R2BOTTOM = r2_score(y_test_list_all_full_top, predictions_all_full_top)\n",
    "print(\"R2OOS Linear Regression: \", R2BOTTOM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chart = np.array([[R2FULL],\n",
    "                  [R2TOP],\n",
    "                  [R2BOTTOM]])\n",
    "\n",
    "huber = pd.DataFrame(chart, columns=['Huber Regression'],\n",
    "                     index=['Full Sample', 'Large Firms', 'Small Firms'])\n",
    "\n",
    "huber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "huber.to_csv(r'huber_model.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = predictions_all_full.tolist()\n",
    "y_true = y_test_list_all_full.tolist()\n",
    "i = dates_all_full.tolist()\n",
    "\n",
    "results = pd.DataFrame(\n",
    "    {'identifier': i,\n",
    "     'yhat': yhat,\n",
    "     'y_true': y_true\n",
    "    })\n",
    "\n",
    "\n",
    "results[\"identifier\"]= results[\"identifier\"].astype(\"str\")\n",
    "results[\"date\"] = results[\"identifier\"].str[12:22]\n",
    "results[\"id\"] = results[\"identifier\"].str[35:40]\n",
    "results.drop([\"identifier\"],axis = 1, inplace=True)\n",
    "results['date'] = pd.to_datetime(results['date'], format='%Y-%m-%d')\n",
    "results['MonthYear'] = results['date'].dt.to_period('M')\n",
    "results = results.sort_values(by = ['date', 'id'], ascending = True)\n",
    "results = results.set_index(['MonthYear','id'])\n",
    "results.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results['yhat'] = results['yhat'].apply(lambda x: x[0])\n",
    "results['y_true'] = results['y_true'].apply(lambda x: x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df[['mvel12', 'macro_tbl', 'macro_svar']].copy()\n",
    "data.reset_index(inplace=True)\n",
    "data['permno2'] = data['permno2'].astype('str')\n",
    "data['MonthYear'] = data['DATE2'].dt.to_period('M')\n",
    "data.drop('DATE2', axis=1, inplace=True)\n",
    "data.rename(columns={'permno2': 'id'}, inplace=True)\n",
    "data.rename(columns={'mvel12': 'market_cap'}, inplace=True)\n",
    "data.rename(columns={'macro_tbl': 'risk_free_rate'}, inplace=True)\n",
    "data = data.set_index(['MonthYear','id'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigdata = pd.merge(results, data,left_index=True, right_index=True)\n",
    "bigdata.reset_index(inplace=True)\n",
    "bigdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigdata['returns'] = bigdata['y_true'] + bigdata['risk_free_rate']\n",
    "bigdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigdata['MonthYear1'] = bigdata['MonthYear'].copy()\n",
    "bigdata['MonthYear'] = bigdata['MonthYear'].astype('int64')\n",
    "bigdata['NumMonth'] = bigdata['MonthYear'] - 251\n",
    "bigdata['NumMonth'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in bigdata['NumMonth'].unique():\n",
    "    globals()['df_' + str(i)] = bigdata[bigdata['NumMonth'] == i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in bigdata[\"NumMonth\"].unique():\n",
    "    globals()['df_' + str(i)][\"rank\"]= globals()['df_' + str(i)]['yhat'].rank(method='first')\n",
    "    \n",
    "for i in bigdata[\"NumMonth\"].unique():\n",
    "    globals()['df_' + str(i)][\"DecileRank\"]=pd.qcut(globals()['df_' + str(i)]['rank'].values, 10, labels = False)\n",
    "\n",
    "#Drop normal rank, retain only decile ranks \n",
    "for i in bigdata[\"NumMonth\"].unique():\n",
    "     globals()['df_' + str(i)].drop('rank', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in bigdata[\"NumMonth\"].unique():\n",
    "    for j,g in globals()['df_' + str(i)].groupby('DecileRank'):\n",
    "        globals()['df_' + str(i)+ \"_\" + str(j)] =  g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for j in np.arange(0,10,1):\n",
    "    globals()['rank_' + str(j)] = pd.concat([globals()['df_1_'+ str(j)], globals()['df_2_'+ str(j)]], axis=0)\n",
    "    \n",
    "# Generate 10 Dataframes for the 10 Decile portfolios 0-9: rank_9: top portfolio, rank_0: bottom portfolio\n",
    "for i in np.arange(2,348,1):\n",
    "    for j in np.arange(0,10,1):\n",
    "        globals()['rank_' + str(j)] = pd.concat([globals()['rank_' + str(j)], globals()['df_' + str(i+1)+ \"_\" + str(j)]], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rank_9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in np.arange(0,10,1):\n",
    "    globals()['rank_' + str(j)][\"eq_weights\"] = 1/globals()['rank_' + str(j)].groupby('MonthYear')[\"id\"].transform('size')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for j in np.arange(0,10,1):\n",
    "    globals()['rank_' + str(j)]['excess_return_stock_ew'] = globals()['rank_' + str(j)][\"y_true\"]*globals()['rank_' + str(j)][\"eq_weights\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in np.arange(0,10,1):\n",
    "    globals()['rank_' + str(j)]['return_stock_ew'] = globals()['rank_' + str(j)][\"returns\"]*globals()['rank_' + str(j)][\"eq_weights\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for j in np.arange(0,10,1):\n",
    "    globals()['rank_' + str(j)]['excess_return_portfolio_ew'] = globals()['rank_' + str(j)].groupby('MonthYear')[\"excess_return_stock_ew\"].transform('sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for j in np.arange(0,10,1):\n",
    "    globals()['rank_' + str(j)]['return_portfolio_ew'] = globals()['rank_' + str(j)].groupby('MonthYear')[\"return_stock_ew\"].transform('sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for j in np.arange(0,10,1):\n",
    "    globals()['rank_' + str(j)]['pred_excess_return_stock_ew'] = globals()['rank_' + str(j)][\"yhat\"]*globals()['rank_' + str(j)][\"eq_weights\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in np.arange(0,10,1):\n",
    "    globals()['rank_' + str(j)]['pred_excess_return_portfolio_ew'] = globals()['rank_' + str(j)].groupby('MonthYear')[\"pred_excess_return_stock_ew\"].transform('sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in np.arange(0,10,1):\n",
    "    globals()['montly_rank_' + str(j)] = globals()['rank_' + str(j)][[\"MonthYear1\", \"DecileRank\",\n",
    "                                                                      \"excess_return_portfolio_ew\",\n",
    "                                                                      \"pred_excess_return_portfolio_ew\",\n",
    "                                                                      \"return_portfolio_ew\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in np.arange(0,10,1):\n",
    "    globals()['montly_rank_' + str(j)]=globals()['montly_rank_' + str(j)].drop_duplicates()\n",
    "    globals()['montly_rank_' + str(j)]=globals()['montly_rank_' + str(j)].set_index(\"MonthYear1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in np.arange(0,10,1):\n",
    "    #Time-series average of realized excess returns\n",
    "    globals()[\"ew_mean_return_rank_\" +  str(j)]= globals()['montly_rank_' + str(j)][\"excess_return_portfolio_ew\"].mean()\n",
    "    #Time-series average of predicted excess returns\n",
    "    globals()[\"ew_mean_pred_return_rank_\" +  str(j)]= globals()['montly_rank_' + str(j)][\"pred_excess_return_portfolio_ew\"].mean()\n",
    "    #Standard deviation of realized excess returns\n",
    "    globals()[\"std_ew_rank_\" +  str(j)]= globals()['montly_rank_' + str(j)][\"excess_return_portfolio_ew\"].std()\n",
    "    #Annualized sharpe ratio of realized excess returns\n",
    "    globals()[\"sharpe_ew_rank_\" +  str(j)]= (globals()['montly_rank_' + str(j)][\"excess_return_portfolio_ew\"].mean()/globals()['montly_rank_' + str(j)][\"return_portfolio_ew\"].std())* np.sqrt(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# For the zero-net-investment long-short portfolio the top (long) and bottom(short) decile portfolios are needed\n",
    "\n",
    "long_monthly = rank_9[[\"NumMonth\",\n",
    "                       \"MonthYear1\",\n",
    "                       \"DecileRank\", \n",
    "                       \"excess_return_portfolio_ew\",\n",
    "                       \"pred_excess_return_portfolio_ew\",\n",
    "                       \"return_portfolio_ew\",\n",
    "                        ]].drop_duplicates()\n",
    "\n",
    "short_monthly = rank_0[[\"NumMonth\",\n",
    "                        \"MonthYear1\",\n",
    "                        \"DecileRank\",\n",
    "                        \"excess_return_portfolio_ew\",\n",
    "                       \"pred_excess_return_portfolio_ew\",\n",
    "                       \"return_portfolio_ew\",\n",
    "                        ]].drop_duplicates()\n",
    "\n",
    "# Create a column, indication the stategy \n",
    "long_monthly[\"Strategy\"]= \"long\"\n",
    "short_monthly[\"Strategy\"]= \"short\"\n",
    "\n",
    "# Merge to get the zero net investment portfolio\n",
    "zeronet_monthly= pd.concat([long_monthly, short_monthly])\n",
    "zeronet_monthly = zeronet_monthly.sort_values(by = ['NumMonth',\"Strategy\"])\n",
    "\n",
    "#Create two new columns containing the exess return of the portfolio and initially set the values to zero.\n",
    "zeronet_monthly[\"excess_return_zeronet_ew\"] =0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, len(zeronet_monthly)):\n",
    "    if zeronet_monthly.iloc[i,7] == \"long\":\n",
    "        zeronet_monthly.iloc[i, -1] = zeronet_monthly.iloc[i, 3]-zeronet_monthly.iloc[i+1, 3]\n",
    "    else:\n",
    "        zeronet_monthly.iloc[i, -1] = zeronet_monthly.iloc[i-1, 3]-zeronet_monthly.iloc[i, 3]\n",
    "        \n",
    "\n",
    "\n",
    "zeronet_monthly[\"pred_excess_return_zeronet_ew\"] =0\n",
    "\n",
    "for i in range(0, len(zeronet_monthly)):\n",
    "    if zeronet_monthly.iloc[i,7] == \"long\":\n",
    "        zeronet_monthly.iloc[i, -1] = zeronet_monthly.iloc[i, 4]-zeronet_monthly.iloc[i+1, 4]\n",
    "    else:\n",
    "        zeronet_monthly.iloc[i, -1] = zeronet_monthly.iloc[i-1, 4]-zeronet_monthly.iloc[i, 4]\n",
    "\n",
    "\n",
    "zeronet_monthly[\"return_zeronet_ew\"] =0\n",
    "\n",
    "for i in range(0, len(zeronet_monthly)):\n",
    "    if zeronet_monthly.iloc[i,7] == \"long\":\n",
    "        zeronet_monthly.iloc[i, -1] = zeronet_monthly.iloc[i, 5]-zeronet_monthly.iloc[i+1, 5]\n",
    "    else:\n",
    "        zeronet_monthly.iloc[i, -1] = zeronet_monthly.iloc[i-1, 5]-zeronet_monthly.iloc[i, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Only the measures at portfolio level are needed\n",
    "zeronet_monthly = zeronet_monthly[['NumMonth', \n",
    "                                   'MonthYear1', \n",
    "                                   'excess_return_zeronet_ew',\n",
    "                                   'pred_excess_return_zeronet_ew',\n",
    "                                   'return_zeronet_ew',\n",
    "                                   ]].drop_duplicates()\n",
    "\n",
    "zeronet_monthly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate zero-net portfolio performance measures                                            \n",
    "#Time-series average of realized excess returns                                             \n",
    "ew_mean_return_zeronet= zeronet_monthly[\"excess_return_zeronet_ew\"].mean()\n",
    "#Time-series average of predicted excess returns\n",
    "ew_mean_pred_return_zeronet = zeronet_monthly[\"pred_excess_return_zeronet_ew\"].mean()\n",
    "#Standard deviation of realized excess returns\n",
    "std_ew_zeronet = zeronet_monthly[\"excess_return_zeronet_ew\"].std()\n",
    "#Annualized sharpe ratio of realized excess returns\n",
    "sharpe_ew_zeronet = (zeronet_monthly[\"excess_return_zeronet_ew\"].mean()/zeronet_monthly[\"return_zeronet_ew\"].std())* np.sqrt(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zeronet_monthly['excess_return_zeronet_ew']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chart_np = np.array([[ew_mean_pred_return_rank_0, ew_mean_return_rank_0, std_ew_rank_0, sharpe_ew_rank_0],\n",
    "                     [ew_mean_pred_return_rank_1, ew_mean_return_rank_1, std_ew_rank_1, sharpe_ew_rank_1],\n",
    "                     [ew_mean_pred_return_rank_2, ew_mean_return_rank_2, std_ew_rank_2, sharpe_ew_rank_2],\n",
    "                     [ew_mean_pred_return_rank_3, ew_mean_return_rank_3, std_ew_rank_3, sharpe_ew_rank_3],\n",
    "                     [ew_mean_pred_return_rank_4, ew_mean_return_rank_4, std_ew_rank_4, sharpe_ew_rank_4],\n",
    "                     [ew_mean_pred_return_rank_5, ew_mean_return_rank_5, std_ew_rank_5, sharpe_ew_rank_5],\n",
    "                     [ew_mean_pred_return_rank_6, ew_mean_return_rank_6, std_ew_rank_6, sharpe_ew_rank_6],\n",
    "                     [ew_mean_pred_return_rank_7, ew_mean_return_rank_7, std_ew_rank_7, sharpe_ew_rank_7],\n",
    "                     [ew_mean_pred_return_rank_8, ew_mean_return_rank_8, std_ew_rank_8, sharpe_ew_rank_8],\n",
    "                     [ew_mean_pred_return_rank_9, ew_mean_return_rank_9, std_ew_rank_9, sharpe_ew_rank_9],\n",
    "                     [ew_mean_pred_return_zeronet, ew_mean_return_zeronet, std_ew_zeronet, sharpe_ew_zeronet]])\n",
    "\n",
    "ew_df = pd.DataFrame(chart_np, columns=['Pred', 'Real', 'Std', 'Sharpe'],\n",
    "                              index=['Low (L)', '2', '3', '4', '5','6','7','8',\"9\",'High (H)', \"H-L\"])\n",
    "\n",
    "ew_df['Pred'] = pd.Series([\"{0:.2f}%\".format(val) for val in ew_df['Pred']], index = ew_df.index)\n",
    "ew_df['Real'] = pd.Series([\"{0:.2f}%\".format(val) for val in ew_df['Real']], index = ew_df.index)\n",
    "ew_df['Std'] = pd.Series([\"{0:.2f}%\".format(val) for val in ew_df['Std']], index = ew_df.index)\n",
    "ew_df['Sharpe'] = pd.Series([(\"%.2f\" % round(val, 2)) for val in ew_df['Sharpe']], index = ew_df.index)\n",
    "ew_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "statclass",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
