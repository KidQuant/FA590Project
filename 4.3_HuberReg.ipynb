{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import sklearn.preprocessing\n",
    "import datetime\n",
    "from TimeBasedCV import TimeBasedCV\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "from sklearn.metrics import make_scorer, r2_score\n",
    "import statsmodels.api as sm\n",
    "import warnings\n",
    "import pickle\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=Warning)\n",
    "from sklearn.linear_model import HuberRegressor\n",
    "# pd.set_option('display.max_rows', None)\n",
    "# more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('data/factors_1965.csv', parse_dates=['DATE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   permno       DATE        mvel1      beta    betasq     chmom     dolvol  \\\n",
      "0   10145 1965-02-26   1498872.00  0.983510  0.967291  0.105988  11.546907   \n",
      "1   10401 1965-02-26  35392058.00  0.780829  0.609694 -0.063768  12.240330   \n",
      "2   10786 1965-02-26   1695284.75  0.806119  0.649827 -0.130519  12.005040   \n",
      "3   10989 1965-02-26   1295887.75  1.199748  1.439395  0.073609  11.756961   \n",
      "4   11260 1965-02-26   2302001.25  1.257269  1.580725 -0.167320  12.240330   \n",
      "\n",
      "    idiovol    indmom     mom1m  ...  macro_ep  macro_bm  macro_ntis  \\\n",
      "0  0.022307  0.035075  0.104116  ...  2.936836  0.471399    0.014823   \n",
      "1  0.013395  0.335139 -0.007326  ...  2.936836  0.471399    0.014823   \n",
      "2  0.024366  0.104106  0.060498  ...  2.936836  0.471399    0.014823   \n",
      "3  0.022717  0.118513  0.068807  ...  2.936836  0.471399    0.014823   \n",
      "4  0.035883  0.185424 -0.036885  ...  2.936836  0.471399    0.014823   \n",
      "\n",
      "   macro_tbl  macro_tms  macro_dfy  macro_svar  macro_mkt-rf  macro_hml  \\\n",
      "0     0.0393    -0.0379     0.0055    0.000393          0.44       0.11   \n",
      "1     0.0393    -0.0379     0.0055    0.000393          0.44       0.11   \n",
      "2     0.0393    -0.0379     0.0055    0.000393          0.44       0.11   \n",
      "3     0.0393    -0.0379     0.0055    0.000393          0.44       0.11   \n",
      "4     0.0393    -0.0379     0.0055    0.000393          0.44       0.11   \n",
      "\n",
      "   macro_smb  \n",
      "0       3.55  \n",
      "1       3.55  \n",
      "2       3.55  \n",
      "3       3.55  \n",
      "4       3.55  \n",
      "\n",
      "[5 rows x 92 columns]\n"
     ]
    }
   ],
   "source": [
    "# with open('data/features_1965.pkl', 'wb') as f:\n",
    "#     pickle.dump(df, f)\n",
    "\n",
    "with open('data/features_1965.pkl', 'rb') as f:\n",
    "    df = pickle.load(f)\n",
    "    print(df.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sort observations by date and stock id\n",
    "df[df.columns[2:]] = df[df.columns[2:]].astype('float32')\n",
    "df = df.sort_values(by = ['DATE', 'permno'], ascending = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['permno2'] = df['permno'].copy()\n",
    "df['DATE2'] = df['DATE'].copy()\n",
    "df['mvel12'] = df['mvel1'].copy()\n",
    "df = df.set_index(['DATE2','permno2'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "p=0.3 \n",
    "df_top= df.groupby('DATE').apply(lambda x: x.nlargest(int(len(x)*p),'mvel1')).reset_index(drop=True)  \n",
    "df_bottom = df.groupby('DATE').apply(lambda x: x.nsmallest(int(len(x)*p),'mvel1')).reset_index(drop=True)  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calculate_r2(y_true, y_pred, in_sample=True, benchmark=None):\n",
    "    if in_sample:\n",
    "        return 1 - (np.sum((y_true - y_pred) ** 2) / \n",
    "                    np.sum((y_true - np.mean(y_true)) ** 2))\n",
    "    else:\n",
    "        if benchmark is None:\n",
    "            raise ValueError(\"Benchmark must be provided for out-of-sample R-squared calculation.\")\n",
    "        return 1 - (np.sum((y_true - y_pred) ** 2) / \n",
    "                    np.sum((y_true - benchmark) ** 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df.columns[~df.columns.isin(['DATE', 'DATE2', \"mvel2\",'sic2' ,'permno',\"permno2\",'risk_premium'])].tolist()\n",
    "df[features]=df.groupby('DATE')[features].rank(pct=True)\n",
    "\n",
    "df[features] = 2*df[features] - 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train period: 1976-01-31 - 1981-01-31 ,val period: 1981-01-31 - 1983-01-31 , Test period 1983-01-31 - 1984-01-31 # train records 11430 ,# val records 6471 , # test records 4416\n",
      "Train period: 1977-01-31 - 1982-01-31 ,val period: 1982-01-31 - 1984-01-31 , Test period 1984-01-31 - 1985-01-31 # train records 13241 ,# val records 7309 , # test records 4368\n",
      "Train period: 1978-01-31 - 1983-01-31 ,val period: 1983-01-31 - 1985-01-31 , Test period 1985-01-31 - 1986-01-31 # train records 14193 ,# val records 8784 , # test records 4870\n",
      "Train period: 1979-01-31 - 1984-01-31 ,val period: 1984-01-31 - 1986-01-31 , Test period 1986-01-31 - 1987-01-31 # train records 16579 ,# val records 9238 , # test records 6416\n",
      "Train period: 1980-01-31 - 1985-01-31 ,val period: 1985-01-31 - 1987-01-31 , Test period 1987-01-31 - 1988-01-31 # train records 18589 ,# val records 11286 , # test records 6641\n",
      "Train period: 1981-01-31 - 1986-01-31 ,val period: 1986-01-31 - 1988-01-31 , Test period 1988-01-31 - 1989-01-31 # train records 20125 ,# val records 13057 , # test records 5931\n",
      "Train period: 1982-01-31 - 1987-01-31 ,val period: 1987-01-31 - 1989-01-31 , Test period 1989-01-31 - 1990-01-31 # train records 22963 ,# val records 12572 , # test records 6850\n",
      "Train period: 1983-01-31 - 1988-01-31 ,val period: 1988-01-31 - 1990-01-31 , Test period 1990-01-31 - 1991-01-31 # train records 26711 ,# val records 12781 , # test records 6553\n",
      "Train period: 1984-01-31 - 1989-01-31 ,val period: 1989-01-31 - 1991-01-31 , Test period 1991-01-31 - 1992-01-31 # train records 28226 ,# val records 13403 , # test records 7063\n",
      "Train period: 1985-01-31 - 1990-01-31 ,val period: 1990-01-31 - 1992-01-31 , Test period 1992-01-31 - 1993-01-31 # train records 30708 ,# val records 13616 , # test records 8743\n",
      "Train period: 1986-01-31 - 1991-01-31 ,val period: 1991-01-31 - 1993-01-31 , Test period 1993-01-31 - 1994-01-31 # train records 32391 ,# val records 15806 , # test records 8628\n",
      "Train period: 1987-01-31 - 1992-01-31 ,val period: 1992-01-31 - 1994-01-31 , Test period 1994-01-31 - 1995-01-31 # train records 33038 ,# val records 17371 , # test records 10193\n",
      "Train period: 1988-01-31 - 1993-01-31 ,val period: 1993-01-31 - 1995-01-31 , Test period 1995-01-31 - 1996-01-31 # train records 35140 ,# val records 18821 , # test records 11176\n",
      "Train period: 1989-01-31 - 1994-01-31 ,val period: 1994-01-31 - 1996-01-31 , Test period 1996-01-31 - 1997-01-31 # train records 37837 ,# val records 21369 , # test records 12945\n",
      "Train period: 1990-01-31 - 1995-01-31 ,val period: 1995-01-31 - 1997-01-31 , Test period 1997-01-31 - 1998-01-31 # train records 41180 ,# val records 24121 , # test records 16010\n",
      "Train period: 1991-01-31 - 1996-01-31 ,val period: 1996-01-31 - 1998-01-31 , Test period 1998-01-31 - 1999-01-31 # train records 45803 ,# val records 28955 , # test records 15949\n",
      "Train period: 1992-01-31 - 1997-01-31 ,val period: 1997-01-31 - 1999-01-31 , Test period 1999-01-31 - 2000-01-31 # train records 51685 ,# val records 31959 , # test records 14847\n",
      "Train period: 1993-01-31 - 1998-01-31 ,val period: 1998-01-31 - 2000-01-31 , Test period 2000-01-31 - 2001-01-31 # train records 58952 ,# val records 30796 , # test records 18389\n",
      "Train period: 1994-01-31 - 1999-01-31 ,val period: 1999-01-31 - 2001-01-31 , Test period 2001-01-31 - 2002-01-31 # train records 66273 ,# val records 33236 , # test records 16233\n",
      "Train period: 1995-01-31 - 2000-01-31 ,val period: 2000-01-31 - 2002-01-31 , Test period 2002-01-31 - 2003-01-31 # train records 70927 ,# val records 34622 , # test records 15449\n",
      "Train period: 1996-01-31 - 2001-01-31 ,val period: 2001-01-31 - 2003-01-31 , Test period 2003-01-31 - 2004-01-31 # train records 78140 ,# val records 31682 , # test records 17642\n",
      "Train period: 1997-01-31 - 2002-01-31 ,val period: 2002-01-31 - 2004-01-31 , Test period 2004-01-31 - 2005-01-31 # train records 81428 ,# val records 33091 , # test records 17980\n",
      "Train period: 1998-01-31 - 2003-01-31 ,val period: 2003-01-31 - 2005-01-31 , Test period 2005-01-31 - 2006-01-31 # train records 80867 ,# val records 35622 , # test records 21590\n",
      "Train period: 1999-01-31 - 2004-01-31 ,val period: 2004-01-31 - 2006-01-31 , Test period 2006-01-31 - 2007-01-31 # train records 82560 ,# val records 39570 , # test records 23521\n",
      "Train period: 2000-01-31 - 2005-01-31 ,val period: 2005-01-31 - 2007-01-31 , Test period 2007-01-31 - 2008-01-31 # train records 85693 ,# val records 45111 , # test records 24470\n",
      "Train period: 2001-01-31 - 2006-01-31 ,val period: 2006-01-31 - 2008-01-31 , Test period 2008-01-31 - 2009-01-31 # train records 88894 ,# val records 47991 , # test records 21949\n",
      "Train period: 2002-01-31 - 2007-01-31 ,val period: 2007-01-31 - 2009-01-31 , Test period 2009-01-31 - 2010-01-31 # train records 96182 ,# val records 46419 , # test records 16767\n",
      "Train period: 2003-01-31 - 2008-01-31 ,val period: 2008-01-31 - 2010-01-31 , Test period 2010-01-31 - 2011-01-31 # train records 105203 ,# val records 38716 , # test records 18170\n",
      "Train period: 2004-01-31 - 2009-01-31 ,val period: 2009-01-31 - 2011-01-31 , Test period 2011-01-31 - 2012-01-31 # train records 109510 ,# val records 34937 , # test records 21578\n",
      "Train period: 2005-01-31 - 2010-01-31 ,val period: 2010-01-31 - 2012-01-31 , Test period 2012-01-31 - 2013-01-31 # train records 108297 ,# val records 39748 , # test records 21516\n",
      "Train period: 2006-01-31 - 2011-01-31 ,val period: 2011-01-31 - 2013-01-31 , Test period 2013-01-31 - 2014-01-31 # train records 104877 ,# val records 43094 , # test records 23877\n",
      "Train period: 2007-01-31 - 2012-01-31 ,val period: 2012-01-31 - 2014-01-31 , Test period 2014-01-31 - 2015-01-31 # train records 102934 ,# val records 45393 , # test records 28640\n",
      "Train period: 2008-01-31 - 2013-01-31 ,val period: 2013-01-31 - 2015-01-31 , Test period 2015-01-31 - 2016-01-31 # train records 99980 ,# val records 52517 , # test records 26461\n",
      "Train period: 2009-01-31 - 2014-01-31 ,val period: 2014-01-31 - 2016-01-31 , Test period 2016-01-31 - 2017-01-31 # train records 101908 ,# val records 55101 , # test records 23187\n",
      "Train period: 2010-01-31 - 2015-01-31 ,val period: 2015-01-31 - 2017-01-31 , Test period 2017-01-31 - 2018-01-31 # train records 113781 ,# val records 49648 , # test records 27102\n",
      "Train period: 2011-01-31 - 2016-01-31 ,val period: 2016-01-31 - 2018-01-31 , Test period 2018-01-31 - 2019-01-31 # train records 122072 ,# val records 50289 , # test records 28421\n",
      "Train period: 2012-01-31 - 2017-01-31 ,val period: 2017-01-31 - 2019-01-31 , Test period 2019-01-31 - 2020-01-31 # train records 123681 ,# val records 55523 , # test records 27271\n",
      "Train period: 2013-01-31 - 2018-01-31 ,val period: 2018-01-31 - 2020-01-31 , Test period 2020-01-31 - 2021-01-31 # train records 129267 ,# val records 55692 , # test records 29168\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 32\u001b[0m\n\u001b[0;32m     30\u001b[0m reg_huber \u001b[38;5;241m=\u001b[39m HuberRegressor(max_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m)\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m#No hyperparameters in OLS --> use df and validation set for training\u001b[39;00m\n\u001b[1;32m---> 32\u001b[0m reg_huber\u001b[38;5;241m.\u001b[39mfit(np\u001b[38;5;241m.\u001b[39mconcatenate((X_train, X_val)), np\u001b[38;5;241m.\u001b[39mconcatenate((y_train, y_val)))\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m#Use test set to generate predictions\u001b[39;00m\n\u001b[0;32m     34\u001b[0m preds \u001b[38;5;241m=\u001b[39m reg_huber\u001b[38;5;241m.\u001b[39mpredict(X_test) \n",
      "File \u001b[1;32mc:\\Users\\drebi\\miniconda3\\envs\\statclass\\Lib\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\drebi\\miniconda3\\envs\\statclass\\Lib\\site-packages\\sklearn\\linear_model\\_huber.py:325\u001b[0m, in \u001b[0;36mHuberRegressor.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    322\u001b[0m bounds \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mtile([\u001b[38;5;241m-\u001b[39mnp\u001b[38;5;241m.\u001b[39minf, np\u001b[38;5;241m.\u001b[39minf], (parameters\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m    323\u001b[0m bounds[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mfinfo(np\u001b[38;5;241m.\u001b[39mfloat64)\u001b[38;5;241m.\u001b[39meps \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[1;32m--> 325\u001b[0m opt_res \u001b[38;5;241m=\u001b[39m optimize\u001b[38;5;241m.\u001b[39mminimize(\n\u001b[0;32m    326\u001b[0m     _huber_loss_and_gradient,\n\u001b[0;32m    327\u001b[0m     parameters,\n\u001b[0;32m    328\u001b[0m     method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mL-BFGS-B\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    329\u001b[0m     jac\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    330\u001b[0m     args\u001b[38;5;241m=\u001b[39m(X, y, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepsilon, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malpha, sample_weight),\n\u001b[0;32m    331\u001b[0m     options\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmaxiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_iter, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgtol\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtol, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miprint\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m},\n\u001b[0;32m    332\u001b[0m     bounds\u001b[38;5;241m=\u001b[39mbounds,\n\u001b[0;32m    333\u001b[0m )\n\u001b[0;32m    335\u001b[0m parameters \u001b[38;5;241m=\u001b[39m opt_res\u001b[38;5;241m.\u001b[39mx\n\u001b[0;32m    337\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m opt_res\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\drebi\\miniconda3\\envs\\statclass\\Lib\\site-packages\\scipy\\optimize\\_minimize.py:731\u001b[0m, in \u001b[0;36mminimize\u001b[1;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[0;32m    728\u001b[0m     res \u001b[38;5;241m=\u001b[39m _minimize_newtoncg(fun, x0, args, jac, hess, hessp, callback,\n\u001b[0;32m    729\u001b[0m                              \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n\u001b[0;32m    730\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m meth \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124ml-bfgs-b\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m--> 731\u001b[0m     res \u001b[38;5;241m=\u001b[39m _minimize_lbfgsb(fun, x0, args, jac, bounds,\n\u001b[0;32m    732\u001b[0m                            callback\u001b[38;5;241m=\u001b[39mcallback, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n\u001b[0;32m    733\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m meth \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtnc\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    734\u001b[0m     res \u001b[38;5;241m=\u001b[39m _minimize_tnc(fun, x0, args, jac, bounds, callback\u001b[38;5;241m=\u001b[39mcallback,\n\u001b[0;32m    735\u001b[0m                         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n",
      "File \u001b[1;32mc:\\Users\\drebi\\miniconda3\\envs\\statclass\\Lib\\site-packages\\scipy\\optimize\\_lbfgsb_py.py:407\u001b[0m, in \u001b[0;36m_minimize_lbfgsb\u001b[1;34m(fun, x0, args, jac, bounds, disp, maxcor, ftol, gtol, eps, maxfun, maxiter, iprint, callback, maxls, finite_diff_rel_step, **unknown_options)\u001b[0m\n\u001b[0;32m    401\u001b[0m task_str \u001b[38;5;241m=\u001b[39m task\u001b[38;5;241m.\u001b[39mtobytes()\n\u001b[0;32m    402\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m task_str\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFG\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m    403\u001b[0m     \u001b[38;5;66;03m# The minimization routine wants f and g at the current x.\u001b[39;00m\n\u001b[0;32m    404\u001b[0m     \u001b[38;5;66;03m# Note that interruptions due to maxfun are postponed\u001b[39;00m\n\u001b[0;32m    405\u001b[0m     \u001b[38;5;66;03m# until the completion of the current minimization iteration.\u001b[39;00m\n\u001b[0;32m    406\u001b[0m     \u001b[38;5;66;03m# Overwrite f and g:\u001b[39;00m\n\u001b[1;32m--> 407\u001b[0m     f, g \u001b[38;5;241m=\u001b[39m func_and_grad(x)\n\u001b[0;32m    408\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m task_str\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNEW_X\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m    409\u001b[0m     \u001b[38;5;66;03m# new iteration\u001b[39;00m\n\u001b[0;32m    410\u001b[0m     n_iterations \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\drebi\\miniconda3\\envs\\statclass\\Lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:343\u001b[0m, in \u001b[0;36mScalarFunction.fun_and_grad\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    341\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray_equal(x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx):\n\u001b[0;32m    342\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_x(x)\n\u001b[1;32m--> 343\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_fun()\n\u001b[0;32m    344\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_grad()\n\u001b[0;32m    345\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mg\n",
      "File \u001b[1;32mc:\\Users\\drebi\\miniconda3\\envs\\statclass\\Lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:294\u001b[0m, in \u001b[0;36mScalarFunction._update_fun\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    292\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_update_fun\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    293\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf_updated:\n\u001b[1;32m--> 294\u001b[0m         fx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wrapped_fun(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx)\n\u001b[0;32m    295\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m fx \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lowest_f:\n\u001b[0;32m    296\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lowest_x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx\n",
      "File \u001b[1;32mc:\\Users\\drebi\\miniconda3\\envs\\statclass\\Lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:20\u001b[0m, in \u001b[0;36m_wrapper_fun.<locals>.wrapped\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     16\u001b[0m ncalls[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Send a copy because the user may overwrite it.\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Overwriting results in undefined behaviour because\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# fun(self.x) will change self.x, with the two no longer linked.\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m fx \u001b[38;5;241m=\u001b[39m fun(np\u001b[38;5;241m.\u001b[39mcopy(x), \u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Make sure the function returns a true scalar\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39misscalar(fx):\n",
      "File \u001b[1;32mc:\\Users\\drebi\\miniconda3\\envs\\statclass\\Lib\\site-packages\\scipy\\optimize\\_optimize.py:79\u001b[0m, in \u001b[0;36mMemoizeJac.__call__\u001b[1;34m(self, x, *args)\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, \u001b[38;5;241m*\u001b[39margs):\n\u001b[0;32m     78\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\" returns the function value \"\"\"\u001b[39;00m\n\u001b[1;32m---> 79\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compute_if_needed(x, \u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m     80\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value\n",
      "File \u001b[1;32mc:\\Users\\drebi\\miniconda3\\envs\\statclass\\Lib\\site-packages\\scipy\\optimize\\_optimize.py:73\u001b[0m, in \u001b[0;36mMemoizeJac._compute_if_needed\u001b[1;34m(self, x, *args)\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39mall(x \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjac \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(x)\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m---> 73\u001b[0m     fg \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfun(x, \u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m     74\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjac \u001b[38;5;241m=\u001b[39m fg[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value \u001b[38;5;241m=\u001b[39m fg[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\drebi\\miniconda3\\envs\\statclass\\Lib\\site-packages\\sklearn\\linear_model\\_huber.py:99\u001b[0m, in \u001b[0;36m_huber_loss_and_gradient\u001b[1;34m(w, X, y, epsilon, alpha, sample_weight)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;66;03m# Gradient due to the squared loss.\u001b[39;00m\n\u001b[0;32m     97\u001b[0m X_non_outliers \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39maxis0_safe_slice(X, \u001b[38;5;241m~\u001b[39moutliers_mask, n_non_outliers)\n\u001b[0;32m     98\u001b[0m grad[:n_features] \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m---> 99\u001b[0m     \u001b[38;5;241m2.0\u001b[39m \u001b[38;5;241m/\u001b[39m sigma \u001b[38;5;241m*\u001b[39m safe_sparse_dot(weighted_non_outliers, X_non_outliers)\n\u001b[0;32m    100\u001b[0m )\n\u001b[0;32m    102\u001b[0m \u001b[38;5;66;03m# Gradient due to the linear loss.\u001b[39;00m\n\u001b[0;32m    103\u001b[0m signed_outliers \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mones_like(outliers)\n",
      "File \u001b[1;32mc:\\Users\\drebi\\miniconda3\\envs\\statclass\\Lib\\site-packages\\sklearn\\utils\\extmath.py:161\u001b[0m, in \u001b[0;36msafe_sparse_dot\u001b[1;34m(a, b, dense_output)\u001b[0m\n\u001b[0;32m    157\u001b[0m         d \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m w \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mfloat\u001b[39m((w \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39msum()) \u001b[38;5;241m/\u001b[39m w\u001b[38;5;241m.\u001b[39msize\n\u001b[0;32m    158\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m d\n\u001b[1;32m--> 161\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msafe_sparse_dot\u001b[39m(a, b, \u001b[38;5;241m*\u001b[39m, dense_output\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    162\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Dot product that handle the sparse matrix case correctly.\u001b[39;00m\n\u001b[0;32m    163\u001b[0m \n\u001b[0;32m    164\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;124;03m           [17, 39, 61]])\u001b[39;00m\n\u001b[0;32m    187\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m    188\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m a\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m b\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tscv = TimeBasedCV(train_period=60,\n",
    "                   val_period=24,\n",
    "                   test_period=12,\n",
    "                   freq='months')\n",
    "\n",
    "features = df.columns[~df.columns.isin(['permno', 'permno2', 'mvel12', 'sic2', 'DATE2', 'risk_premium'])].tolist()\n",
    "\n",
    "X = df[features]\n",
    "y = df[['risk_premium']]\n",
    "\n",
    "predictions = []\n",
    "y_test_list =[]\n",
    "dates = []\n",
    "dic_r2_all = {}\n",
    "\n",
    "\n",
    "for train_index, val_index, test_index in tscv.split(X, first_split_date= datetime.date(1981,1,31), second_split_date= datetime.date(1991,1,31)):\n",
    "\n",
    "\n",
    "    X_train   = X.loc[train_index].drop('DATE', axis=1)\n",
    "    y_train = y.loc[train_index]\n",
    "    \n",
    "    X_val   = X.loc[val_index].drop('DATE', axis=1)\n",
    "    y_val = y.loc[val_index]\n",
    "\n",
    "    X_test    = X.loc[test_index].drop('DATE', axis=1)\n",
    "    y_test  = y.loc[test_index]\n",
    "    \n",
    "    #OLS regression with huber loss function\n",
    "    reg_huber = HuberRegressor(max_iter=1000)\n",
    "    #No hyperparameters in OLS --> use df and validation set for training\n",
    "    reg_huber.fit(np.concatenate((X_train, X_val)), np.concatenate((y_train, y_val)))\n",
    "    #Use test set to generate predictions\n",
    "    preds = reg_huber.predict(X_test) \n",
    "    #Save predictions, dates and the true values of the dependent variable to list  \n",
    "    predictions.append(preds)\n",
    "    dates.append(y_test.index)\n",
    "    y_test_list.append(y_test)\n",
    "    \n",
    "    #Calculate OOS model performance the for current window\n",
    "    r2 = 1-np.sum(pow(y_test['risk_premium']-preds,2))/np.sum(pow(y_test['risk_premium'],2))\n",
    "    #Save OOS model performance and the respective month to dictionary\n",
    "    dic_r2_all[\"r2.\" + str(y_test.index)] = r2\n",
    "   \n",
    "    \n",
    "    \n",
    "#Concatenate to get results over the whole OOS test period (Jan 2010-Dec 2019)\n",
    "predictions_all= np.concatenate(predictions, axis=0)\n",
    "y_test_list_all= np.concatenate(y_test_list, axis=0) \n",
    "dates_all= np.concatenate(dates, axis=0)\n",
    "\n",
    "#Calculate OOS model performance over the entire test period in line with Gu et al (2020)\n",
    "# R2OOS_LR = 1-np.sum(pow(y_test_list_all-predictions_all,2))/np.sum(pow(y_test_list_all,2))\n",
    "# print(\"R2OOS Hubber Regression: \", R2OOS_LR)\n",
    "R2FULL = r2_score(y_test_list_all, predictions_all)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train period: 1976-01-31 - 1981-01-31 ,val period: 1981-01-31 - 1983-01-31 , Test period 1983-01-31 - 1984-01-31 # train records 3405 ,# val records 1930 , # test records 1320\n",
      "Train period: 1977-01-31 - 1982-01-31 ,val period: 1982-01-31 - 1984-01-31 , Test period 1984-01-31 - 1985-01-31 # train records 3947 ,# val records 2182 , # test records 1304\n",
      "Train period: 1978-01-31 - 1983-01-31 ,val period: 1983-01-31 - 1985-01-31 , Test period 1985-01-31 - 1986-01-31 # train records 4232 ,# val records 2624 , # test records 1458\n",
      "Train period: 1979-01-31 - 1984-01-31 ,val period: 1984-01-31 - 1986-01-31 , Test period 1986-01-31 - 1987-01-31 # train records 4949 ,# val records 2762 , # test records 1919\n",
      "Train period: 1980-01-31 - 1985-01-31 ,val period: 1985-01-31 - 1987-01-31 , Test period 1987-01-31 - 1988-01-31 # train records 5549 ,# val records 3377 , # test records 1987\n",
      "Train period: 1981-01-31 - 1986-01-31 ,val period: 1986-01-31 - 1988-01-31 , Test period 1988-01-31 - 1989-01-31 # train records 6012 ,# val records 3906 , # test records 1777\n",
      "Train period: 1982-01-31 - 1987-01-31 ,val period: 1987-01-31 - 1989-01-31 , Test period 1989-01-31 - 1990-01-31 # train records 6863 ,# val records 3764 , # test records 2049\n",
      "Train period: 1983-01-31 - 1988-01-31 ,val period: 1988-01-31 - 1990-01-31 , Test period 1990-01-31 - 1991-01-31 # train records 7988 ,# val records 3826 , # test records 1959\n",
      "Train period: 1984-01-31 - 1989-01-31 ,val period: 1989-01-31 - 1991-01-31 , Test period 1991-01-31 - 1992-01-31 # train records 8445 ,# val records 4008 , # test records 2113\n",
      "Train period: 1985-01-31 - 1990-01-31 ,val period: 1990-01-31 - 1992-01-31 , Test period 1992-01-31 - 1993-01-31 # train records 9190 ,# val records 4072 , # test records 2619\n",
      "Train period: 1986-01-31 - 1991-01-31 ,val period: 1991-01-31 - 1993-01-31 , Test period 1993-01-31 - 1994-01-31 # train records 9691 ,# val records 4732 , # test records 2583\n",
      "Train period: 1987-01-31 - 1992-01-31 ,val period: 1992-01-31 - 1994-01-31 , Test period 1994-01-31 - 1995-01-31 # train records 9885 ,# val records 5202 , # test records 3051\n",
      "Train period: 1988-01-31 - 1993-01-31 ,val period: 1993-01-31 - 1995-01-31 , Test period 1995-01-31 - 1996-01-31 # train records 10517 ,# val records 5634 , # test records 3348\n",
      "Train period: 1989-01-31 - 1994-01-31 ,val period: 1994-01-31 - 1996-01-31 , Test period 1996-01-31 - 1997-01-31 # train records 11323 ,# val records 6399 , # test records 3879\n",
      "Train period: 1990-01-31 - 1995-01-31 ,val period: 1995-01-31 - 1997-01-31 , Test period 1997-01-31 - 1998-01-31 # train records 12325 ,# val records 7227 , # test records 4797\n",
      "Train period: 1991-01-31 - 1996-01-31 ,val period: 1996-01-31 - 1998-01-31 , Test period 1998-01-31 - 1999-01-31 # train records 13714 ,# val records 8676 , # test records 4780\n",
      "Train period: 1992-01-31 - 1997-01-31 ,val period: 1997-01-31 - 1999-01-31 , Test period 1999-01-31 - 2000-01-31 # train records 15480 ,# val records 9577 , # test records 4451\n",
      "Train period: 1993-01-31 - 1998-01-31 ,val period: 1998-01-31 - 2000-01-31 , Test period 2000-01-31 - 2001-01-31 # train records 17658 ,# val records 9231 , # test records 5511\n",
      "Train period: 1994-01-31 - 1999-01-31 ,val period: 1999-01-31 - 2001-01-31 , Test period 2001-01-31 - 2002-01-31 # train records 19855 ,# val records 9962 , # test records 4865\n",
      "Train period: 1995-01-31 - 2000-01-31 ,val period: 2000-01-31 - 2002-01-31 , Test period 2002-01-31 - 2003-01-31 # train records 21255 ,# val records 10376 , # test records 4631\n",
      "Train period: 1996-01-31 - 2001-01-31 ,val period: 2001-01-31 - 2003-01-31 , Test period 2003-01-31 - 2004-01-31 # train records 23418 ,# val records 9496 , # test records 5287\n",
      "Train period: 1997-01-31 - 2002-01-31 ,val period: 2002-01-31 - 2004-01-31 , Test period 2004-01-31 - 2005-01-31 # train records 24404 ,# val records 9918 , # test records 5390\n",
      "Train period: 1998-01-31 - 2003-01-31 ,val period: 2003-01-31 - 2005-01-31 , Test period 2005-01-31 - 2006-01-31 # train records 24238 ,# val records 10677 , # test records 6472\n",
      "Train period: 1999-01-31 - 2004-01-31 ,val period: 2004-01-31 - 2006-01-31 , Test period 2006-01-31 - 2007-01-31 # train records 24745 ,# val records 11862 , # test records 7051\n",
      "Train period: 2000-01-31 - 2005-01-31 ,val period: 2005-01-31 - 2007-01-31 , Test period 2007-01-31 - 2008-01-31 # train records 25684 ,# val records 13523 , # test records 7335\n",
      "Train period: 2001-01-31 - 2006-01-31 ,val period: 2006-01-31 - 2008-01-31 , Test period 2008-01-31 - 2009-01-31 # train records 26645 ,# val records 14386 , # test records 6578\n",
      "Train period: 2002-01-31 - 2007-01-31 ,val period: 2007-01-31 - 2009-01-31 , Test period 2009-01-31 - 2010-01-31 # train records 28831 ,# val records 13913 , # test records 5023\n",
      "Train period: 2003-01-31 - 2008-01-31 ,val period: 2008-01-31 - 2010-01-31 , Test period 2010-01-31 - 2011-01-31 # train records 31535 ,# val records 11601 , # test records 5446\n",
      "Train period: 2004-01-31 - 2009-01-31 ,val period: 2009-01-31 - 2011-01-31 , Test period 2011-01-31 - 2012-01-31 # train records 32826 ,# val records 10469 , # test records 6469\n",
      "Train period: 2005-01-31 - 2010-01-31 ,val period: 2010-01-31 - 2012-01-31 , Test period 2012-01-31 - 2013-01-31 # train records 32459 ,# val records 11915 , # test records 6450\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tscv = TimeBasedCV(train_period=60,\n",
    "                   val_period=24,\n",
    "                   test_period=12,\n",
    "                   freq='months')\n",
    "\n",
    "features = df.columns[~df.columns.isin(['permno', 'permno2', 'mvel12','sic2' , 'DATE2', 'risk_premium'])].tolist()\n",
    "\n",
    "X = df_top[features]\n",
    "y = df_top[['risk_premium']]\n",
    "\n",
    "predictions_top = []\n",
    "y_test_list_top =[]\n",
    "dates_top = []\n",
    "dic_r2_all_top = {}\n",
    "\n",
    "for train_index, val_index, test_index in tscv.split(X, first_split_date= datetime.date(1981,1,31), second_split_date= datetime.date(1991,1,31)):\n",
    "\n",
    "    X_train   = X.loc[train_index].drop('DATE', axis=1)\n",
    "    y_train = y.loc[train_index]\n",
    "    \n",
    "    X_val   = X.loc[val_index].drop('DATE', axis=1)\n",
    "    y_val = y.loc[val_index]\n",
    "\n",
    "    X_test    = X.loc[test_index].drop('DATE', axis=1)\n",
    "    y_test  = y.loc[test_index]\n",
    "\n",
    "    reg_huber = HuberRegressor(max_iter=1000)\n",
    "    reg_huber.fit(np.concatenate((X_train, X_val)), np.concatenate((y_train, y_val)))\n",
    "\n",
    "\n",
    "    preds = reg_huber.predict(X_test)    \n",
    "    predictions_top.append(preds)\n",
    "    dates_top.append(y_test.index)\n",
    "    y_test_list_top.append(y_test)\n",
    "    \n",
    "    r2_top = 1-np.sum(pow(y_test['risk_premium']-preds,2))/np.sum(pow(y_test['risk_premium'],2))\n",
    "    dic_r2_all_top[\"r2.\" + str(y_test.index)] = r2\n",
    "\n",
    "predictions_all_top= np.concatenate(predictions_top, axis=0)\n",
    "y_test_list_all_top= np.concatenate(y_test_list_top, axis=0) \n",
    "dates_all_top= np.concatenate(dates_top, axis=0)\n",
    "\n",
    "R2TOP = r2_score(y_test_list_all_top, predictions_all_top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train period: 1976-01-31 - 1981-01-31 ,val period: 1981-01-31 - 1983-01-31 , Test period 1983-01-31 - 1984-01-31 # train records 3405 ,# val records 1930 , # test records 1320\n",
      "Train period: 1977-01-31 - 1982-01-31 ,val period: 1982-01-31 - 1984-01-31 , Test period 1984-01-31 - 1985-01-31 # train records 3947 ,# val records 2182 , # test records 1304\n",
      "Train period: 1978-01-31 - 1983-01-31 ,val period: 1983-01-31 - 1985-01-31 , Test period 1985-01-31 - 1986-01-31 # train records 4232 ,# val records 2624 , # test records 1458\n",
      "Train period: 1979-01-31 - 1984-01-31 ,val period: 1984-01-31 - 1986-01-31 , Test period 1986-01-31 - 1987-01-31 # train records 4949 ,# val records 2762 , # test records 1919\n",
      "Train period: 1980-01-31 - 1985-01-31 ,val period: 1985-01-31 - 1987-01-31 , Test period 1987-01-31 - 1988-01-31 # train records 5549 ,# val records 3377 , # test records 1987\n",
      "Train period: 1981-01-31 - 1986-01-31 ,val period: 1986-01-31 - 1988-01-31 , Test period 1988-01-31 - 1989-01-31 # train records 6012 ,# val records 3906 , # test records 1777\n",
      "Train period: 1982-01-31 - 1987-01-31 ,val period: 1987-01-31 - 1989-01-31 , Test period 1989-01-31 - 1990-01-31 # train records 6863 ,# val records 3764 , # test records 2049\n",
      "Train period: 1983-01-31 - 1988-01-31 ,val period: 1988-01-31 - 1990-01-31 , Test period 1990-01-31 - 1991-01-31 # train records 7988 ,# val records 3826 , # test records 1959\n",
      "Train period: 1984-01-31 - 1989-01-31 ,val period: 1989-01-31 - 1991-01-31 , Test period 1991-01-31 - 1992-01-31 # train records 8445 ,# val records 4008 , # test records 2113\n",
      "Train period: 1985-01-31 - 1990-01-31 ,val period: 1990-01-31 - 1992-01-31 , Test period 1992-01-31 - 1993-01-31 # train records 9190 ,# val records 4072 , # test records 2619\n",
      "Train period: 1986-01-31 - 1991-01-31 ,val period: 1991-01-31 - 1993-01-31 , Test period 1993-01-31 - 1994-01-31 # train records 9691 ,# val records 4732 , # test records 2583\n",
      "Train period: 1987-01-31 - 1992-01-31 ,val period: 1992-01-31 - 1994-01-31 , Test period 1994-01-31 - 1995-01-31 # train records 9885 ,# val records 5202 , # test records 3051\n",
      "Train period: 1988-01-31 - 1993-01-31 ,val period: 1993-01-31 - 1995-01-31 , Test period 1995-01-31 - 1996-01-31 # train records 10517 ,# val records 5634 , # test records 3348\n",
      "Train period: 1989-01-31 - 1994-01-31 ,val period: 1994-01-31 - 1996-01-31 , Test period 1996-01-31 - 1997-01-31 # train records 11323 ,# val records 6399 , # test records 3879\n",
      "Train period: 1990-01-31 - 1995-01-31 ,val period: 1995-01-31 - 1997-01-31 , Test period 1997-01-31 - 1998-01-31 # train records 12325 ,# val records 7227 , # test records 4797\n",
      "Train period: 1991-01-31 - 1996-01-31 ,val period: 1996-01-31 - 1998-01-31 , Test period 1998-01-31 - 1999-01-31 # train records 13714 ,# val records 8676 , # test records 4780\n",
      "Train period: 1992-01-31 - 1997-01-31 ,val period: 1997-01-31 - 1999-01-31 , Test period 1999-01-31 - 2000-01-31 # train records 15480 ,# val records 9577 , # test records 4451\n",
      "Train period: 1993-01-31 - 1998-01-31 ,val period: 1998-01-31 - 2000-01-31 , Test period 2000-01-31 - 2001-01-31 # train records 17658 ,# val records 9231 , # test records 5511\n",
      "Train period: 1994-01-31 - 1999-01-31 ,val period: 1999-01-31 - 2001-01-31 , Test period 2001-01-31 - 2002-01-31 # train records 19855 ,# val records 9962 , # test records 4865\n",
      "Train period: 1995-01-31 - 2000-01-31 ,val period: 2000-01-31 - 2002-01-31 , Test period 2002-01-31 - 2003-01-31 # train records 21255 ,# val records 10376 , # test records 4631\n",
      "Train period: 1996-01-31 - 2001-01-31 ,val period: 2001-01-31 - 2003-01-31 , Test period 2003-01-31 - 2004-01-31 # train records 23418 ,# val records 9496 , # test records 5287\n",
      "Train period: 1997-01-31 - 2002-01-31 ,val period: 2002-01-31 - 2004-01-31 , Test period 2004-01-31 - 2005-01-31 # train records 24404 ,# val records 9918 , # test records 5390\n",
      "Train period: 1998-01-31 - 2003-01-31 ,val period: 2003-01-31 - 2005-01-31 , Test period 2005-01-31 - 2006-01-31 # train records 24238 ,# val records 10677 , # test records 6472\n",
      "Train period: 1999-01-31 - 2004-01-31 ,val period: 2004-01-31 - 2006-01-31 , Test period 2006-01-31 - 2007-01-31 # train records 24745 ,# val records 11862 , # test records 7051\n",
      "Train period: 2000-01-31 - 2005-01-31 ,val period: 2005-01-31 - 2007-01-31 , Test period 2007-01-31 - 2008-01-31 # train records 25684 ,# val records 13523 , # test records 7335\n",
      "Train period: 2001-01-31 - 2006-01-31 ,val period: 2006-01-31 - 2008-01-31 , Test period 2008-01-31 - 2009-01-31 # train records 26645 ,# val records 14386 , # test records 6578\n",
      "Train period: 2002-01-31 - 2007-01-31 ,val period: 2007-01-31 - 2009-01-31 , Test period 2009-01-31 - 2010-01-31 # train records 28831 ,# val records 13913 , # test records 5023\n",
      "Train period: 2003-01-31 - 2008-01-31 ,val period: 2008-01-31 - 2010-01-31 , Test period 2010-01-31 - 2011-01-31 # train records 31535 ,# val records 11601 , # test records 5446\n",
      "Train period: 2004-01-31 - 2009-01-31 ,val period: 2009-01-31 - 2011-01-31 , Test period 2011-01-31 - 2012-01-31 # train records 32826 ,# val records 10469 , # test records 6469\n",
      "Train period: 2005-01-31 - 2010-01-31 ,val period: 2010-01-31 - 2012-01-31 , Test period 2012-01-31 - 2013-01-31 # train records 32459 ,# val records 11915 , # test records 6450\n"
     ]
    }
   ],
   "source": [
    "tscv = TimeBasedCV(train_period=60,\n",
    "                   val_period=24,\n",
    "                   test_period=12,\n",
    "                   freq='months')\n",
    "\n",
    "\n",
    "features = df.columns[~df.columns.isin(['permno', 'permno2', 'mvel12','sic2' , 'DATE2', 'risk_premium'])].tolist()\n",
    "X = df_bottom[features]\n",
    "y = df_bottom[['risk_premium']]\n",
    "\n",
    "#Empty containers to save results from each window\n",
    "\n",
    "predictions_bottom = []\n",
    "y_test_list_bottom =[]\n",
    "dates_bottom = []\n",
    "dic_r2_all_bottom = {}\n",
    "\n",
    "for train_index, val_index, test_index in tscv.split(X, first_split_date= datetime.date(1981,1,31), second_split_date= datetime.date(1991,1,31)):\n",
    "\n",
    "    X_train   = X.loc[train_index].drop('DATE', axis=1)\n",
    "    y_train = y.loc[train_index]\n",
    "    \n",
    "    X_val   = X.loc[val_index].drop('DATE', axis=1)\n",
    "    y_val = y.loc[val_index]\n",
    "\n",
    "    X_test    = X.loc[test_index].drop('DATE', axis=1)\n",
    "    y_test  = y.loc[test_index]\n",
    "\n",
    "    reg_huber = HuberRegressor(max_iter=1000)\n",
    "    reg_huber.fit(np.concatenate((X_train, X_val)), np.concatenate((y_train, y_val)))\n",
    "\n",
    "\n",
    "    preds = reg_huber.predict(X_test)    \n",
    "    predictions_bottom.append(preds)\n",
    "    dates_bottom.append(y_test.index)\n",
    "    y_test_list_bottom.append(y_test)\n",
    "    \n",
    "    r2_bottom = 1-np.sum(pow(y_test['risk_premium']-preds,2))/np.sum(pow(y_test['risk_premium'],2))\n",
    "    dic_r2_all_bottom[\"r2.\" + str(y_test.index)] = r2\n",
    "\n",
    "predictions_all_bottom= np.concatenate(predictions_bottom, axis=0)\n",
    "y_test_list_all_bottom= np.concatenate(y_test_list_bottom, axis=0) \n",
    "dates_all_bottom= np.concatenate(dates_bottom, axis=0)\n",
    "\n",
    "R2BOTTOM = r2_score(y_test_list_all_bottom, predictions_all_bottom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Huber Regression</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Full Sample</th>\n",
       "      <td>-0.006539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Large Firms</th>\n",
       "      <td>-0.054777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Small Firms</th>\n",
       "      <td>0.000026</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Huber Regression\n",
       "Full Sample         -0.006539\n",
       "Large Firms         -0.054777\n",
       "Small Firms          0.000026"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chart = np.array([[R2FULL],\n",
    "                  [R2TOP],\n",
    "                  [R2BOTTOM]])\n",
    "\n",
    "huber = pd.DataFrame(chart, columns=['Huber Regression'],\n",
    "                     index=['Full Sample', 'Large Firms', 'Small Firms'])\n",
    "\n",
    "huber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "huber.to_csv(r'huber_model.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0008624898788281987\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "features = df.columns[~df.columns.isin(['permno', 'permno2', 'mvel12','sic2' , 'DATE2', 'DATE', 'risk_premium'])].tolist()\n",
    "df['year'] = df['DATE'].dt.year\n",
    "\n",
    "X_train = df[features].loc[(df[\"year\"]>=2013) & (df[\"year\"]<=2018)]\n",
    "y_train = df[\"risk_premium\"].loc[(df[\"year\"]>=2013) & (df[\"year\"]<=2018)]\n",
    "\n",
    "X_val = df[features].loc[(df[\"year\"]>=2018) & (df[\"year\"]<=2020)]\n",
    "y_val = df[\"risk_premium\"].loc[(df[\"year\"]>=2018) & (df[\"year\"]<=2020)]\n",
    "\n",
    "\n",
    "reg_huber = HuberRegressor(max_iter=1000)\n",
    "reg_huber.fit(np.concatenate((X_train, X_val)), np.concatenate((y_train, y_val)))\n",
    "preds = reg_huber.predict(np.concatenate((X_train, X_val))) \n",
    "\n",
    "R2OOS_all = 1-np.sum(pow(np.concatenate((y_train, y_val))-preds,2))/np.sum(pow(np.concatenate((y_train, y_val)),2))\n",
    "print(R2OOS_all)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in features:\n",
    "    globals()['df_' + str(j)] =  df.copy()\n",
    "    globals()['df_' + str(j)][str(j)] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic = {}\n",
    "\n",
    "    \n",
    "for j in features:\n",
    "    df_var = globals()['df_' + str(j)]\n",
    "    \n",
    "    X_train = df[features].loc[(df[\"year\"]>=2013) & (df[\"year\"]<=2018)]\n",
    "    y_train = df[\"risk_premium\"].loc[(df[\"year\"]>=2013) & (df[\"year\"]<=2018)]\n",
    "\n",
    "    X_val = df[features].loc[(df[\"year\"]>=2018) & (df[\"year\"]<=2020)]\n",
    "    y_val = df[\"risk_premium\"].loc[(df[\"year\"]>=2018) & (df[\"year\"]<=2020)]\n",
    "    \n",
    "    reg_huber = HuberRegressor()\n",
    "    reg_huber.fit(np.concatenate((X_train, X_val)), np.concatenate((y_train, y_val)))\n",
    "    kpreds = reg_huber.predict(np.concatenate((X_train, X_val))) \n",
    "\n",
    "    R2OOS_var = 1-np.sum(pow(np.concatenate((y_train, y_val))-preds,2))/np.sum(pow(np.concatenate((y_train, y_val)),2))\n",
    "    dic['R2OOS_' + str(j)] = R2OOS_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'R2OOS_mvel1': np.float64(0.00011999259471884294),\n",
       " 'R2OOS_beta': np.float64(0.00011999259471884294),\n",
       " 'R2OOS_betasq': np.float64(0.00011999259471884294),\n",
       " 'R2OOS_chmom': np.float64(0.00011999259471884294),\n",
       " 'R2OOS_dolvol': np.float64(0.00011999259471884294),\n",
       " 'R2OOS_idiovol': np.float64(0.00011999259471884294),\n",
       " 'R2OOS_indmom': np.float64(0.00011999259471884294),\n",
       " 'R2OOS_mom1m': np.float64(0.00011999259471884294),\n",
       " 'R2OOS_mom6m': np.float64(0.00011999259471884294),\n",
       " 'R2OOS_mom12m': np.float64(0.00011999259471884294),\n",
       " 'R2OOS_mom36m': np.float64(0.00011999259471884294),\n",
       " 'R2OOS_pricedelay': np.float64(0.00011999259471884294),\n",
       " 'R2OOS_turn': np.float64(0.00011999259471884294),\n",
       " 'R2OOS_absacc': np.float64(0.00011999259471884294),\n",
       " 'R2OOS_acc': np.float64(0.00011999259471884294),\n",
       " 'R2OOS_age': np.float64(0.00011999259471884294),\n",
       " 'R2OOS_agr': np.float64(0.00011999259471884294),\n",
       " 'R2OOS_bm': np.float64(0.00011999259471884294),\n",
       " 'R2OOS_bm_ia': np.float64(0.00011999259471884294),\n",
       " 'R2OOS_cashdebt': np.float64(0.00011999259471884294),\n",
       " 'R2OOS_cashpr': np.float64(0.00011999259471884294),\n",
       " 'R2OOS_cfp': np.float64(0.00011999259471884294),\n",
       " 'R2OOS_cfp_ia': np.float64(0.00011999259471884294),\n",
       " 'R2OOS_chatoia': np.float64(0.00011999259471884294),\n",
       " 'R2OOS_chcsho': np.float64(0.00011999259471884294),\n",
       " 'R2OOS_chempia': np.float64(0.00011999259471884294),\n",
       " 'R2OOS_chinv': np.float64(0.00011999259471884294),\n",
       " 'R2OOS_chpmia': np.float64(0.00011999259471884294),\n",
       " 'R2OOS_convind': np.float64(0.00011999259471884294),\n",
       " 'R2OOS_currat': np.float64(0.00011999259471884294),\n",
       " 'R2OOS_depr': np.float64(0.00011999259471884294),\n",
       " 'R2OOS_divi': np.float64(0.00011999259471884294),\n",
       " 'R2OOS_divo': np.float64(0.00011999259471884294),\n",
       " 'R2OOS_dy': np.float64(0.00011999259471884294),\n",
       " 'R2OOS_egr': np.float64(0.00011999259471884294),\n",
       " 'R2OOS_ep': np.float64(0.00011999259471884294),\n",
       " 'R2OOS_gma': np.float64(0.00011999259471884294),\n",
       " 'R2OOS_grcapx': np.float64(0.00011999259471884294),\n",
       " 'R2OOS_grltnoa': np.float64(0.00011999259471884294),\n",
       " 'R2OOS_herf': np.float64(0.00011999259471884294),\n",
       " 'R2OOS_hire': np.float64(0.00011999259471884294),\n",
       " 'R2OOS_invest': np.float64(0.00011999259471884294),\n",
       " 'R2OOS_lev': np.float64(0.00011999259471884294),\n",
       " 'R2OOS_lgr': np.float64(0.00011999259471884294),\n",
       " 'R2OOS_mve_ia': np.float64(0.00011999259471884294),\n",
       " 'R2OOS_operprof': np.float64(0.00011999259471884294),\n",
       " 'R2OOS_orgcap': np.float64(0.00011999259471884294),\n",
       " 'R2OOS_pchcapx_ia': np.float64(0.00011999259471884294),\n",
       " 'R2OOS_pchcurrat': np.float64(0.00011999259471884294),\n",
       " 'R2OOS_pchdepr': np.float64(0.00011999259471884294),\n",
       " 'R2OOS_pchgm_pchsale': np.float64(0.00011999259471884294),\n",
       " 'R2OOS_pchquick': np.float64(0.00011999259471884294),\n",
       " 'R2OOS_pchsale_pchinvt': np.float64(0.00011999259471884294),\n",
       " 'R2OOS_pchsale_pchrect': np.float64(0.00011999259471884294),\n",
       " 'R2OOS_pchsale_pchxsga': np.float64(0.00011999259471884294),\n",
       " 'R2OOS_pchsaleinv': np.float64(0.00011999259471884294),\n",
       " 'R2OOS_pctacc': np.float64(0.00011999259471884294),\n",
       " 'R2OOS_ps': np.float64(0.00011999259471884294),\n",
       " 'R2OOS_quick': np.float64(0.00011999259471884294),\n",
       " 'R2OOS_rd': np.float64(0.00011999259471884294),\n",
       " 'R2OOS_roic': np.float64(0.00011999259471884294),\n",
       " 'R2OOS_salecash': np.float64(0.00011999259471884294),\n",
       " 'R2OOS_saleinv': np.float64(0.00011999259471884294),\n",
       " 'R2OOS_salerec': np.float64(0.00011999259471884294),\n",
       " 'R2OOS_securedind': np.float64(0.00011999259471884294),\n",
       " 'R2OOS_sgr': np.float64(0.00011999259471884294),\n",
       " 'R2OOS_sin': np.float64(0.00011999259471884294),\n",
       " 'R2OOS_sp': np.float64(0.00011999259471884294),\n",
       " 'R2OOS_tang': np.float64(0.00011999259471884294),\n",
       " 'R2OOS_tb': np.float64(0.00011999259471884294),\n",
       " 'R2OOS_baspread': np.float64(0.00011999259471884294),\n",
       " 'R2OOS_ill': np.float64(0.00011999259471884294),\n",
       " 'R2OOS_maxret': np.float64(0.00011999259471884294),\n",
       " 'R2OOS_retvol': np.float64(0.00011999259471884294),\n",
       " 'R2OOS_std_dolvol': np.float64(0.00011999259471884294),\n",
       " 'R2OOS_std_turn': np.float64(0.00011999259471884294),\n",
       " 'R2OOS_zerotrade': np.float64(0.00011999259471884294),\n",
       " 'R2OOS_macro_dp': np.float64(0.00011999259471884294),\n",
       " 'R2OOS_macro_ep': np.float64(0.00011999259471884294),\n",
       " 'R2OOS_macro_bm': np.float64(0.00011999259471884294),\n",
       " 'R2OOS_macro_ntis': np.float64(0.00011999259471884294),\n",
       " 'R2OOS_macro_tbl': np.float64(0.00011999259471884294),\n",
       " 'R2OOS_macro_tms': np.float64(0.00011999259471884294),\n",
       " 'R2OOS_macro_dfy': np.float64(0.00011999259471884294),\n",
       " 'R2OOS_macro_svar': np.float64(0.00011999259471884294),\n",
       " 'R2OOS_macro_mkt-rf': np.float64(0.00011999259471884294),\n",
       " 'R2OOS_macro_hml': np.float64(0.00011999259471884294),\n",
       " 'R2OOS_macro_smb': np.float64(0.00011999259471884294),\n",
       " 'R2OOS_year': np.float64(0.00011999259471884294)}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature</th>\n",
       "      <th>R2OOS</th>\n",
       "      <th>red_R2OOS</th>\n",
       "      <th>var_imp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mvel1</td>\n",
       "      <td>0.00012</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>beta</td>\n",
       "      <td>0.00012</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>betasq</td>\n",
       "      <td>0.00012</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>chmom</td>\n",
       "      <td>0.00012</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dolvol</td>\n",
       "      <td>0.00012</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Feature    R2OOS  red_R2OOS  var_imp\n",
       "0   mvel1  0.00012        0.0      NaN\n",
       "1    beta  0.00012        0.0      NaN\n",
       "2  betasq  0.00012        0.0      NaN\n",
       "3   chmom  0.00012        0.0      NaN\n",
       "4  dolvol  0.00012        0.0      NaN"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(dic.items())\n",
    "imp=pd.DataFrame(dic.items(), columns=['Feature', 'R2OOS'])\n",
    "# Feature: name of the variable whose values are set to zero\n",
    "imp[\"Feature\"] = imp[\"Feature\"].str[6:]\n",
    "\n",
    "# Calculate reduction in predictive R2OOS \n",
    "imp[\"red_R2OOS\"] = R2OOS_all -imp[\"R2OOS\"]\n",
    "imp[\"var_imp\"] = imp[\"red_R2OOS\"]/np.sum(imp[\"red_R2OOS\"])\n",
    "imp=imp.sort_values(by = ['var_imp'], ascending = False)\n",
    "imp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = predictions_all_full.tolist()\n",
    "y_true = y_test_list_all_full.tolist()\n",
    "i = dates_all_full.tolist()\n",
    "\n",
    "results = pd.DataFrame(\n",
    "    {'identifier': i,\n",
    "     'yhat': yhat,\n",
    "     'y_true': y_true\n",
    "    })\n",
    "\n",
    "\n",
    "results[\"identifier\"]= results[\"identifier\"].astype(\"str\")\n",
    "results[\"date\"] = results[\"identifier\"].str[12:22]\n",
    "results[\"id\"] = results[\"identifier\"].str[35:40]\n",
    "results.drop([\"identifier\"],axis = 1, inplace=True)\n",
    "results['date'] = pd.to_datetime(results['date'], format='%Y-%m-%d')\n",
    "results['MonthYear'] = results['date'].dt.to_period('M')\n",
    "results = results.sort_values(by = ['date', 'id'], ascending = True)\n",
    "results = results.set_index(['MonthYear','id'])\n",
    "results.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results['yhat'] = results['yhat'].apply(lambda x: x[0])\n",
    "results['y_true'] = results['y_true'].apply(lambda x: x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df[['mvel12', 'macro_tbl', 'macro_svar']].copy()\n",
    "data.reset_index(inplace=True)\n",
    "data['permno2'] = data['permno2'].astype('str')\n",
    "data['MonthYear'] = data['DATE2'].dt.to_period('M')\n",
    "data.drop('DATE2', axis=1, inplace=True)\n",
    "data.rename(columns={'permno2': 'id'}, inplace=True)\n",
    "data.rename(columns={'mvel12': 'market_cap'}, inplace=True)\n",
    "data.rename(columns={'macro_tbl': 'risk_free_rate'}, inplace=True)\n",
    "data = data.set_index(['MonthYear','id'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigdata = pd.merge(results, data,left_index=True, right_index=True)\n",
    "bigdata.reset_index(inplace=True)\n",
    "bigdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigdata['returns'] = bigdata['y_true'] + bigdata['risk_free_rate']\n",
    "bigdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigdata['MonthYear1'] = bigdata['MonthYear'].copy()\n",
    "bigdata['MonthYear'] = bigdata['MonthYear'].astype('int64')\n",
    "bigdata['NumMonth'] = bigdata['MonthYear'] - 179\n",
    "bigdata['NumMonth'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in bigdata['NumMonth'].unique():\n",
    "    globals()['df_' + str(i)] = bigdata[bigdata['NumMonth'] == i]\n",
    "\n",
    "for i in bigdata[\"NumMonth\"].unique():\n",
    "    globals()['df_' + str(i)][\"rank\"]= globals()['df_' + str(i)]['yhat'].rank(method='first')\n",
    "    \n",
    "for i in bigdata[\"NumMonth\"].unique():\n",
    "    globals()['df_' + str(i)][\"DecileRank\"]=pd.qcut(globals()['df_' + str(i)]['rank'].values, 10, labels = False)\n",
    "\n",
    "#Drop normal rank, retain only decile ranks \n",
    "for i in bigdata[\"NumMonth\"].unique():\n",
    "     globals()['df_' + str(i)].drop('rank', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in bigdata[\"NumMonth\"].unique():\n",
    "    for j,g in globals()['df_' + str(i)].groupby('DecileRank'):\n",
    "        globals()['df_' + str(i)+ \"_\" + str(j)] =  g\n",
    "\n",
    "for j in np.arange(0,10,1):\n",
    "    globals()['rank_' + str(j)] = pd.concat([globals()['df_1_'+ str(j)], globals()['df_2_'+ str(j)]], axis=0)\n",
    "    \n",
    "# Generate 10 Dataframes for the 10 Decile portfolios 0-9: rank_9: top portfolio, rank_0: bottom portfolio\n",
    "for i in np.arange(2,361,1):\n",
    "    for j in np.arange(0,10,1):\n",
    "        globals()['rank_' + str(j)] = pd.concat([globals()['rank_' + str(j)], globals()['df_' + str(i+1)+ \"_\" + str(j)]], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get equal und value weights per stock per month in each decile portfolio:\n",
    "# ew = Equally weighted\n",
    "# vw = Value weighted\n",
    "\n",
    "for j in np.arange(0,10,1):\n",
    "    globals()['rank_' + str(j)][\"eq_weights\"] = 1/globals()['rank_' + str(j)].groupby('MonthYear')[\"id\"].transform('size')\n",
    "    globals()['rank_' + str(j)][\"me_weights\"] = globals()['rank_' + str(j)][\"market_cap\"]/globals()['rank_' + str(j)].groupby('MonthYear')[\"market_cap\"].transform('sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weighted excess return per stock in t+1\n",
    "for j in np.arange(0,10,1):\n",
    "    globals()['rank_' + str(j)]['excess_return_stock_ew'] = globals()['rank_' + str(j)][\"y_true\"]*globals()['rank_' + str(j)][\"eq_weights\"]\n",
    "    globals()['rank_' + str(j)]['excess_return_stock_vw'] = globals()['rank_' + str(j)][\"y_true\"]*globals()['rank_' + str(j)][\"me_weights\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weighted return per stock in t+1 (to use for the sharpe ratio)\n",
    "for j in np.arange(0,10,1):\n",
    "    globals()['rank_' + str(j)]['return_stock_ew'] = globals()['rank_' + str(j)][\"returns\"]*globals()['rank_' + str(j)][\"eq_weights\"]\n",
    "    globals()['rank_' + str(j)]['return_stock_vw'] = globals()['rank_' + str(j)][\"returns\"]*globals()['rank_' + str(j)][\"me_weights\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Portfolio excess return in t+1\n",
    "for j in np.arange(0,10,1):\n",
    "    globals()['rank_' + str(j)]['excess_return_portfolio_ew'] = globals()['rank_' + str(j)].groupby('MonthYear')[\"excess_return_stock_ew\"].transform('sum')\n",
    "    globals()['rank_' + str(j)]['excess_return_portfolio_vw'] = globals()['rank_' + str(j)].groupby('MonthYear')[\"excess_return_stock_vw\"].transform('sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Portfolio return in t+1 (to use for the sharpe ratio) \n",
    "for j in np.arange(0,10,1):\n",
    "    globals()['rank_' + str(j)]['return_portfolio_ew'] = globals()['rank_' + str(j)].groupby('MonthYear')[\"return_stock_ew\"].transform('sum')\n",
    "    globals()['rank_' + str(j)]['return_portfolio_vw'] = globals()['rank_' + str(j)].groupby('MonthYear')[\"return_stock_vw\"].transform('sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Weighted predicted excess return per stock in t+1\n",
    "for j in np.arange(0,10,1):\n",
    "    globals()['rank_' + str(j)]['pred_excess_return_stock_ew'] = globals()['rank_' + str(j)][\"yhat\"]*globals()['rank_' + str(j)][\"eq_weights\"]\n",
    "    globals()['rank_' + str(j)]['pred_excess_return_stock_vw'] = globals()['rank_' + str(j)][\"yhat\"]*globals()['rank_' + str(j)][\"me_weights\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Portfolio predicted excess return in t+1\n",
    "for j in np.arange(0,10,1):\n",
    "    globals()['rank_' + str(j)]['pred_excess_return_portfolio_ew'] = globals()['rank_' + str(j)].groupby('MonthYear')[\"pred_excess_return_stock_ew\"].transform('sum')\n",
    "    globals()['rank_' + str(j)]['pred_excess_return_portfolio_vw'] = globals()['rank_' + str(j)].groupby('MonthYear')[\"pred_excess_return_stock_vw\"].transform('sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate dataframes, containing the portfolio returns on mohtly basis for each decile portfolio\n",
    "# e.g., montly_rank_0: dataframe, containing only the monthly portfolio excess returns (predicted and real) \n",
    "# for  the bottom rank\n",
    "\n",
    "for j in np.arange(0,10,1):\n",
    "    globals()['montly_rank_' + str(j)] = globals()['rank_' + str(j)][[\"MonthYear1\", \"DecileRank\",\n",
    "                                                                      \"excess_return_portfolio_ew\",\n",
    "                                                                      \"excess_return_portfolio_vw\", \n",
    "                                                                      \"pred_excess_return_portfolio_ew\",\n",
    "                                                                      \"pred_excess_return_portfolio_vw\",\n",
    "                                                                      \"return_portfolio_ew\",\n",
    "                                                                      \"return_portfolio_vw\"]]\n",
    "    \n",
    "for j in np.arange(0,10,1):\n",
    "    globals()['montly_rank_' + str(j)]=globals()['montly_rank_' + str(j)].drop_duplicates()\n",
    "    globals()['montly_rank_' + str(j)]=globals()['montly_rank_' + str(j)].set_index(\"MonthYear1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in np.arange(0,10,1):\n",
    "    #Time-series average of realized excess returns\n",
    "    globals()[\"ew_mean_return_rank_\" +  str(j)]= globals()['montly_rank_' + str(j)][\"excess_return_portfolio_ew\"].mean()\n",
    "    globals()[\"vw_mean_return_rank_\" +  str(j)]= globals()['montly_rank_' + str(j)][\"excess_return_portfolio_vw\"].mean()\n",
    "    #Time-series average of predicted excess returns\n",
    "    globals()[\"ew_mean_pred_return_rank_\" +  str(j)]= globals()['montly_rank_' + str(j)][\"pred_excess_return_portfolio_ew\"].mean()\n",
    "    globals()[\"vw_mean_pred_return_rank_\" +  str(j)]= globals()['montly_rank_' + str(j)][\"pred_excess_return_portfolio_vw\"].mean()\n",
    "    #Standard deviation of realized excess returns\n",
    "    globals()[\"std_ew_rank_\" +  str(j)]= globals()['montly_rank_' + str(j)][\"excess_return_portfolio_ew\"].std()\n",
    "    globals()[\"std_vw_rank_\" +  str(j)]= globals()['montly_rank_' + str(j)][\"excess_return_portfolio_vw\"].std()\n",
    "    #Annualized sharpe ratio of realized excess returns\n",
    "    globals()[\"sharpe_ew_rank_\" +  str(j)]= (globals()['montly_rank_' + str(j)][\"excess_return_portfolio_ew\"].mean()/globals()['montly_rank_' + str(j)][\"return_portfolio_ew\"].std())* np.sqrt(12)\n",
    "    globals()[\"sharpe_vw_rank_\" +  str(j)]= (globals()['montly_rank_' + str(j)][\"excess_return_portfolio_vw\"].mean()/globals()['montly_rank_' + str(j)][\"return_portfolio_vw\"].std())* np.sqrt(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the zero-net-investment long-short portfolio the top (long) and bottom(short) decile portfolios are needed\n",
    "\n",
    "long_monthly = rank_9[[\"NumMonth\",\"MonthYear1\", \"DecileRank\", \"excess_return_portfolio_ew\",\n",
    "                       \"excess_return_portfolio_vw\",\"pred_excess_return_portfolio_ew\",\n",
    "                       \"pred_excess_return_portfolio_vw\",\"return_portfolio_ew\",\n",
    "                        \"return_portfolio_vw\"]].drop_duplicates()\n",
    "\n",
    "short_monthly = rank_0[[\"NumMonth\",\"MonthYear1\", \"DecileRank\", \"excess_return_portfolio_ew\",\n",
    "                       \"excess_return_portfolio_vw\",\"pred_excess_return_portfolio_ew\",\n",
    "                       \"pred_excess_return_portfolio_vw\",\"return_portfolio_ew\",\n",
    "                        \"return_portfolio_vw\"]].drop_duplicates()\n",
    "\n",
    "# Create a column, indication the stategy \n",
    "long_monthly[\"Strategy\"]= \"long\"\n",
    "short_monthly[\"Strategy\"]= \"short\"\n",
    "\n",
    "# Merge to get the zero net investment portfolio\n",
    "zeronet_monthly= pd.concat([long_monthly, short_monthly])\n",
    "zeronet_monthly = zeronet_monthly.sort_values(by = ['NumMonth',\"Strategy\"])\n",
    "zeronet_monthly[\"return_portfolio_vw\"] = zeronet_monthly[\"return_portfolio_vw\"].astype('float64')\n",
    "\n",
    "#Create two new columns containing the exess return of the portfolio and initially set the values to zero.\n",
    "zeronet_monthly[\"excess_return_zeronet_ew\"] =0\n",
    "zeronet_monthly[\"excess_return_zeronet_vw\"] =0\n",
    "\n",
    "# excess return zeronet in t = (weigted excess return long in t) - (weigted excess return short in t)\n",
    "for i in range(0, len(zeronet_monthly)):\n",
    "    if zeronet_monthly.iloc[i,9] == \"long\":\n",
    "        zeronet_monthly.iloc[i, -2] = zeronet_monthly.iloc[i, 3]-zeronet_monthly.iloc[i+1, 3]\n",
    "    else:\n",
    "        zeronet_monthly.iloc[i, -2] = zeronet_monthly.iloc[i-1, 3]-zeronet_monthly.iloc[i, 3]\n",
    "        \n",
    "for i in range(0, len(zeronet_monthly)):\n",
    "    if zeronet_monthly.iloc[i,9] == \"long\":\n",
    "        zeronet_monthly.iloc[i, -1] = zeronet_monthly.iloc[i, 4]-zeronet_monthly.iloc[i+1, 4]\n",
    "    else:\n",
    "        zeronet_monthly.iloc[i, -1] = zeronet_monthly.iloc[i-1, 4]-zeronet_monthly.iloc[i, 4]\n",
    "\n",
    "#Create two new columns containing predicted the exess return of the portfolio and initially set the values to zero.\n",
    "zeronet_monthly[\"pred_excess_return_zeronet_ew\"] =0\n",
    "zeronet_monthly[\"pred_excess_return_zeronet_vw\"] =0\n",
    "\n",
    "# predicted excess return zeronet in t = (weigted predicted excess return long in t) - (weigted predicted excess return short in t)\n",
    "for i in range(0, len(zeronet_monthly)):\n",
    "    if zeronet_monthly.iloc[i,9] == \"long\":\n",
    "        zeronet_monthly.iloc[i, -2] = zeronet_monthly.iloc[i, 5]-zeronet_monthly.iloc[i+1, 5]\n",
    "    else:\n",
    "        zeronet_monthly.iloc[i, -2] = zeronet_monthly.iloc[i-1, 5]-zeronet_monthly.iloc[i, 5]\n",
    "        \n",
    "for i in range(0, len(zeronet_monthly)):\n",
    "    if zeronet_monthly.iloc[i,9] == \"long\":\n",
    "        zeronet_monthly.iloc[i, -1] = zeronet_monthly.iloc[i, 6]-zeronet_monthly.iloc[i+1, 6]\n",
    "    else:\n",
    "        zeronet_monthly.iloc[i, -1] = zeronet_monthly.iloc[i-1, 6]-zeronet_monthly.iloc[i, 6]\n",
    "\n",
    "#Create two new columns containing return of the portfolio and initially set the values to zero.       \n",
    "zeronet_monthly[\"return_zeronet_ew\"] =0\n",
    "zeronet_monthly[\"return_zeronet_vw\"] =0\n",
    "\n",
    "# return zeronet in t = (weigted return long in t) - (weigted return short in t)\n",
    "for i in range(0, len(zeronet_monthly)):\n",
    "    if zeronet_monthly.iloc[i,9] == \"long\":\n",
    "        zeronet_monthly.iloc[i, -2] = zeronet_monthly.iloc[i, 7]-zeronet_monthly.iloc[i+1, 7]\n",
    "    else:\n",
    "        zeronet_monthly.iloc[i, -2] = zeronet_monthly.iloc[i-1, 7]-zeronet_monthly.iloc[i, 7]\n",
    "        \n",
    "for i in range(0, len(zeronet_monthly)):\n",
    "    if zeronet_monthly.iloc[i,9] == \"long\":\n",
    "        zeronet_monthly.iloc[i, -1] = zeronet_monthly.iloc[i, 8]-zeronet_monthly.iloc[i+1, 8]\n",
    "    else:\n",
    "        zeronet_monthly.iloc[i, -1] = zeronet_monthly.iloc[i-1, 8]-zeronet_monthly.iloc[i, 8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zeronet_monthly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Only the measures at portfolio level are needed\n",
    "zeronet_monthly = zeronet_monthly[['NumMonth', 'MonthYear1', 'excess_return_zeronet_ew',\n",
    "                                   'excess_return_zeronet_vw', 'pred_excess_return_zeronet_ew',\n",
    "                                   'pred_excess_return_zeronet_vw','return_zeronet_ew',\n",
    "                                   'return_zeronet_vw']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Calculate zero-net portfolio performance measures                                            \n",
    "#Time-series average of realized excess returns                                             \n",
    "ew_mean_return_zeronet= zeronet_monthly[\"excess_return_zeronet_ew\"].mean()\n",
    "vw_mean_return_zeronet= zeronet_monthly[\"excess_return_zeronet_vw\"].mean()\n",
    "#Time-series average of predicted excess returns\n",
    "ew_mean_pred_return_zeronet = zeronet_monthly[\"pred_excess_return_zeronet_ew\"].mean()\n",
    "vw_mean_pred_return_zeronet = zeronet_monthly[\"pred_excess_return_zeronet_vw\"].mean()\n",
    "#Standard deviation of realized excess returns\n",
    "std_ew_zeronet = zeronet_monthly[\"excess_return_zeronet_ew\"].std()\n",
    "std_vw_zeronet = zeronet_monthly[\"excess_return_zeronet_vw\"].std()\n",
    "#Annualized sharpe ratio of realized excess returns\n",
    "sharpe_ew_zeronet = (zeronet_monthly[\"excess_return_zeronet_ew\"].mean()/zeronet_monthly[\"return_zeronet_ew\"].std())* np.sqrt(12)\n",
    "sharpe_vw_zeronet = (zeronet_monthly[\"excess_return_zeronet_vw\"].mean()/zeronet_monthly[\"return_zeronet_vw\"].std())* np.sqrt(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chart_np = np.array([[ew_mean_pred_return_rank_0, ew_mean_return_rank_0, std_ew_rank_0, sharpe_ew_rank_0],\n",
    "                     [ew_mean_pred_return_rank_1, ew_mean_return_rank_1, std_ew_rank_1, sharpe_ew_rank_1],\n",
    "                     [ew_mean_pred_return_rank_2, ew_mean_return_rank_2, std_ew_rank_2, sharpe_ew_rank_2],\n",
    "                     [ew_mean_pred_return_rank_3, ew_mean_return_rank_3, std_ew_rank_3, sharpe_ew_rank_3],\n",
    "                     [ew_mean_pred_return_rank_4, ew_mean_return_rank_4, std_ew_rank_4, sharpe_ew_rank_4],\n",
    "                     [ew_mean_pred_return_rank_5, ew_mean_return_rank_5, std_ew_rank_5, sharpe_ew_rank_5],\n",
    "                     [ew_mean_pred_return_rank_6, ew_mean_return_rank_6, std_ew_rank_6, sharpe_ew_rank_6],\n",
    "                     [ew_mean_pred_return_rank_7, ew_mean_return_rank_7, std_ew_rank_7, sharpe_ew_rank_7],\n",
    "                     [ew_mean_pred_return_rank_8, ew_mean_return_rank_8, std_ew_rank_8, sharpe_ew_rank_8],\n",
    "                     [ew_mean_pred_return_rank_9, ew_mean_return_rank_9, std_ew_rank_9, sharpe_ew_rank_9],\n",
    "                     [ew_mean_pred_return_zeronet, ew_mean_return_zeronet, std_ew_zeronet, sharpe_ew_zeronet]])\n",
    "\n",
    "ew_df = pd.DataFrame(chart_np, columns=['Pred', 'Real', 'Std', 'Sharpe'],\n",
    "                              index=['Low (L)', '2', '3', '4', '5','6','7','8',\"9\",'High (H)', \"H-L\"])\n",
    "\n",
    "ew_df['Pred'] = pd.Series([\"{0:.2f}%\".format(val) for val in ew_df['Pred']], index = ew_df.index)\n",
    "ew_df['Real'] = pd.Series([\"{0:.2f}%\".format(val) for val in ew_df['Real']], index = ew_df.index)\n",
    "ew_df['Std'] = pd.Series([\"{0:.2f}%\".format(val) for val in ew_df['Std']], index = ew_df.index)\n",
    "ew_df['Sharpe'] = pd.Series([(\"%.2f\" % round(val, 2)) for val in ew_df['Sharpe']], index = ew_df.index)\n",
    "ew_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "statclass",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
