{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the stock data\n",
    "df = pd.read_csv('data/return_predictability_data.csv')\n",
    "\n",
    "# The industry dataframe\n",
    "# ind=pd.read_csv('data/industry.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[~np.isnan(df['bm'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcSMB(df):\n",
    "    \"\"\"\n",
    "    Returns SMB for FF\n",
    "    \"\"\"\n",
    "    # Define Quantile\n",
    "    SQuantile = 0.3\n",
    "    LQuantile = 0.7\n",
    "    df[\"SMB\"] = \"\"\n",
    "\n",
    "    # Assigns stock size based on market cap\n",
    "    df.SMB[df.mvel1.quantile(q=SQuantile) > df.mvel1] = \"SCap\"\n",
    "    df.SMB[(df.mvel1 > df.mvel1.quantile(SQuantile)) & (df.mvel1 < df.mvel1.quantile(q=LQuantile))] = \"MCap\"\n",
    "    df.SMB[df.mvel1.quantile(q=LQuantile) < df.mvel1] = \"LCap\"\n",
    "\n",
    "    # Calculate average return of stocks in portfolio subset based on size\n",
    "    SmallCapReturn = df.risk_premium.loc[df['SMB'] == \"SCap\"].mean()\n",
    "    LargeCapReturn = df.risk_premium.loc[df['SMB'] == \"LCap\"].mean()\n",
    "\n",
    "    # Returns SMB based on definition\n",
    "    SMB = SmallCapReturn - LargeCapReturn\n",
    "    print(round(SMB, 4))\n",
    "    return round(SMB, 4)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcHML(df):\n",
    "    \"\"\"\n",
    "    Returns HML for FF\n",
    "    \"\"\"\n",
    "\n",
    "    # Assigns stock size based on market cap\n",
    "    SQuantile = 0.3\n",
    "    LQuantile = 0.7\n",
    "    df[\"HML\"] = \"\"\n",
    "\n",
    "    #Assign stock size based on market cap\n",
    "    df.HML[df.bm <= df.bm.quantile(q=SQuantile)] = \"SValue\"\n",
    "    df.HML[(df.bm > df.bm.quantile(q=SQuantile)) & (df.bm < df.bm.quantile(q=LQuantile))] = \"MValue\"\n",
    "    df.HML[df.bm >= df.bm.quantile(q=LQuantile)] = \"LValue\"\n",
    "\n",
    "    #Calculates average return of stocks in portfolio subset based on size\n",
    "    SmallValueReturn = df.risk_premium.loc[df['HML'] == \"SValue\"].mean()\n",
    "    LargeValueReturn = df.risk_premium.loc[df[\"HML\"] == \"LValue\"].mean()\n",
    "\n",
    "    # Return SMB based on definition\n",
    "    HML = SmallValueReturn - LargeValueReturn\n",
    "    print(round(HML, 4))\n",
    "    return round(HML, 4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.assign(RM_RF=lambda x: (x['risk_premium'] - x['macro_tbl'])/x['beta'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DATE\n",
       "1962-07-31   -5.441999\n",
       "1962-08-31   -0.646829\n",
       "1962-09-28   -7.485565\n",
       "1962-10-31   -7.014487\n",
       "1962-11-30    6.929655\n",
       "                ...   \n",
       "2021-07-30   -0.733862\n",
       "2021-08-31    4.917440\n",
       "2021-09-30    1.309428\n",
       "2021-10-29    8.932851\n",
       "2021-11-30   -2.861788\n",
       "Name: RM_RF, Length: 713, dtype: float64"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('DATE')['RM_RF'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "-2.3033\n",
      "4.8829\n",
      "-3.0822\n",
      "-2.6332\n",
      "-1.755\n",
      "-1.5756\n",
      "-4.6442\n",
      "0.7038\n",
      "-2.2953\n",
      "2.0675\n",
      "1.4554\n",
      "1.1627\n",
      "2.7908\n",
      "-1.2658\n",
      "0.0294\n",
      "-0.8295\n",
      "-0.7222\n",
      "0.7571\n",
      "-0.1468\n",
      "-0.067\n",
      "-0.8816\n",
      "-0.3119\n",
      "0.6605\n",
      "-1.3638\n",
      "-0.841\n",
      "1.935\n",
      "-1.9976\n",
      "0.8379\n",
      "-4.8326\n",
      "3.5556\n",
      "-1.7523\n",
      "-2.1131\n",
      "0.6991\n",
      "0.858\n",
      "0.5614\n",
      "1.2672\n",
      "1.197\n",
      "0.2726\n",
      "-1.2462\n",
      "1.337\n",
      "-0.6108\n",
      "-0.0489\n",
      "-1.2145\n",
      "2.7357\n",
      "2.4648\n",
      "-0.1965\n",
      "2.651\n",
      "0.2467\n",
      "-0.6773\n",
      "1.9117\n",
      "-1.2104\n",
      "3.1252\n",
      "-1.6358\n",
      "2.5378\n",
      "-2.8782\n",
      "3.5282\n",
      "0.2042\n",
      "-0.3653\n",
      "2.4904\n",
      "-4.358\n",
      "-2.0371\n",
      "2.2576\n",
      "0.6667\n",
      "2.7798\n",
      "-1.0242\n",
      "3.0104\n",
      "-2.5861\n",
      "3.02\n",
      "-1.2444\n",
      "3.7267\n",
      "-3.5149\n",
      "4.0571\n",
      "-4.1622\n",
      "4.9321\n",
      "-2.3583\n",
      "2.8013\n",
      "3.346\n",
      "-0.8921\n",
      "-1.3881\n",
      "1.0934\n",
      "3.6703\n",
      "-2.9759\n",
      "-2.0959\n",
      "-0.27\n",
      "0.3641\n",
      "0.273\n",
      "0.7454\n",
      "-2.6893\n",
      "0.1909\n",
      "-0.6598\n",
      "2.6998\n",
      "-5.7726\n",
      "3.1866\n",
      "2.3266\n",
      "0.8106\n",
      "-1.1766\n",
      "-5.7349\n",
      "12.1411\n",
      "0.3556\n",
      "4.9311\n",
      "-0.4824\n",
      "2.3851\n",
      "-0.4286\n",
      "1.5017\n",
      "-1.413\n",
      "5.3374\n",
      "-4.0852\n",
      "10.1701\n",
      "-4.2983\n",
      "5.3456\n",
      "-1.9072\n",
      "1.8797\n",
      "-0.5439\n",
      "7.4501\n",
      "-0.1328\n",
      "4.9977\n",
      "2.1884\n",
      "-1.2615\n",
      "-1.8028\n",
      "8.4361\n",
      "-7.2059\n",
      "12.5313\n",
      "-0.4339\n",
      "-3.2271\n",
      "-0.018\n",
      "-1.3842\n",
      "-1.0922\n",
      "7.5987\n",
      "-2.8619\n",
      "11.155\n",
      "-1.4373\n",
      "1.1169\n",
      "-4.7406\n",
      "0.9305\n",
      "-1.8413\n",
      "3.4843\n",
      "-1.2898\n",
      "3.0726\n",
      "-1.8301\n",
      "-0.4806\n",
      "1.1452\n",
      "0.1165\n",
      "0.2265\n",
      "6.4824\n",
      "-0.5615\n",
      "0.7336\n",
      "-1.6071\n",
      "-3.1398\n",
      "-0.3416\n",
      "-0.5664\n",
      "1.3865\n",
      "-2.594\n",
      "-0.6607\n",
      "1.6172\n",
      "-0.1528\n",
      "-5.8441\n",
      "-1.976\n",
      "-4.7357\n",
      "3.7418\n",
      "-0.9552\n",
      "1.2539\n",
      "-1.7493\n",
      "4.4541\n",
      "5.3394\n",
      "0.1061\n",
      "-3.1853\n",
      "0.6921\n",
      "-8.85\n",
      "-2.7391\n",
      "8.956\n",
      "-1.3627\n",
      "-3.5785\n",
      "-4.7436\n",
      "-2.2501\n",
      "-7.9375\n",
      "-7.2186\n",
      "-1.5053\n",
      "-0.5098\n",
      "-2.9072\n",
      "-3.9471\n",
      "0.4421\n",
      "-4.5741\n",
      "0.427\n",
      "1.0196\n",
      "4.9552\n",
      "13.681\n",
      "-0.6822\n",
      "-6.0096\n",
      "0.524\n",
      "-10.0431\n",
      "-2.7202\n",
      "0.5884\n",
      "-3.8658\n",
      "16.6152\n",
      "0.8186\n",
      "6.5102\n",
      "1.3292\n",
      "2.086\n",
      "0.328\n",
      "-1.2187\n",
      "0.4398\n",
      "-3.3375\n",
      "2.0749\n",
      "-3.5267\n",
      "-0.2594\n",
      "-2.3643\n",
      "-0.6761\n",
      "-1.2503\n",
      "1.5326\n",
      "0.1192\n",
      "1.0048\n",
      "-2.1596\n",
      "1.7798\n",
      "-3.7081\n",
      "-0.6279\n",
      "3.0553\n",
      "-5.643\n",
      "15.9737\n",
      "-0.0204\n",
      "2.9739\n",
      "1.3905\n",
      "-0.8748\n",
      "-1.3319\n",
      "-0.5232\n",
      "2.1912\n",
      "-4.479\n",
      "0.8273\n",
      "-0.8367\n",
      "-0.9904\n",
      "-1.1121\n",
      "-1.8759\n",
      "-3.6828\n",
      "0.3968\n",
      "-1.8303\n",
      "0.9074\n",
      "-3.7317\n",
      "-1.7168\n",
      "-3.3325\n",
      "2.0256\n",
      "-3.5336\n",
      "-4.4999\n",
      "4.6742\n",
      "-2.3497\n",
      "-1.8018\n",
      "-2.8302\n",
      "-1.0658\n",
      "-4.7684\n",
      "-1.1916\n",
      "-0.2008\n",
      "-6.793\n",
      "-1.6487\n",
      "-0.3112\n",
      "5.7447\n",
      "5.6479\n",
      "0.0842\n",
      "-3.5472\n",
      "4.2379\n",
      "-3.7226\n",
      "-3.5074\n",
      "2.1394\n",
      "-3.3765\n",
      "-4.495\n",
      "1.7372\n",
      "-9.6744\n",
      "-8.3994\n",
      "20.0724\n",
      "-2.6019\n",
      "1.4737\n",
      "-1.7269\n",
      "6.0119\n",
      "-2.7984\n",
      "1.6352\n",
      "-0.6902\n",
      "-0.1928\n",
      "-1.4352\n",
      "1.3911\n",
      "-3.3444\n",
      "0.5253\n",
      "-2.7626\n",
      "2.2109\n",
      "-3.3981\n",
      "1.2139\n",
      "9.1377\n",
      "-7.8371\n",
      "1.9402\n",
      "-3.5528\n",
      "3.343\n",
      "-7.0784\n",
      "-10.7697\n",
      "21.6369\n",
      "2.0083\n",
      "1.8947\n",
      "-2.6484\n",
      "8.7259\n",
      "3.4495\n",
      "-1.2102\n",
      "1.5289\n",
      "3.3229\n",
      "-1.5121\n",
      "3.3355\n",
      "-4.3488\n",
      "9.0426\n",
      "1.9502\n",
      "-2.365\n",
      "0.0443\n",
      "0.3872\n",
      "2.2834\n",
      "-4.7592\n",
      "-1.2225\n",
      "-1.7072\n",
      "-1.052\n",
      "1.404\n",
      "-12.0984\n",
      "11.2732\n",
      "-12.9948\n",
      "16.844\n",
      "0.0704\n",
      "0.8494\n",
      "0.1516\n",
      "0.3141\n",
      "2.9582\n",
      "-1.4642\n",
      "0.8698\n",
      "-3.8377\n",
      "-1.0668\n",
      "0.5004\n",
      "-0.3387\n",
      "-0.9542\n",
      "1.0317\n",
      "-1.0732\n",
      "0.408\n",
      "-0.7745\n",
      "0.0477\n",
      "-0.8997\n",
      "-3.4017\n",
      "5.5536\n",
      "-5.5062\n",
      "11.2989\n",
      "-2.152\n",
      "3.3756\n",
      "-0.1462\n",
      "2.5948\n",
      "-1.5235\n",
      "0.3206\n",
      "-1.6146\n",
      "2.0655\n",
      "0.0401\n",
      "-0.7923\n",
      "-1.6134\n",
      "3.9694\n",
      "1.2149\n",
      "1.0347\n",
      "-0.2645\n",
      "1.125\n",
      "-1.5217\n",
      "1.1186\n",
      "-1.2556\n",
      "2.5878\n",
      "0.2021\n",
      "1.4876\n",
      "-3.7646\n",
      "6.4291\n",
      "-2.0976\n",
      "4.3081\n",
      "-1.8522\n",
      "3.9145\n",
      "-0.0442\n",
      "1.996\n",
      "-0.8895\n",
      "6.484\n",
      "-0.8273\n",
      "2.7975\n",
      "1.0013\n",
      "-0.5058\n",
      "-1.092\n",
      "8.7642\n",
      "-2.1489\n",
      "3.8923\n",
      "-0.927\n",
      "-6.8969\n",
      "2.8221\n",
      "0.7378\n",
      "1.9987\n",
      "-1.3003\n",
      "-3.4085\n",
      "6.9988\n",
      "-1.9371\n",
      "1.9793\n",
      "1.2237\n",
      "1.0379\n",
      "-1.428\n",
      "2.9322\n",
      "-1.7262\n",
      "0.8009\n",
      "1.0047\n",
      "-0.6215\n",
      "-0.506\n",
      "-1.3333\n",
      "1.7955\n",
      "1.6867\n",
      "1.1138\n",
      "0.1538\n",
      "0.1138\n",
      "-0.9217\n",
      "4.4609\n",
      "-2.9415\n",
      "2.6395\n",
      "3.6643\n",
      "0.5491\n",
      "8.7531\n",
      "1.2579\n",
      "3.6015\n",
      "-1.5986\n",
      "-3.6813\n",
      "1.4595\n",
      "-3.095\n",
      "0.5173\n",
      "-0.9771\n",
      "0.7898\n",
      "0.267\n",
      "3.9557\n",
      "0.5584\n",
      "3.3596\n",
      "6.1655\n",
      "3.0302\n",
      "4.0327\n",
      "3.3371\n",
      "3.3755\n",
      "10.8518\n",
      "-2.6948\n",
      "-1.696\n",
      "-1.9254\n",
      "-6.0433\n",
      "5.4906\n",
      "-2.5772\n",
      "-1.1932\n",
      "-1.0554\n",
      "1.4349\n",
      "-2.2465\n",
      "4.2815\n",
      "0.5274\n",
      "1.2519\n",
      "-5.9411\n",
      "2.9402\n",
      "-0.7188\n",
      "-0.3309\n",
      "-5.3901\n",
      "-0.5525\n",
      "-5.7655\n",
      "-3.1552\n",
      "5.682\n",
      "-1.1918\n",
      "-3.9125\n",
      "-4.3663\n",
      "-2.1593\n",
      "-0.0453\n",
      "-2.9165\n",
      "4.6011\n",
      "-6.3565\n",
      "1.2751\n",
      "-3.3707\n",
      "0.7\n",
      "1.3489\n",
      "0.6879\n",
      "-1.7341\n",
      "2.5861\n",
      "-4.7056\n",
      "-0.7585\n",
      "-2.6002\n",
      "1.0334\n",
      "0.6012\n",
      "-8.6127\n",
      "-3.1155\n",
      "0.3626\n",
      "4.201\n",
      "-1.0744\n",
      "-1.5053\n",
      "4.327\n",
      "-0.2944\n",
      "3.7496\n",
      "8.8294\n",
      "14.8805\n",
      "-4.2444\n",
      "2.4112\n",
      "-3.134\n",
      "2.6084\n",
      "0.5426\n",
      "2.4162\n",
      "4.9178\n",
      "11.326\n",
      "1.8754\n",
      "3.4325\n",
      "-5.88\n",
      "2.9734\n",
      "-3.5582\n",
      "-2.9508\n",
      "-3.0315\n",
      "-3.9697\n",
      "-5.9241\n",
      "-1.5007\n",
      "0.4456\n",
      "-3.3376\n",
      "-4.0007\n",
      "-1.0585\n",
      "-4.124\n",
      "6.2328\n",
      "-2.9963\n",
      "-0.0588\n",
      "-2.6908\n",
      "-0.6209\n",
      "-2.2373\n",
      "-2.6024\n",
      "-2.5642\n",
      "1.6771\n",
      "0.8821\n",
      "-2.7098\n",
      "-3.9648\n",
      "-2.6536\n",
      "3.068\n",
      "-5.3436\n",
      "-4.3212\n",
      "0.5089\n",
      "-2.9213\n",
      "-3.4767\n",
      "-4.4861\n",
      "-4.327\n",
      "-0.2998\n",
      "-3.3325\n",
      "5.9874\n",
      "2.7991\n",
      "2.427\n",
      "7.7499\n",
      "-2.1476\n",
      "-0.0181\n",
      "-2.6997\n",
      "-1.7246\n",
      "-0.4583\n",
      "-4.2397\n",
      "-1.5487\n",
      "-3.1194\n",
      "-0.0856\n",
      "0.5617\n",
      "-0.6873\n",
      "0.278\n",
      "-3.2975\n",
      "-1.2086\n",
      "-0.5984\n",
      "-5.3663\n",
      "0.299\n",
      "-3.892\n",
      "0.652\n",
      "-3.6572\n",
      "1.329\n",
      "5.3184\n",
      "-0.2488\n",
      "-1.4278\n",
      "-1.0981\n",
      "-0.5926\n",
      "0.1614\n",
      "2.2909\n",
      "0.3403\n",
      "-1.3958\n",
      "-0.8695\n",
      "0.3172\n",
      "-2.9061\n",
      "0.7724\n",
      "-2.7651\n",
      "-5.4326\n",
      "-4.4952\n",
      "2.0339\n",
      "0.7861\n",
      "-3.6496\n",
      "-1.6534\n",
      "-1.6906\n",
      "-2.2748\n",
      "-1.4486\n",
      "2.6115\n",
      "0.9196\n",
      "4.0124\n",
      "2.1359\n",
      "-1.4078\n",
      "5.0877\n",
      "-2.2946\n",
      "3.3179\n",
      "-1.5162\n",
      "1.8038\n",
      "-1.7838\n",
      "-1.3622\n",
      "-3.3765\n",
      "0.7205\n",
      "-0.4947\n",
      "-3.5975\n",
      "-1.5886\n",
      "1.1492\n",
      "-5.528\n",
      "-1.7648\n",
      "-3.0825\n",
      "0.1341\n",
      "0.7624\n",
      "-10.5692\n",
      "1.5222\n",
      "7.9993\n",
      "0.0579\n",
      "-2.7505\n",
      "-0.8241\n",
      "4.2861\n",
      "-0.9847\n",
      "1.0884\n",
      "-2.8018\n",
      "-0.2328\n",
      "0.605\n",
      "-3.0573\n",
      "-1.0183\n",
      "2.6661\n",
      "-2.2521\n",
      "0.2923\n",
      "-0.5074\n",
      "-2.0335\n",
      "-2.5208\n",
      "-2.9415\n",
      "-1.7796\n",
      "-2.6987\n",
      "0.7804\n",
      "-1.9635\n",
      "0.1443\n",
      "4.5625\n",
      "-0.3265\n",
      "1.1953\n",
      "-1.1319\n",
      "0.2065\n",
      "0.5247\n",
      "-1.3234\n",
      "-1.005\n",
      "-1.9586\n",
      "-3.0007\n",
      "0.1953\n",
      "0.7045\n",
      "-5.2961\n",
      "0.7251\n",
      "-2.3524\n",
      "0.3088\n",
      "1.9369\n",
      "0.6557\n",
      "1.0984\n",
      "0.6031\n",
      "-3.8688\n",
      "-0.2093\n",
      "-3.2193\n",
      "-0.5349\n",
      "7.7595\n",
      "1.4301\n",
      "0.6824\n",
      "1.7879\n",
      "0.7743\n",
      "1.397\n",
      "3.6755\n",
      "3.77\n",
      "-5.3648\n",
      "1.303\n",
      "2.1413\n",
      "-0.5198\n",
      "1.9721\n",
      "-4.5254\n",
      "2.3477\n",
      "-1.2737\n",
      "1.331\n",
      "-0.4897\n",
      "-2.4961\n",
      "3.9951\n",
      "-8.2278\n",
      "0.4535\n",
      "-8.3978\n",
      "0.8053\n",
      "4.3966\n",
      "3.595\n",
      "8.7959\n",
      "2.4657\n",
      "8.0864\n",
      "1.8411\n",
      "7.5359\n",
      "1.2086\n",
      "-1.5084\n",
      "-1.1446\n",
      "3.6246\n",
      "0.0214\n",
      "-0.5807\n",
      "0.1107\n",
      "0.1736\n",
      "2.3469\n",
      "2.1036\n",
      "1.8983\n",
      "0.8228\n",
      "0.9766\n",
      "4.9115\n",
      "2.1726\n",
      "-11.853\n",
      "-1.1076\n",
      "26.2037\n",
      "-2.5694\n",
      "8.4891\n",
      "-3.9641\n",
      "4.6159\n",
      "-4.1752\n",
      "-3.3471\n",
      "-2.2105\n",
      "0.4985\n",
      "-2.6228\n",
      "-1.5115\n",
      "-0.3258\n",
      "-2.8485\n",
      "-1.4758\n",
      "0.1361\n",
      "0.5021\n",
      "-0.819\n",
      "1.6998\n",
      "-1.9116\n",
      "1.4663\n",
      "1.2874\n",
      "-2.5136\n",
      "1.7091\n",
      "-2.0731\n",
      "10.5903\n",
      "-7.0498\n",
      "1.959\n",
      "-2.9485\n",
      "0.0793\n",
      "-3.4508\n",
      "3.9749\n",
      "1.0923\n",
      "2.3805\n",
      "-1.0441\n",
      "3.801\n",
      "-3.2204\n",
      "1.2912\n",
      "1.5221\n",
      "-0.8171\n",
      "-0.0595\n",
      "3.1209\n",
      "0.3963\n",
      "4.4255\n",
      "-1.5308\n",
      "1.5445\n",
      "-0.9113\n",
      "-4.0657\n",
      "-2.8208\n",
      "5.4285\n",
      "-2.0556\n",
      "1.6838\n",
      "-3.1911\n",
      "2.3844\n",
      "-3.8772\n",
      "-3.223\n",
      "-2.1105\n",
      "0.9696\n",
      "-3.7018\n",
      "0.0205\n",
      "-0.542\n",
      "-1.4879\n",
      "1.0272\n",
      "-3.0154\n",
      "-0.8048\n",
      "3.9229\n",
      "-0.8492\n",
      "-0.7579\n",
      "-1.8858\n",
      "-0.3883\n",
      "-0.9498\n",
      "-5.2676\n",
      "0.3868\n",
      "6.7488\n",
      "-0.3294\n",
      "-1.641\n",
      "0.5357\n",
      "-0.9007\n",
      "-2.5329\n",
      "0.7474\n",
      "-0.6489\n",
      "-0.7881\n",
      "3.7493\n",
      "2.1435\n",
      "2.2511\n",
      "0.7455\n",
      "0.1345\n",
      "3.2772\n",
      "0.301\n",
      "2.1868\n",
      "-2.0933\n",
      "-1.2763\n",
      "0.4601\n",
      "-6.2963\n",
      "-0.7999\n",
      "-2.1492\n",
      "-1.0771\n",
      "6.6819\n",
      "2.1783\n",
      "2.8667\n",
      "0.0761\n",
      "2.6737\n",
      "1.63\n",
      "2.1808\n",
      "1.9151\n",
      "9.7209\n",
      "-3.9091\n",
      "0.9738\n",
      "-6.8905\n",
      "-1.754\n",
      "1.3661\n",
      "-1.822\n",
      "1.3448\n",
      "-2.5913\n",
      "-5.9871\n",
      "-3.0839\n",
      "-2.3949\n",
      "-3.8954\n",
      "-2.185\n",
      "-4.5695\n",
      "2.5571\n",
      "9.8093\n",
      "-5.5749\n",
      "1.3945\n",
      "-5.8537\n",
      "-0.4346\n",
      "-2.0985\n",
      "-4.4108\n",
      "4.8877\n",
      "-3.7861\n",
      "-2.4947\n",
      "-0.6379\n",
      "-0.3282\n",
      "-2.9885\n",
      "0.4378\n",
      "4.8484\n",
      "0.3357\n",
      "2.6137\n",
      "-3.2054\n",
      "5.338\n",
      "-2.808\n",
      "-4.6496\n",
      "-4.2259\n",
      "-10.3353\n",
      "0.4352\n",
      "7.1134\n",
      "0.6922\n",
      "-2.9032\n",
      "-0.8939\n",
      "0.9357\n",
      "0.0082\n",
      "4.3251\n",
      "-2.7294\n",
      "1.1643\n",
      "0.154\n",
      "-6.157\n",
      "-1.9163\n",
      "2.1849\n",
      "-5.4015\n",
      "-1.3093\n",
      "4.341\n",
      "-4.744\n",
      "3.5373\n",
      "-4.2977\n",
      "4.5029\n",
      "6.4539\n",
      "2.5277\n",
      "-5.5574\n",
      "2.9382\n",
      "16.6733\n",
      "-2.0819\n",
      "3.9088\n",
      "-0.0183\n",
      "-4.0397\n",
      "-1.6047\n",
      "0.6935\n",
      "-0.1165\n",
      "4.3051\n",
      "0.7698\n",
      "-2.4159\n",
      "-1.8585\n",
      "6.1961\n",
      "-0.8864\n",
      "-0.9777\n",
      "2.4189\n",
      "-0.8857\n",
      "3.1734\n",
      "-3.0092\n",
      "6.3974\n",
      "9.7635\n",
      "8.4721\n",
      "0.633\n",
      "0.3556\n",
      "19.0131\n",
      "8.9089\n",
      "9.5794\n",
      "-1.7607\n",
      "-3.8176\n",
      "-6.0472\n",
      "-4.4892\n",
      "-3.3449\n",
      "-3.6314\n",
      "5.2573\n",
      "1.0209\n",
      "-7.4866\n",
      "0.4543\n",
      "4.0534\n",
      "-3.7663\n",
      "-6.3399\n",
      "-0.3427\n",
      "-4.9885\n",
      "-4.3289\n",
      "-15.2861\n",
      "-3.2103\n",
      "-6.8528\n",
      "-16.7585\n",
      "18.9868\n",
      "37.3221\n",
      "-15.0691\n",
      "1.9597\n",
      "-9.7513\n",
      "-1.0554\n",
      "9.1695\n",
      "-2.7727\n",
      "-1.1629\n",
      "7.3187\n",
      "0.0027\n",
      "-1.0796\n",
      "-3.9672\n",
      "1.8439\n",
      "-5.0697\n",
      "1.5771\n",
      "-3.4348\n",
      "1.8855\n",
      "5.971\n",
      "3.7476\n",
      "3.1134\n",
      "-0.89\n",
      "1.2871\n",
      "0.4434\n",
      "-7.3996\n",
      "8.9864\n",
      "-5.4021\n",
      "-1.2762\n",
      "-0.3559\n",
      "0.1354\n",
      "-7.8904\n",
      "2.3628\n",
      "-6.619\n",
      "2.405\n",
      "-5.3986\n",
      "-2.6182\n",
      "-1.6395\n",
      "2.0426\n",
      "0.0528\n",
      "0.8606\n",
      "-0.8922\n",
      "-0.9045\n",
      "3.4142\n",
      "0.1568\n",
      "4.541\n",
      "8.8646\n",
      "-4.7799\n",
      "-0.537\n",
      "-1.4666\n",
      "7.0391\n",
      "-1.0804\n",
      "0.6478\n",
      "2.3292\n",
      "0.479\n",
      "2.4205\n",
      "2.1861\n",
      "6.5181\n",
      "10.8236\n",
      "-0.43\n",
      "7.8368\n",
      "-1.0246\n",
      "5.6278\n",
      "-2.2095\n",
      "1.9398\n",
      "-3.4419\n",
      "8.4923\n",
      "-3.0452\n",
      "2.0274\n",
      "-1.4965\n",
      "1.834\n",
      "-3.7406\n",
      "1.3495\n",
      "-1.6753\n",
      "9.9422\n",
      "-1.4307\n",
      "-0.8587\n",
      "-1.1335\n",
      "1.1276\n",
      "0.3811\n",
      "2.3314\n",
      "0.1213\n",
      "-3.8067\n",
      "-1.0234\n",
      "-1.8285\n",
      "-5.7197\n",
      "-0.9886\n",
      "-1.5576\n",
      "-1.7804\n",
      "0.9067\n",
      "-0.0232\n",
      "0.199\n",
      "-0.7693\n",
      "-0.4061\n",
      "4.1268\n",
      "1.9714\n",
      "5.4698\n",
      "-3.4277\n",
      "0.4717\n",
      "-3.2358\n",
      "-1.5523\n",
      "-2.8109\n",
      "-1.6798\n",
      "-1.3115\n",
      "-2.4532\n",
      "2.9025\n",
      "-1.9202\n",
      "-1.333\n",
      "0.2771\n",
      "0.7207\n",
      "0.9724\n",
      "-2.2379\n",
      "0.1578\n",
      "-0.3175\n",
      "0.1399\n",
      "-0.3162\n",
      "-0.9282\n",
      "0.9096\n",
      "-2.1783\n",
      "-0.9254\n",
      "1.4756\n",
      "1.5494\n",
      "2.8394\n",
      "0.724\n",
      "2.2112\n",
      "0.1143\n",
      "1.9441\n",
      "-2.5783\n",
      "0.4091\n",
      "-3.1519\n",
      "-0.4383\n",
      "-1.4146\n",
      "-1.4267\n",
      "-2.874\n",
      "-1.3287\n",
      "0.9979\n",
      "-1.0606\n",
      "-1.0902\n",
      "-0.6111\n",
      "1.0026\n",
      "0.0724\n",
      "-0.2139\n",
      "-0.3984\n",
      "-2.5944\n",
      "1.9707\n",
      "0.9707\n",
      "0.9514\n",
      "-0.5178\n",
      "1.8036\n",
      "-0.2654\n",
      "-1.6048\n",
      "0.8876\n",
      "-0.7386\n",
      "0.286\n",
      "-4.2307\n",
      "-0.8587\n",
      "1.5299\n",
      "0.0557\n",
      "1.2019\n",
      "2.3054\n",
      "-5.1642\n",
      "1.3455\n",
      "-1.6518\n",
      "2.7432\n",
      "-2.8345\n",
      "-0.2049\n",
      "-4.8585\n",
      "0.8415\n",
      "-2.0992\n",
      "-2.8444\n",
      "2.7859\n",
      "0.0981\n",
      "1.5223\n",
      "-0.616\n",
      "-3.6263\n",
      "1.4798\n",
      "-5.4071\n",
      "2.1305\n",
      "-1.2075\n",
      "1.6012\n",
      "-0.0228\n",
      "0.4515\n",
      "-2.309\n",
      "-0.8268\n",
      "-0.9967\n",
      "-3.0839\n",
      "-3.266\n",
      "-1.4466\n",
      "0.7484\n",
      "1.1612\n",
      "-5.4095\n",
      "1.4957\n",
      "-4.7786\n",
      "5.3541\n",
      "16.2419\n",
      "3.2896\n",
      "-0.3353\n",
      "1.6891\n",
      "7.5012\n",
      "-2.6541\n",
      "9.0347\n",
      "-1.8943\n",
      "11.5919\n",
      "2.8092\n",
      "5.5627\n",
      "-3.0578\n",
      "-2.8061\n",
      "-5.561\n",
      "9.0149\n",
      "-2.4176\n",
      "3.1632\n",
      "0.0401\n",
      "-0.4711\n",
      "0.0613\n",
      "-4.9386\n",
      "0.6828\n",
      "-0.7264\n",
      "-4.1131\n",
      "7.759\n",
      "-0.1467\n",
      "-2.0438\n",
      "-1.9095\n",
      "0.9638\n",
      "-3.7319\n",
      "8.106\n",
      "2.2006\n",
      "-1.1342\n",
      "1.8447\n",
      "-2.4023\n",
      "0.123\n",
      "-3.4949\n",
      "-0.2119\n",
      "-0.3846\n",
      "2.9728\n",
      "-2.0619\n",
      "0.5285\n",
      "1.5672\n",
      "1.056\n",
      "-0.551\n",
      "0.5655\n",
      "1.47\n",
      "-2.2343\n",
      "3.5196\n",
      "0.8419\n",
      "-0.5894\n",
      "1.6881\n",
      "-2.1257\n",
      "1.1878\n",
      "-3.104\n",
      "0.898\n",
      "-1.7273\n",
      "-0.631\n",
      "-1.3067\n",
      "-1.3215\n",
      "3.0475\n",
      "0.0656\n",
      "-3.0289\n",
      "-1.5619\n",
      "-0.4111\n",
      "5.1161\n",
      "-7.1643\n",
      "-0.4174\n",
      "-3.1994\n",
      "-1.0816\n",
      "-1.353\n",
      "1.3146\n",
      "7.0987\n",
      "-0.9231\n",
      "2.0607\n",
      "-0.3354\n",
      "2.3497\n",
      "-1.4428\n",
      "0.1154\n",
      "-1.0928\n",
      "0.2546\n",
      "0.0336\n",
      "-2.0109\n",
      "1.0174\n",
      "-0.6994\n",
      "-0.3431\n",
      "-1.25\n",
      "-0.0544\n",
      "2.7112\n",
      "-3.6391\n",
      "-0.4905\n",
      "-1.1593\n",
      "-1.9828\n",
      "-1.6021\n",
      "-0.8403\n",
      "-1.9504\n",
      "3.7457\n",
      "-0.7728\n",
      "0.0921\n",
      "1.0181\n",
      "-0.8244\n",
      "0.7034\n",
      "-1.5025\n",
      "-0.0188\n",
      "2.9871\n",
      "-0.4143\n",
      "0.3751\n",
      "1.452\n",
      "2.1071\n",
      "-0.8634\n",
      "3.1386\n",
      "-1.1383\n",
      "1.3445\n",
      "-2.5893\n",
      "-0.8273\n",
      "1.2707\n",
      "1.3269\n",
      "0.1988\n",
      "1.0917\n",
      "1.5832\n",
      "7.7175\n",
      "0.2693\n",
      "-0.6128\n",
      "-2.6341\n",
      "0.2042\n",
      "-1.7681\n",
      "-4.2565\n",
      "0.0475\n",
      "-2.9585\n",
      "1.5872\n",
      "-0.9767\n",
      "-1.8968\n",
      "0.2232\n",
      "1.3637\n",
      "-1.8906\n",
      "0.5307\n",
      "-0.3894\n",
      "3.6042\n",
      "-4.8581\n",
      "1.6712\n",
      "-2.5328\n",
      "1.3589\n",
      "-0.8711\n",
      "0.8957\n",
      "0.4657\n",
      "2.9393\n",
      "0.8797\n",
      "0.9441\n",
      "-1.3296\n",
      "-3.683\n",
      "3.755\n",
      "1.598\n",
      "-1.5425\n",
      "1.5428\n",
      "0.2632\n",
      "4.7943\n",
      "-5.9062\n",
      "-3.7199\n",
      "3.4243\n",
      "-1.1174\n",
      "-0.863\n",
      "0.0166\n",
      "-3.0167\n",
      "3.1959\n",
      "0.3187\n",
      "0.4755\n",
      "-0.8172\n",
      "-3.173\n",
      "-1.8421\n",
      "-0.9243\n",
      "0.1112\n",
      "-1.6561\n",
      "0.6698\n",
      "-4.7495\n",
      "5.8498\n",
      "2.799\n",
      "-3.3956\n",
      "-3.7151\n",
      "0.9998\n",
      "-0.5389\n",
      "2.4553\n",
      "-0.113\n",
      "1.9004\n",
      "0.9873\n",
      "2.7287\n",
      "-3.0241\n",
      "-3.4799\n",
      "-3.0709\n",
      "-0.2994\n",
      "-2.2067\n",
      "-0.7802\n",
      "-0.2247\n",
      "2.1871\n",
      "2.5645\n",
      "-1.3841\n",
      "0.8965\n",
      "0.9839\n",
      "1.4162\n",
      "-2.5657\n",
      "0.9942\n",
      "-3.4961\n",
      "2.0956\n",
      "1.8962\n",
      "-1.0847\n",
      "-2.9307\n",
      "-0.1329\n",
      "-1.7783\n",
      "-0.1903\n",
      "5.5163\n",
      "-1.5812\n",
      "-2.1001\n",
      "0.1573\n",
      "0.4232\n",
      "0.3213\n",
      "1.3721\n",
      "0.7966\n",
      "-0.4266\n",
      "0.897\n",
      "-0.2562\n",
      "0.9599\n",
      "0.8339\n",
      "-0.7461\n",
      "0.3904\n",
      "2.8347\n",
      "1.4268\n",
      "1.6499\n",
      "-0.1409\n",
      "-0.7275\n",
      "-4.2455\n",
      "4.9397\n",
      "-0.6051\n",
      "1.3056\n",
      "0.1259\n",
      "-4.0181\n",
      "0.0497\n",
      "0.7598\n",
      "-5.8589\n",
      "-1.7831\n",
      "-5.781\n",
      "2.4018\n",
      "8.8707\n",
      "2.2285\n",
      "1.006\n",
      "2.052\n",
      "-0.0792\n",
      "-0.5673\n",
      "-2.7677\n",
      "0.6598\n",
      "-2.0847\n",
      "2.5172\n",
      "-5.2925\n",
      "0.5496\n",
      "-3.5356\n",
      "2.8137\n",
      "-1.3218\n",
      "-4.1669\n",
      "0.6961\n",
      "1.9743\n",
      "-3.6218\n",
      "3.8689\n",
      "-2.5221\n",
      "-2.5448\n",
      "4.0873\n",
      "6.4572\n",
      "4.0881\n",
      "3.2958\n",
      "2.7991\n",
      "8.1734\n",
      "-3.5643\n",
      "0.4204\n",
      "9.7153\n",
      "4.676\n",
      "3.7172\n",
      "-3.2616\n",
      "13.0513\n",
      "0.8018\n",
      "1.4406\n",
      "0.3353\n",
      "-1.0835\n",
      "1.6382\n",
      "1.8242\n",
      "-2.1864\n",
      "0.4886\n",
      "-5.2412\n",
      "10.1898\n",
      "-0.5157\n",
      "4.8759\n",
      "-3.7411\n",
      "22.2993\n",
      "-5.0835\n",
      "7.2089\n",
      "-4.8348\n",
      "-1.4105\n",
      "0.8662\n",
      "-4.5949\n",
      "-4.7668\n",
      "2.3653\n",
      "2.1254\n",
      "4.3918\n",
      "-0.1612\n",
      "-5.4062\n",
      "0.2147\n",
      "0.5585\n",
      "-3.3684\n",
      "1.4522\n",
      "0.7635\n",
      "-4.9848\n",
      "-1.1611\n",
      "-4.1741\n"
     ]
    }
   ],
   "source": [
    "dateList = list(df['DATE'].unique())\n",
    "\n",
    "FFA = pd.DataFrame(columns=\n",
    "    ['Date',\n",
    "     'HML',\n",
    "     'SMB',\n",
    "     'MktPrem',\n",
    "     'Rf',\n",
    "     'Equity_Premium',\n",
    "     ]\n",
    ")\n",
    "\n",
    "FFAIndex = 0\n",
    "for i in dateList:\n",
    "    FFA.loc[FFAIndex] = [i,\n",
    "        calcHML(df.loc[df['DATE'] == i]),\n",
    "        calcSMB(df.loc[df['DATE'] == i]),\n",
    "        df.loc[df['DATE'] == i]['RM_RF'].mean(),\n",
    "        df.loc[df['DATE'] == i]['macro_tbl'].mean(),\n",
    "        df.loc[df['DATE'] == i]['risk_premium'].mean(),\n",
    "\n",
    "    ]\n",
    "    FFAIndex += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>HML</th>\n",
       "      <th>SMB</th>\n",
       "      <th>MktPrem</th>\n",
       "      <th>Rf</th>\n",
       "      <th>Equity_Premium</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1962-07-31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-5.441999</td>\n",
       "      <td>0.0292</td>\n",
       "      <td>-6.781600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1962-08-31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.646829</td>\n",
       "      <td>0.0282</td>\n",
       "      <td>-0.769200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1962-09-28</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-7.485565</td>\n",
       "      <td>0.0278</td>\n",
       "      <td>-8.740000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1962-10-31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-7.014487</td>\n",
       "      <td>0.0274</td>\n",
       "      <td>-8.393800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1962-11-30</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.929655</td>\n",
       "      <td>0.0283</td>\n",
       "      <td>8.418200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>708</th>\n",
       "      <td>2021-07-30</td>\n",
       "      <td>-0.1612</td>\n",
       "      <td>-5.4062</td>\n",
       "      <td>-0.733862</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>-3.285524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>709</th>\n",
       "      <td>2021-08-31</td>\n",
       "      <td>0.2147</td>\n",
       "      <td>0.5585</td>\n",
       "      <td>4.917440</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>1.918121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>710</th>\n",
       "      <td>2021-09-30</td>\n",
       "      <td>-3.3684</td>\n",
       "      <td>1.4522</td>\n",
       "      <td>1.309428</td>\n",
       "      <td>0.0004</td>\n",
       "      <td>-2.473316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>711</th>\n",
       "      <td>2021-10-29</td>\n",
       "      <td>0.7635</td>\n",
       "      <td>-4.9848</td>\n",
       "      <td>8.932851</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>2.907306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>712</th>\n",
       "      <td>2021-11-30</td>\n",
       "      <td>-1.1611</td>\n",
       "      <td>-4.1741</td>\n",
       "      <td>-2.861788</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>-5.301971</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>713 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Date     HML     SMB   MktPrem      Rf  Equity_Premium\n",
       "0    1962-07-31     NaN     NaN -5.441999  0.0292       -6.781600\n",
       "1    1962-08-31     NaN     NaN -0.646829  0.0282       -0.769200\n",
       "2    1962-09-28     NaN     NaN -7.485565  0.0278       -8.740000\n",
       "3    1962-10-31     NaN     NaN -7.014487  0.0274       -8.393800\n",
       "4    1962-11-30     NaN     NaN  6.929655  0.0283        8.418200\n",
       "..          ...     ...     ...       ...     ...             ...\n",
       "708  2021-07-30 -0.1612 -5.4062 -0.733862  0.0005       -3.285524\n",
       "709  2021-08-31  0.2147  0.5585  4.917440  0.0005        1.918121\n",
       "710  2021-09-30 -3.3684  1.4522  1.309428  0.0004       -2.473316\n",
       "711  2021-10-29  0.7635 -4.9848  8.932851  0.0005        2.907306\n",
       "712  2021-11-30 -1.1611 -4.1741 -2.861788  0.0005       -5.301971\n",
       "\n",
       "[713 rows x 6 columns]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FFA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "FFA = FFA.interpolate(limit_direction='backward')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.any(np.isfinite(FFA))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test = train_test_split(FFA, test_size=0.25, random_state=42)\n",
    "\n",
    "train.reset_index(drop=True, inplace=True)\n",
    "test.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(FFA.iloc[:,:4], FFA['Equity_Premium'], test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HML</th>\n",
       "      <th>SMB</th>\n",
       "      <th>MktPrem</th>\n",
       "      <th>Rf</th>\n",
       "      <th>Equity_Premium</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-2.3033</td>\n",
       "      <td>4.8829</td>\n",
       "      <td>-5.441999</td>\n",
       "      <td>0.0292</td>\n",
       "      <td>-6.781600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-2.3033</td>\n",
       "      <td>4.8829</td>\n",
       "      <td>-0.646829</td>\n",
       "      <td>0.0282</td>\n",
       "      <td>-0.769200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-2.3033</td>\n",
       "      <td>4.8829</td>\n",
       "      <td>-7.485565</td>\n",
       "      <td>0.0278</td>\n",
       "      <td>-8.740000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-2.3033</td>\n",
       "      <td>4.8829</td>\n",
       "      <td>-7.014487</td>\n",
       "      <td>0.0274</td>\n",
       "      <td>-8.393800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-2.3033</td>\n",
       "      <td>4.8829</td>\n",
       "      <td>6.929655</td>\n",
       "      <td>0.0283</td>\n",
       "      <td>8.418200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>708</th>\n",
       "      <td>-0.1612</td>\n",
       "      <td>-5.4062</td>\n",
       "      <td>-0.733862</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>-3.285524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>709</th>\n",
       "      <td>0.2147</td>\n",
       "      <td>0.5585</td>\n",
       "      <td>4.917440</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>1.918121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>710</th>\n",
       "      <td>-3.3684</td>\n",
       "      <td>1.4522</td>\n",
       "      <td>1.309428</td>\n",
       "      <td>0.0004</td>\n",
       "      <td>-2.473316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>711</th>\n",
       "      <td>0.7635</td>\n",
       "      <td>-4.9848</td>\n",
       "      <td>8.932851</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>2.907306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>712</th>\n",
       "      <td>-1.1611</td>\n",
       "      <td>-4.1741</td>\n",
       "      <td>-2.861788</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>-5.301971</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>713 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        HML     SMB   MktPrem      Rf  Equity_Premium\n",
       "0   -2.3033  4.8829 -5.441999  0.0292       -6.781600\n",
       "1   -2.3033  4.8829 -0.646829  0.0282       -0.769200\n",
       "2   -2.3033  4.8829 -7.485565  0.0278       -8.740000\n",
       "3   -2.3033  4.8829 -7.014487  0.0274       -8.393800\n",
       "4   -2.3033  4.8829  6.929655  0.0283        8.418200\n",
       "..      ...     ...       ...     ...             ...\n",
       "708 -0.1612 -5.4062 -0.733862  0.0005       -3.285524\n",
       "709  0.2147  0.5585  4.917440  0.0005        1.918121\n",
       "710 -3.3684  1.4522  1.309428  0.0004       -2.473316\n",
       "711  0.7635 -4.9848  8.932851  0.0005        2.907306\n",
       "712 -1.1611 -4.1741 -2.861788  0.0005       -5.301971\n",
       "\n",
       "[713 rows x 5 columns]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FFA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "\n",
    "lm = LinearRegression()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "\nAll the 5 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n5 fits failed with the following error:\nTraceback (most recent call last):\n  File \"c:\\Users\\drebi\\miniconda3\\envs\\statclass\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"c:\\Users\\drebi\\miniconda3\\envs\\statclass\\Lib\\site-packages\\sklearn\\base.py\", line 1473, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\drebi\\miniconda3\\envs\\statclass\\Lib\\site-packages\\sklearn\\linear_model\\_base.py\", line 609, in fit\n    X, y = self._validate_data(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\drebi\\miniconda3\\envs\\statclass\\Lib\\site-packages\\sklearn\\base.py\", line 650, in _validate_data\n    X, y = check_X_y(X, y, **check_params)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\drebi\\miniconda3\\envs\\statclass\\Lib\\site-packages\\sklearn\\utils\\validation.py\", line 1301, in check_X_y\n    X = check_array(\n        ^^^^^^^^^^^^\n  File \"c:\\Users\\drebi\\miniconda3\\envs\\statclass\\Lib\\site-packages\\sklearn\\utils\\validation.py\", line 1064, in check_array\n    _assert_all_finite(\n  File \"c:\\Users\\drebi\\miniconda3\\envs\\statclass\\Lib\\site-packages\\sklearn\\utils\\validation.py\", line 123, in _assert_all_finite\n    _assert_all_finite_element_wise(\n  File \"c:\\Users\\drebi\\miniconda3\\envs\\statclass\\Lib\\site-packages\\sklearn\\utils\\validation.py\", line 172, in _assert_all_finite_element_wise\n    raise ValueError(msg_err)\nValueError: Input X contains infinity or a value too large for dtype('float64').\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[81], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m cv \u001b[38;5;241m=\u001b[39m KFold(n_splits\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m45\u001b[39m)\n\u001b[0;32m      6\u001b[0m r2 \u001b[38;5;241m=\u001b[39m make_scorer(r2_score)\n\u001b[1;32m----> 7\u001b[0m r2_val_score \u001b[38;5;241m=\u001b[39m cross_val_score(lm, X_train, y_train, cv\u001b[38;5;241m=\u001b[39mcv, scoring\u001b[38;5;241m=\u001b[39mr2)\n\u001b[0;32m      8\u001b[0m scores\u001b[38;5;241m=\u001b[39m[r2_val_score\u001b[38;5;241m.\u001b[39mmean()]\n\u001b[0;32m      9\u001b[0m scores\n",
      "File \u001b[1;32mc:\\Users\\drebi\\miniconda3\\envs\\statclass\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    211\u001b[0m         )\n\u001b[0;32m    212\u001b[0m     ):\n\u001b[1;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    223\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\drebi\\miniconda3\\envs\\statclass\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:712\u001b[0m, in \u001b[0;36mcross_val_score\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, params, pre_dispatch, error_score)\u001b[0m\n\u001b[0;32m    709\u001b[0m \u001b[38;5;66;03m# To ensure multimetric format is not supported\u001b[39;00m\n\u001b[0;32m    710\u001b[0m scorer \u001b[38;5;241m=\u001b[39m check_scoring(estimator, scoring\u001b[38;5;241m=\u001b[39mscoring)\n\u001b[1;32m--> 712\u001b[0m cv_results \u001b[38;5;241m=\u001b[39m cross_validate(\n\u001b[0;32m    713\u001b[0m     estimator\u001b[38;5;241m=\u001b[39mestimator,\n\u001b[0;32m    714\u001b[0m     X\u001b[38;5;241m=\u001b[39mX,\n\u001b[0;32m    715\u001b[0m     y\u001b[38;5;241m=\u001b[39my,\n\u001b[0;32m    716\u001b[0m     groups\u001b[38;5;241m=\u001b[39mgroups,\n\u001b[0;32m    717\u001b[0m     scoring\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscore\u001b[39m\u001b[38;5;124m\"\u001b[39m: scorer},\n\u001b[0;32m    718\u001b[0m     cv\u001b[38;5;241m=\u001b[39mcv,\n\u001b[0;32m    719\u001b[0m     n_jobs\u001b[38;5;241m=\u001b[39mn_jobs,\n\u001b[0;32m    720\u001b[0m     verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[0;32m    721\u001b[0m     fit_params\u001b[38;5;241m=\u001b[39mfit_params,\n\u001b[0;32m    722\u001b[0m     params\u001b[38;5;241m=\u001b[39mparams,\n\u001b[0;32m    723\u001b[0m     pre_dispatch\u001b[38;5;241m=\u001b[39mpre_dispatch,\n\u001b[0;32m    724\u001b[0m     error_score\u001b[38;5;241m=\u001b[39merror_score,\n\u001b[0;32m    725\u001b[0m )\n\u001b[0;32m    726\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cv_results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_score\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\drebi\\miniconda3\\envs\\statclass\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    211\u001b[0m         )\n\u001b[0;32m    212\u001b[0m     ):\n\u001b[1;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    223\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\drebi\\miniconda3\\envs\\statclass\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:443\u001b[0m, in \u001b[0;36mcross_validate\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, params, pre_dispatch, return_train_score, return_estimator, return_indices, error_score)\u001b[0m\n\u001b[0;32m    422\u001b[0m parallel \u001b[38;5;241m=\u001b[39m Parallel(n_jobs\u001b[38;5;241m=\u001b[39mn_jobs, verbose\u001b[38;5;241m=\u001b[39mverbose, pre_dispatch\u001b[38;5;241m=\u001b[39mpre_dispatch)\n\u001b[0;32m    423\u001b[0m results \u001b[38;5;241m=\u001b[39m parallel(\n\u001b[0;32m    424\u001b[0m     delayed(_fit_and_score)(\n\u001b[0;32m    425\u001b[0m         clone(estimator),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    440\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m train, test \u001b[38;5;129;01min\u001b[39;00m indices\n\u001b[0;32m    441\u001b[0m )\n\u001b[1;32m--> 443\u001b[0m _warn_or_raise_about_fit_failures(results, error_score)\n\u001b[0;32m    445\u001b[0m \u001b[38;5;66;03m# For callable scoring, the return type is only know after calling. If the\u001b[39;00m\n\u001b[0;32m    446\u001b[0m \u001b[38;5;66;03m# return type is a dictionary, the error scores can now be inserted with\u001b[39;00m\n\u001b[0;32m    447\u001b[0m \u001b[38;5;66;03m# the correct key.\u001b[39;00m\n\u001b[0;32m    448\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(scoring):\n",
      "File \u001b[1;32mc:\\Users\\drebi\\miniconda3\\envs\\statclass\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:529\u001b[0m, in \u001b[0;36m_warn_or_raise_about_fit_failures\u001b[1;34m(results, error_score)\u001b[0m\n\u001b[0;32m    522\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m num_failed_fits \u001b[38;5;241m==\u001b[39m num_fits:\n\u001b[0;32m    523\u001b[0m     all_fits_failed_message \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    524\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mAll the \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m fits failed.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    525\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIt is very likely that your model is misconfigured.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    526\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou can try to debug the error by setting error_score=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    527\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBelow are more details about the failures:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfit_errors_summary\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    528\u001b[0m     )\n\u001b[1;32m--> 529\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(all_fits_failed_message)\n\u001b[0;32m    531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    532\u001b[0m     some_fits_failed_message \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    533\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mnum_failed_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m fits failed out of a total of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    534\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe score on these train-test partitions for these parameters\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    538\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBelow are more details about the failures:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfit_errors_summary\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    539\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: \nAll the 5 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n5 fits failed with the following error:\nTraceback (most recent call last):\n  File \"c:\\Users\\drebi\\miniconda3\\envs\\statclass\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"c:\\Users\\drebi\\miniconda3\\envs\\statclass\\Lib\\site-packages\\sklearn\\base.py\", line 1473, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\drebi\\miniconda3\\envs\\statclass\\Lib\\site-packages\\sklearn\\linear_model\\_base.py\", line 609, in fit\n    X, y = self._validate_data(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\drebi\\miniconda3\\envs\\statclass\\Lib\\site-packages\\sklearn\\base.py\", line 650, in _validate_data\n    X, y = check_X_y(X, y, **check_params)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\drebi\\miniconda3\\envs\\statclass\\Lib\\site-packages\\sklearn\\utils\\validation.py\", line 1301, in check_X_y\n    X = check_array(\n        ^^^^^^^^^^^^\n  File \"c:\\Users\\drebi\\miniconda3\\envs\\statclass\\Lib\\site-packages\\sklearn\\utils\\validation.py\", line 1064, in check_array\n    _assert_all_finite(\n  File \"c:\\Users\\drebi\\miniconda3\\envs\\statclass\\Lib\\site-packages\\sklearn\\utils\\validation.py\", line 123, in _assert_all_finite\n    _assert_all_finite_element_wise(\n  File \"c:\\Users\\drebi\\miniconda3\\envs\\statclass\\Lib\\site-packages\\sklearn\\utils\\validation.py\", line 172, in _assert_all_finite_element_wise\n    raise ValueError(msg_err)\nValueError: Input X contains infinity or a value too large for dtype('float64').\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=45)\n",
    "r2 = make_scorer(r2_score)\n",
    "r2_val_score = cross_val_score(lm, X_train, y_train, cv=cv, scoring=r2)\n",
    "scores=[r2_val_score.mean()]\n",
    "scores\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "statclass",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
